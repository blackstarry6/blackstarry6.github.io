<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>基于鱼书（第一册）的知识整理（1）</title>
    <link href="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/"/>
    <url>/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p>​对于复杂的函数，感知机也隐含着能够表示它的可能性。即便是计算机进行的复杂处理，感知机（理论上）也可以将其表示出来。但坏消息是，设定权重的工作，即确定合适的、能符合预期的输入与输出的权重，现在还是由人工进行的。</p><p>​神经网络的出现就是为了解决刚才的坏消息。具体地讲，神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AE%80%E5%8D%95%E4%BE%8B%E5%AD%90.png"></p><p>​中间层有时也称为隐藏层。“隐藏”一词的意思是，隐藏层的神经元（和输入层、输出层不同）肉眼看不见。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>​如“激活”一词所示，激活函数的作用在于决定如何来激活输入信号的总和。</p><p>​“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数的模型。“多层感知机”是指神经网络，即使用sigmoid函数等平滑的激活函数的多层网络。</p><h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><p>$$<br>\sigma(x) &#x3D; \frac{1}{1 + e^{-x}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pylab <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br>x = np.arange(-<span class="hljs-number">5.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">0.1</span>)<br>y = sigmoid(x)<br>plt.plot(x, y)<br>plt.ylim(-<span class="hljs-number">0.1</span>, <span class="hljs-number">1.1</span>) <span class="hljs-comment"># 指定y轴的范围</span><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/sigmoid%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%BE%E5%BD%A2.png"></p><p>​感知机中神经元之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数值信号。</p><p>​神经网络的激活函数必须使用非线性函数。换句话说，激活函数不能使用线性函数。因为线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。因此，为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。</p><h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><p>$$<br>\text{ReLU}(x) &#x3D; \max(0, x)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pylab <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> np.maximum(<span class="hljs-number">0</span>, x)<br><br>x = np.arange(-<span class="hljs-number">5.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">0.1</span>)<br>y = sigmoid(x)<br>plt.plot(x, y)<br>plt.ylim(-<span class="hljs-number">1</span>, <span class="hljs-number">5</span>) <span class="hljs-comment"># 指定y轴的范围</span><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/ReLU%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%BE%E5%BD%A2.png"></p><h2 id="3层神经网络的实现"><a href="#3层神经网络的实现" class="headerlink" title="3层神经网络的实现"></a>3层神经网络的实现</h2><h3 id="矩阵的运算"><a href="#矩阵的运算" class="headerlink" title="矩阵的运算"></a>矩阵的运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#矩阵的点积/内积</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>np.dot(A,B)<br></code></pre></td></tr></table></figure><p>​巧妙地使用NumPy数组，可以用很少的代码完成神经网络的前向处理。</p><h3 id="符号确认"><a href="#符号确认" class="headerlink" title="符号确认"></a>符号确认</h3><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/%E6%9D%83%E9%87%8D%E7%9A%84%E7%AC%A6%E5%8F%B7.png"></p><h3 id="各层间信号传递的实现"><a href="#各层间信号传递的实现" class="headerlink" title="各层间信号传递的实现"></a>各层间信号传递的实现</h3><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/%E4%BB%8E%E8%BE%93%E5%85%A5%E5%B1%82%E5%88%B0%E7%AC%AC1%E5%B1%82%E7%9A%84%E4%BF%A1%E5%8F%B7%E4%BC%A0%E9%80%92.png"></p><p>​上图增加了表示偏置的神经元“1”。请注意，偏置的右下角的索引号只有一个。这是因为前一层的偏置神经元（神经元“1”）只有一个。</p><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/%E4%BB%8E%E8%BE%93%E5%85%A5%E5%B1%82%E5%88%B0%E7%AC%AC1%E5%B1%82%E7%9A%84%E4%BF%A1%E5%8F%B7%E4%BC%A0%E9%80%921.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">X = np.array([<span class="hljs-number">1.0</span>, <span class="hljs-number">0.5</span>])<br>W1 = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>]])<br>B1 = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>])<br><br><span class="hljs-built_in">print</span>(W1.shape) <span class="hljs-comment"># (2, 3)</span><br><span class="hljs-built_in">print</span>(X.shape) <span class="hljs-comment"># (2,)</span><br><span class="hljs-built_in">print</span>(B1.shape) <span class="hljs-comment"># (3,)</span><br><br>A1 = np.dot(X, W1) + B1<br></code></pre></td></tr></table></figure><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/%E7%AC%AC1%E5%B1%82%E4%B8%AD%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E8%AE%A1%E7%AE%97.png"></p><p>​隐藏层的加权和（加权信号和偏置的总和）用a表示，被激活函数转换后的信号用z表示。此外，图中h()表示激活函数，这里我们使用的是sigmoid函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">Z1 = sigmoid(A1)<br><br><span class="hljs-built_in">print</span>(A1) <span class="hljs-comment"># [0.3, 0.7, 1.1]</span><br><span class="hljs-built_in">print</span>(Z1) <span class="hljs-comment"># [0.57444252, 0.66818777, 0.75026011]</span><br></code></pre></td></tr></table></figure><p>​第1层到第2层的信号传递如下：</p><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/%E7%AC%AC1%E5%B1%82%E5%88%B0%E7%AC%AC2%E5%B1%82%E7%9A%84%E4%BF%A1%E5%8F%B7%E4%BC%A0%E9%80%92.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">W2 = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.4</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.6</span>]])<br>B2 = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>])<br><br><span class="hljs-built_in">print</span>(Z1.shape) <span class="hljs-comment"># (3,)</span><br><span class="hljs-built_in">print</span>(W2.shape) <span class="hljs-comment"># (3, 2)</span><br><span class="hljs-built_in">print</span>(B2.shape) <span class="hljs-comment"># (2,)</span><br><br>A2 = np.dot(Z1, W2) + B2<br>Z2 = sigmoid(A2)<br><br></code></pre></td></tr></table></figure><p>​最后是第2层到输出层的信号传递:</p><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/%E7%AC%AC2%E5%B1%82%E5%88%B0%E8%BE%93%E5%87%BA%E5%B1%82%E7%9A%84%E4%BF%A1%E5%8F%B7%E4%BC%A0%E9%80%92.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">identity_function</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> x<br><br>W3 = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>]])<br>B3 = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>])<br><br>A3 = np.dot(Z2, W3) + B3<br>Y = identity_function(A3) <span class="hljs-comment"># 或者Y = A3</span><br></code></pre></td></tr></table></figure><p>​这里我们定义了identity_function()函数（也称为“恒等函数”），并将其作为输出层的激活函数。恒等函数会将输入按原样输出，因此，这个例子中没有必要特意定义identity_function()。这里这样实现只是为了和之前的流程保持统一。另外，输出层的激活函数用σ()表示，不同于隐藏层的激活函数h()（σ读作sigma）。</p><h3 id="代码小结"><a href="#代码小结" class="headerlink" title="代码小结"></a>代码小结</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">identity_function</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> x<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_network</span>():<br>    network = &#123;&#125;<br>    network[<span class="hljs-string">&#x27;W1&#x27;</span>] = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>]])<br>    network[<span class="hljs-string">&#x27;b1&#x27;</span>] = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>])<br>    network[<span class="hljs-string">&#x27;W2&#x27;</span>] = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.4</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.6</span>]])<br>    network[<span class="hljs-string">&#x27;b2&#x27;</span>] = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>])<br>    network[<span class="hljs-string">&#x27;W3&#x27;</span>] = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>]])<br>    network[<span class="hljs-string">&#x27;b3&#x27;</span>] = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>])<br><br>    <span class="hljs-keyword">return</span> network<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">network, x</span>):<br>    W1, W2, W3 = network[<span class="hljs-string">&#x27;W1&#x27;</span>], network[<span class="hljs-string">&#x27;W2&#x27;</span>], network[<span class="hljs-string">&#x27;W3&#x27;</span>]<br>    b1, b2, b3 = network[<span class="hljs-string">&#x27;b1&#x27;</span>], network[<span class="hljs-string">&#x27;b2&#x27;</span>], network[<span class="hljs-string">&#x27;b3&#x27;</span>]<br><br>    a1 = np.dot(x, W1) + b1<br>    z1 = sigmoid(a1)<br>    a2 = np.dot(z1, W2) + b2<br>    z2 = sigmoid(a2)<br>    a3 = np.dot(z2, W3) + b3<br>    y = identity_function(a3)<br><br>    <span class="hljs-keyword">return</span> y<br><br>network = init_network()<br>x = np.array([<span class="hljs-number">1.0</span>, <span class="hljs-number">0.5</span>])<br>y = forward(network, x)<br><span class="hljs-built_in">print</span>(y) <span class="hljs-comment"># [ 0.31682708 0.69627909]</span><br><br><br></code></pre></td></tr></table></figure><ul><li>init_network()函数进行权重和偏置的初始化，并将它们保存在字典变量network中。这个字典变量network中保存了每一层所需的参数（权重和偏置）。</li><li>forward()函数中封装了将输入信号转换为输出信号的处理过程。</li></ul><h2 id="输出层的设计"><a href="#输出层的设计" class="headerlink" title="输出层的设计"></a>输出层的设计</h2><p>​一般而言，回归问题用恒等函数，分类问题用softmax函数。</p><p>​机器学习的问题大致可以分为分类问题和回归问题。分类问题是数据属于哪一个类别的问题。而回归问题是根据某个输入预测一个（连续的）数值的问题。</p><h3 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h3><p>$$<br>\text{Softmax}(x_i) &#x3D; \frac{e^{x_i}}{\sum_{j} e^{x_j}}<br>$$</p><p>​exp(x)是表示的指数函数（e是纳皮尔常数2.7182 …）。上式表示假设输出层共有n个神经元，计算第k个神经元的输出。softmax函数的分子是输入信号的指数函数，分母是所有输入信号的指数函数的和。</p><p>​softmax函数的输出通过箭头与所有的输入信号相连。这是因为，从式子中可以看出，输出层的各个神经元都受到所有输入信号的影响。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">a</span>):<br>    exp_a = np.exp(a)<br>    sum_exp_a = np.<span class="hljs-built_in">sum</span>(exp_a)<br>    y = exp_a / sum_exp_a<br><br>    <span class="hljs-keyword">return</span> y<br><br></code></pre></td></tr></table></figure><h3 id="实现softmax函数时的注意事项"><a href="#实现softmax函数时的注意事项" class="headerlink" title="实现softmax函数时的注意事项"></a>实现softmax函数时的注意事项</h3><p>​softmax函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大。比如，e^10的值会超过20000，会变成一个后面有40多个0的超大值，e^100的结果会返回一个表示无穷大的inf。如果在这些超大值之间进行除法运算，结果会出现“不确定”的情况。</p><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/softmax%E5%87%BD%E6%95%B0%E7%9A%84%E6%94%B9%E8%BF%9B.png"></p><p>​式(3.11)说明，在进行softmax的指数函数的运算时，加上（或者减去）某个常数并不会改变运算的结果。这里的C’可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">a</span>):<br>    c = np.<span class="hljs-built_in">max</span>(a)<br>    exp_a = np.exp(a - c) <span class="hljs-comment"># 溢出对策</span><br>    sum_exp_a = np.<span class="hljs-built_in">sum</span>(exp_a)<br>    y = exp_a / sum_exp_a<br><br>    <span class="hljs-keyword">return</span> y<br><br></code></pre></td></tr></table></figure><h3 id="softmax函数的特征"><a href="#softmax函数的特征" class="headerlink" title="softmax函数的特征"></a>softmax函数的特征</h3><p>​softmax函数的输出是0.0到1.0之间的实数。并且，softmax函数的输出值的总和是1。输出总和为1是softmax函数的一个重要性质。正因为有了这个性质，我们才可以把softmax函数的输出解释为“概率”。</p><p>​一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此，神经网络在进行分类时，输出层的softmax函数可以省略。</p><p>​在实际的问题中，由于指数函数的运算需要一定的计算机运算量，因此输出层的softmax函数一般会被省略。</p><p>​求解机器学习问题的步骤可以分为“学习”和“推理”两个阶段。首先，在学习阶段进行模型的学习，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。</p><p>​如前所述，推理阶段一般会省略输出层的softmax函数。在输出层使用softmax函数是因为它和神经网络的学习有关系。</p><h3 id="输出层的神经元数量"><a href="#输出层的神经元数量" class="headerlink" title="输出层的神经元数量"></a>输出层的神经元数量</h3><p>​对于分类问题，输出层的神经元数量一般设定为类别的数量。</p><h2 id="手写数字识别"><a href="#手写数字识别" class="headerlink" title="手写数字识别"></a>手写数字识别</h2><p>​假设学习已经全部结束，我们使用学习到的参数，先实现神经网络的“推理处理”。这个推理处理也称为神经网络的前向传播(forward propagation)。</p><p>​和求解机器学习问题的步骤（分成学习和推理两个阶段进行）一样，使用神经网络解决问题时，也需要首先使用训练数据（学习数据）进行权重参数的学习；进行推理时，使用刚才学习到的参数，对输入数据进行分类。</p><h3 id="MNIST数据集"><a href="#MNIST数据集" class="headerlink" title="MNIST数据集"></a>MNIST数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#需要科学上网下载数据集</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir) <span class="hljs-comment"># 为了导入父目录中的文件而进行的设定</span><br><span class="hljs-keyword">from</span> dataset.mnist <span class="hljs-keyword">import</span> load_mnist<br><br><span class="hljs-comment"># 第一次调用会花费几分钟……</span><br>(x_train, t_train), (x_test, t_test) = load_mnist(flatten=<span class="hljs-literal">True</span>,<br>normalize=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># 输出各个数据的形状</span><br><span class="hljs-built_in">print</span>(x_train.shape) <span class="hljs-comment"># (60000, 784)</span><br><span class="hljs-built_in">print</span>(t_train.shape) <span class="hljs-comment"># (60000,)</span><br><span class="hljs-built_in">print</span>(x_test.shape) <span class="hljs-comment"># (10000, 784)</span><br><span class="hljs-built_in">print</span>(t_test.shape) <span class="hljs-comment"># (10000,)</span><br>·<br></code></pre></td></tr></table></figure><p>​load_mnist 函数以“（训练图像,训练标签），（测试图像,测试标签）”的形式返回读入的MNIST数据。</p><p>​还可以像load_mnist(normalize&#x3D;True, flatten&#x3D;True, one_hot_label&#x3D;False)这样，设置3个参数。</p><ul><li>第1个参数normalize设置是否将输入图像正规化为0.0～1.0的值。如果将该参数设置为False，则输入图像的像素会保持原来的0～255。</li><li>第2个参数flatten设置是否展开输入图像（变成一维数组）。如果将该参数设置为False，则输入图像为1×28×28的三维数组；若设置为True，则输入图像会保存为由784个元素构成的一维数组。</li><li>第3个参数one_hot_label设置是否将标签保存为one-hot表示(one-hot representation)。one-hot表示是仅正确解标签为1，其余皆为0的数组，就像[0,0,1,0,0,0,0,0,0,0]这样。当one_hot_label为False时，只是像7、2这样简单保存正确解标签；当one_hot_label为True时，标签则保存为one-hot表示。</li></ul><p>​Python有pickle这个便利的功能。这个功能可以将程序运行中的对象保存为文件。如果加载保存过的pickle文件，可以立刻复原之前程序运行中的对象。用于读入MNIST数据集的load_mnist()函数内部也使用了pickle功能（在第2次及以后读入时）。利用pickle功能，可以高效地完成MNIST数据的准备工作。</p><p>​试着显示MNIST图像：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir)<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> dataset.mnist <span class="hljs-keyword">import</span> load_mnist<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">img_show</span>(<span class="hljs-params">img</span>):<br>    pil_img = Image.fromarray(np.uint8(img))<br>    pil_img.show()<br><br>(x_train, t_train), (x_test, t_test) = load_mnist(flatten=<span class="hljs-literal">True</span>,<br>normalize=<span class="hljs-literal">False</span>)<br>img = x_train[<span class="hljs-number">0</span>]<br>label = t_train[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(label) <span class="hljs-comment"># 5</span><br><br><span class="hljs-built_in">print</span>(img.shape)          <span class="hljs-comment"># (784,)</span><br>img = img.reshape(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>) <span class="hljs-comment"># 把图像的形状变成原来的尺寸</span><br><span class="hljs-built_in">print</span>(img.shape)          <span class="hljs-comment"># (28, 28)</span><br><br>img_show(img)<br><br></code></pre></td></tr></table></figure><p>​flatten&#x3D;True时读入的图像是以一列（一维）NumPy数组的形式保存的。因此，显示图像时，需要把它变为原来的28像素×28像素的形状。可以通过reshape()方法的参数指定期望的形状，更改NumPy数组的形状。此外，还需要把保存为NumPy数组的图像数据转换为PIL用的数据对象，这个转换处理由Image.fromarray()来完成。</p><h3 id="神经网络的推理处理"><a href="#神经网络的推理处理" class="headerlink" title="神经网络的推理处理"></a>神经网络的推理处理</h3><p>​对这个MNIST数据集实现神经网络的推理处理。神经网络的输入层有784个神经元，输出层有10个神经元。输入层的784这个数字来源于图像大小的28×28 &#x3D; 784，输出层的10这个数字来源于10类别分类（数字0到9，共10类别）。此外，这个神经网络有2个隐藏层，第1个隐藏层有50个神经元，第2个隐藏层有100个神经元。这个50和100可以设置为任何值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># coding: utf-8</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir)  <span class="hljs-comment"># 为了导入父目录的文件而进行的设定</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">from</span> dataset.mnist <span class="hljs-keyword">import</span> load_mnist<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">if</span> x.ndim == <span class="hljs-number">2</span>:<br>        x = x.T<br>        x = x - np.<span class="hljs-built_in">max</span>(x, axis=<span class="hljs-number">0</span>)<br>        y = np.exp(x) / np.<span class="hljs-built_in">sum</span>(np.exp(x), axis=<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> y.T<br><br>    x = x - np.<span class="hljs-built_in">max</span>(x) <span class="hljs-comment"># 溢出对策</span><br>    <span class="hljs-keyword">return</span> np.exp(x) / np.<span class="hljs-built_in">sum</span>(np.exp(x))<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_data</span>():<br>    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="hljs-literal">True</span>, flatten=<span class="hljs-literal">True</span>, one_hot_label=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">return</span> x_test, t_test<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_network</span>():<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;sample_weight.pkl&quot;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        network = pickle.load(f)<br>    <span class="hljs-keyword">return</span> network<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">network, x</span>):<br>    W1, W2, W3 = network[<span class="hljs-string">&#x27;W1&#x27;</span>], network[<span class="hljs-string">&#x27;W2&#x27;</span>], network[<span class="hljs-string">&#x27;W3&#x27;</span>]<br>    b1, b2, b3 = network[<span class="hljs-string">&#x27;b1&#x27;</span>], network[<span class="hljs-string">&#x27;b2&#x27;</span>], network[<span class="hljs-string">&#x27;b3&#x27;</span>]<br><br>    a1 = np.dot(x, W1) + b1<br>    z1 = sigmoid(a1)<br>    a2 = np.dot(z1, W2) + b2<br>    z2 = sigmoid(a2)<br>    a3 = np.dot(z2, W3) + b3<br>    y = softmax(a3)<br><br>    <span class="hljs-keyword">return</span> y<br><br><br>x, t = get_data()<br>network = init_network()<br>accuracy_cnt = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x)):<br>    y = predict(network, x[i])<br>    p= np.argmax(y) <span class="hljs-comment"># 获取概率最高的元素的索引</span><br>    <span class="hljs-keyword">if</span> p == t[i]:<br>        accuracy_cnt += <span class="hljs-number">1</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy:&quot;</span> + <span class="hljs-built_in">str</span>(<span class="hljs-built_in">float</span>(accuracy_cnt) / <span class="hljs-built_in">len</span>(x)))<br></code></pre></td></tr></table></figure><p>​init_network()会读入保存在pickle文件sample_weight.pkl中的学习到的权重参数。这个文件中以字典变量的形式保存了权重和偏置参数。</p><p>​现在，我们用这3个函数来实现神经网络的推理处理。然后，评价它的识别精度(accuracy)，即能在多大程度上正确分类。</p><p>​predict()函数以NumPy数组的形式输出各个标签对应的概率。比如输出[0.1, 0.3, 0.2, …, 0.04]的数组，该数组表示“0”的概率为0.1，“1”的概率为0.3，等等。然后，我们取出这个概率列表中的最大值的索引（第几个元素的概率最高），作为预测结果。可以用np.argmax(x)函数取出数组中的最大值的索引，np.argmax(x)将获取被赋给参数x的数组中的最大值元素的索引。最后，比较神经网络所预测的答案和正确解标签，将回答正确的概率作为识别精度。</p><p>​在这个例子中，我们把load_mnist函数的参数normalize设置成了True。将normalize设置成True后，函数内部会进行转换，将图像的各个像素值除以255，使得数据的值在0.0～1.0的范围内。像这样把数据限定到某个范围内的处理称为正规化(normalization)。此外，对神经网络的输入数据进行某种既定的转换称为预处理(pre-processing)。这里，作为对输入图像的一种预处理，我们进行了正规化。</p><h2 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h2><p>​这种打包式的输入数据称为批(batch)。</p><p>​批处理对计算机的运算大有利处，可以大幅缩短每张图像的处理时间。</p><p>​为什么批处理可以缩短处理时间呢？这是因为大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化。并且，在神经网络的运算中，当数据传送成为瓶颈时，批处理可以减轻数据总线的负荷（严格地讲，相对于数据读入，可以将更多的时间用在计算上）。也就是说，批处理一次性计算大型数组要比分开逐步计算各个小型数组速度更快。</p><p>​我们给定了参数axis&#x3D;1。这指定了在100×10的数组中，沿着第1维方向（以第1维为轴）找到值最大的元素的索引（第0维对应第1个维度）。</p><p>​使用批处理，可以实现高速且高效的运算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">x, t = get_data()<br>network = init_network()<br><br>batch_size = <span class="hljs-number">100</span> <span class="hljs-comment"># 批数量</span><br>accuracy_cnt = <span class="hljs-number">0</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(x), batch_size):<br>    x_batch = x[i:i+batch_size]<br>    y_batch = predict(network, x_batch)<br>    p = np.argmax(y_batch, axis=<span class="hljs-number">1</span>)<br>    accuracy_cnt += np.<span class="hljs-built_in">sum</span>(p == t[i:i+batch_size])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># coding: utf-8</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir)  <span class="hljs-comment"># 为了导入父目录的文件而进行的设定</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">from</span> dataset.mnist <span class="hljs-keyword">import</span> load_mnist<br><span class="hljs-keyword">from</span> functions <span class="hljs-keyword">import</span> sigmoid, softmax<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_data</span>():<br>    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="hljs-literal">True</span>, flatten=<span class="hljs-literal">True</span>, one_hot_label=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">return</span> x_test, t_test<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_network</span>():<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;sample_weight.pkl&quot;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        network = pickle.load(f)<br>    <span class="hljs-keyword">return</span> network<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">network, x</span>):<br>    w1, w2, w3 = network[<span class="hljs-string">&#x27;W1&#x27;</span>], network[<span class="hljs-string">&#x27;W2&#x27;</span>], network[<span class="hljs-string">&#x27;W3&#x27;</span>]<br>    b1, b2, b3 = network[<span class="hljs-string">&#x27;b1&#x27;</span>], network[<span class="hljs-string">&#x27;b2&#x27;</span>], network[<span class="hljs-string">&#x27;b3&#x27;</span>]<br><br>    a1 = np.dot(x, w1) + b1<br>    z1 = sigmoid(a1)<br>    a2 = np.dot(z1, w2) + b2<br>    z2 = sigmoid(a2)<br>    a3 = np.dot(z2, w3) + b3<br>    y = softmax(a3)<br><br>    <span class="hljs-keyword">return</span> y<br><br><br>x, t = get_data()<br>network = init_network()<br><br>batch_size = <span class="hljs-number">100</span> <span class="hljs-comment"># 批数量</span><br>accuracy_cnt = <span class="hljs-number">0</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(x), batch_size):<br>    x_batch = x[i:i+batch_size]<br>    y_batch = predict(network, x_batch)<br>    p = np.argmax(y_batch, axis=<span class="hljs-number">1</span>)<br>    accuracy_cnt += np.<span class="hljs-built_in">sum</span>(p == t[i:i+batch_size])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy:&quot;</span> + <span class="hljs-built_in">str</span>(<span class="hljs-built_in">float</span>(accuracy_cnt) / <span class="hljs-built_in">len</span>(x)))<br><br></code></pre></td></tr></table></figure><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>​神经网络和感知机在信号的按层传递这一点上是相同的，但是，向下一个神经元发送信号时，改变信号的激活函数有很大差异。神经网络中使用的是平滑变化的sigmoid函数，而感知机中使用的是信号急剧变化的阶跃函数。这个差异对于神经网络的学习非常重要。</p><ul><li>神经网络中的激活函数使用平滑变化的sigmoid函数或ReLU函数。</li><li>通过巧妙地使用NumPy多维数组，可以高效地实现神经网络。</li><li>机器学习的问题大体上可以分为回归问题和分类问题。</li><li>关于输出层的激活函数，回归问题中一般用恒等函数，分类问题中一般用softmax函数。</li><li>分类问题中，输出层的神经元的数量设置为要分类的类别数</li><li>·输入数据的集合称为批。通过以批为单位进行推理处理，能够实现高速的运算。</li></ul>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI-Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Win7环境下VMareTools的安装</title>
    <link href="/2025/03/04/win7-vmare-tools/"/>
    <url>/2025/03/04/win7-vmare-tools/</url>
    
    <content type="html"><![CDATA[<h1 id="安装VMare-Tools"><a href="#安装VMare-Tools" class="headerlink" title="安装VMare Tools"></a>安装VMare Tools</h1><h2 id="安装流程"><a href="#安装流程" class="headerlink" title="安装流程"></a>安装流程</h2><p>1、在VMare中启动win7后，点击VMare主界面的**虚拟机(M)**选项，启动VMare Tools的安装;</p><p><img src="/2025/03/04/win7-vmare-tools/vmare_tools_dow_1.png"></p><p><img src="/2025/03/04/win7-vmare-tools/vmare_tools_dow_2.png"></p><p><img src="/2025/03/04/win7-vmare-tools/vmare_tools_dow_3.png"></p><p>2、点击允许 setup64.exe，一路下一步直到完成安装；</p><p><img src="/2025/03/04/win7-vmare-tools/vmare_tools_dow_4.png"></p><h2 id="安装失败的问题"><a href="#安装失败的问题" class="headerlink" title="安装失败的问题"></a>安装失败的问题</h2><p>​安装程序中途报错，显示无法自动安装 Virtual Machine Communication Interface Sockets (VSock) 驱动程序，必须手动安装此驱动程序。</p><p><img src="/2025/03/04/win7-vmare-tools/vmare_tools_dow_5.png"></p><p>​原因是微软更新了驱动程序的签名算法，从 2019 年初开始，逐步弃用SHA-1，改为SHA-2。可以通过安装补丁来解决这个问题。</p><p>​可以从 Microsoft Update Catalog 下载 KB4474419 和 KB4490628 这两个补丁，然后安装到 Win7 虚拟机中。在没有成功安装 VMware Tools的情况下，传文件不太方便，可以用虚拟机里浏览器访问下载页面，然后直接在虚拟机里下载安装。下载地址：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim">https://www.catalog.<span class="hljs-keyword">update</span>.microsoft.<span class="hljs-keyword">com</span>/<span class="hljs-built_in">search</span>.aspx?q=kb4474419<br></code></pre></td></tr></table></figure><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim">https://www.catalog.<span class="hljs-keyword">update</span>.microsoft.<span class="hljs-keyword">com</span>/<span class="hljs-built_in">search</span>.aspx?q=<span class="hljs-number">4490628</span><br></code></pre></td></tr></table></figure><p>​安装补丁后，重启虚拟机，然后重新 VMware Tools 即可。</p><p>​</p><p>​或者使用附录的ISO文件进行补丁安装。</p><h2 id="附录文件"><a href="#附录文件" class="headerlink" title="附录文件"></a>附录文件</h2><p>1、<a href="https://dl.google.com/release2/chrome/acihtkcueyye3ymoj2afvv7ulzxa_109.0.5414.120/109.0.5414.120_chrome_installer.exe">适合win7的32位Google </a></p><p>2、<a href="ISO%E8%A1%A5%E4%B8%81.zip">下载 ISO </a></p><p>3、<a href="ISO%E8%A1%A5%E4%B8%81.z01">下载 ISO 1</a></p><p>4、<a href="ISO%E8%A1%A5%E4%B8%81.z02">下载 ISO 2</a></p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>1、<a href="https://ihacksoft.com/archive/1397.html">完美解决：安装程序无法自动安装 Virtual Machine Communication…… - 嗨软</a></p><p>2、<a href="https://www.catalog.update.microsoft.com/search.aspx?q=kb4474419">https://www.catalog.update.microsoft.com/search.aspx?q=kb4474419</a></p><p>3、<a href="https://blog.csdn.net/Drug_/article/details/141402451">Chrome 浏览器 Windows 7 最终版下载（官方原版）_chrome win7-CSDN博客</a></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>Env_Config</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VMaretools</tag>
      
      <tag>win7</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DS的基础使用</title>
    <link href="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/"/>
    <url>/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="AI原生应用市场"><a href="#AI原生应用市场" class="headerlink" title="AI原生应用市场"></a>AI原生应用市场</h2><ul><li>国产大模型迭代和应用加速（技术突破：强化学习和混合专家架构结合）；</li><li>端侧和边缘计算应用繁荣；</li><li>智能安全和安全智能。</li></ul><h2 id="中小型企业"><a href="#中小型企业" class="headerlink" title="中小型企业"></a>中小型企业</h2><ul><li>价格竞争；</li><li>生态选择；</li><li>场景应用。</li></ul><h2 id="政府及头部企业市场"><a href="#政府及头部企业市场" class="headerlink" title="政府及头部企业市场"></a>政府及头部企业市场</h2><ul><li>智算中心+专业智能体；</li><li>企业级安全与合规保障；</li><li>终端及边缘智能化。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>破除算力霸权；</li><li>商业模型颠覆（NaaS:模型即服务）；</li><li>技术平权。</li></ul><h1 id="基础介绍"><a href="#基础介绍" class="headerlink" title="基础介绍"></a>基础介绍</h1><h2 id="ChatGPT"><a href="#ChatGPT" class="headerlink" title="ChatGPT"></a>ChatGPT</h2><ul><li>“Chat”代表聊天；</li><li>“G”（Generative）代表生成式；</li><li>“P”（Pre-trained）代表预训练；</li><li>“T”<strong>（Transformer）代表Transformer架构</strong>。</li><li>在GPT (Generative Pre-trained Transformer)中，生成式 (generative)意味着这个模型能够生成新的文本序列。</li></ul><h2 id="影响模型的三要素"><a href="#影响模型的三要素" class="headerlink" title="影响模型的三要素"></a>影响模型的三要素</h2><ul><li>算力；</li><li>算法；</li><li>数据。</li></ul><h2 id="提示词框架：BRROKE（融合OKR）"><a href="#提示词框架：BRROKE（融合OKR）" class="headerlink" title="提示词框架：BRROKE（融合OKR）"></a>提示词框架：BRROKE（融合OKR）</h2><ol><li>背景：提供足够的背景信息，使得GPT能够理解问题的上下文；</li><li>角色：设定特定的角色，让GPT能根据该角色来生成响应；</li><li>目标：明确任务目标，让GPT清楚知道需要实现什么；</li><li>要点：定义关键的、可以衡量的结果，以便让GPT知道如何衡量目标的完成情况；</li><li>测试：1次不对，就鼓励AI分步骤，多试几次。通过调整来测试结果，并根据需要进行优化。</li></ol><h2 id="五个技巧"><a href="#五个技巧" class="headerlink" title="五个技巧"></a>五个技巧</h2><ul><li>定目标——抛弃套路，直抒胸臆，但是注意细节；<ul><li>通用公式：我要XX，要给XX用，希望达到XX效果，但是担心XX问题。</li></ul></li><li>说人话；</li><li>补数据——数据类型包括文案、示例、PDF、Excel等，用来给AI补充背景信息；</li><li>勤反问——发挥人的主动性；</li><li>善联动。</li></ul><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E5%AE%9A%E7%9B%AE%E6%A0%87%E7%9A%84%E7%BB%86%E8%8A%82.png"></p><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E8%AF%B4%E4%BA%BA%E8%AF%9D.png"></p><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E8%A1%A5%E6%95%B0%E6%8D%AE.png"></p><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E9%AB%98%E7%BA%A7%E6%90%9C%E7%B4%A2%E6%8A%80%E5%B7%A7.png"></p><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E5%8B%A4%E5%8F%8D%E9%97%AE.png"></p><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E5%AE%9E%E6%93%8D.png"></p><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E4%B8%8EAI%E6%B2%9F%E9%80%9A.png"></p><h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><ul><li>秘塔AI搜索；</li><li>纳米AI搜索；</li><li>DeepSeek +阿里通义 快速生成PPT模板；</li><li>DeepSeek +即梦AI 完成AI绘画；</li><li>AI精选工具库</li></ul><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E4%B8%8D%E5%90%8CAI%E7%9A%84%E4%BC%98%E5%8A%BF.png"></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI-Use</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>个人博客搭建流程</title>
    <link href="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/"/>
    <url>/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="一、前期准备"><a href="#一、前期准备" class="headerlink" title="一、前期准备"></a>一、前期准备</h1><h2 id="1、github账号"><a href="#1、github账号" class="headerlink" title="1、github账号"></a>1、github账号</h2><h2 id="2、git安装"><a href="#2、git安装" class="headerlink" title="2、git安装"></a>2、git安装</h2><p>配置</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">git config <span class="hljs-attr">--global</span> user<span class="hljs-selector-class">.name</span> <span class="hljs-string">&quot;Your Name&quot;</span><br>git config <span class="hljs-attr">--global</span> user<span class="hljs-selector-class">.email</span> <span class="hljs-string">&quot;email@example.com&quot;</span><br></code></pre></td></tr></table></figure><h2 id="3、NodeJS安装和（可选）配置"><a href="#3、NodeJS安装和（可选）配置" class="headerlink" title="3、NodeJS安装和（可选）配置"></a>3、NodeJS安装和（可选）配置</h2><p>备注：以管理员身份打开cmd窗口</p><ol><li><p><a href="https://nodejs.org/zh-cn/download/">NodeJS官网下载</a>，安装路径可以选择自定义路径，如D:\myweb\NodeJs</p></li><li><p>验证NodeJS安装，即查看node和npm版本</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs crmsh"><span class="hljs-keyword">node</span> <span class="hljs-title">-v</span><br>npm -v<br></code></pre></td></tr></table></figure></li><li><p>在安装目录（D:\myweb\NodeJs）下创建两个文件夹，<code>node_global</code> 存放全局包，<code>node_cache</code> 存放node缓存和log记录</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">npm root -<span class="hljs-selector-tag">g</span> <br><span class="hljs-comment">//查看当前路径</span><br></code></pre></td></tr></table></figure></li><li><p>在cmd命令行中执行如下两条命令</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros">npm<span class="hljs-built_in"> config </span><span class="hljs-built_in">set</span><span class="hljs-built_in"> prefix </span><span class="hljs-string">&quot;D:\xgweb\NodeJs\node_global&quot;</span><br><br>npm<span class="hljs-built_in"> config </span><span class="hljs-built_in">set</span> cache <span class="hljs-string">&quot;D:\xgweb\NodeJs\node_cache&quot;</span><br><br></code></pre></td></tr></table></figure></li><li><p>设置电脑环境变量，右键 “我的电脑”&#x3D;》属性&#x3D;》高级系统设置&#x3D;》环境变量；进入环境变量对话框，在【系统变量】中新建环境变量 <code>NODE_PATH</code>，值为 <code>D:\xgweb\NodeJs\node_global\node_modules</code>，其中 <code>D:\xgweb\NodeJs\node_global</code> 是新创建的全局模块安装路径；修改【用户变量】中的 <code>path</code> 变量，将 <code>C:\Users\..\AppData\Roaming\npm</code> 修改为<code>D:\xgweb\NodeJs\node_global</code></p></li><li><p>重启电脑，使配置生效</p></li></ol><h1 id="二、创建仓库"><a href="#二、创建仓库" class="headerlink" title="二、创建仓库"></a>二、创建仓库</h1><ol><li><p>在<code>GitHub</code>上创建一个新的代码仓库用于保存我们的网页。</p></li><li><p>点击<code>Your repositories</code>，进入仓库页面。点击<code>New</code>按钮，进入仓库创建页面，填写仓库名，格式必须为<code>&lt;用户名&gt;.github.io</code>，然后点击<code>Create repository</code>。</p></li><li><p>点击<code>creating a new file</code>创建一个新文件，作为我们网站的主页。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-meta">&lt;!DOCTYPE <span class="hljs-keyword">html</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">html</span> <span class="hljs-attr">lang</span>=<span class="hljs-string">&quot;en&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">head</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">charset</span>=<span class="hljs-string">&quot;UTF-8&quot;</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">title</span>&gt;</span>111<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">head</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">body</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">h1</span>&gt;</span>111的个人主页<span class="hljs-tag">&lt;/<span class="hljs-name">h1</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">h1</span>&gt;</span>Hello ~<span class="hljs-tag">&lt;/<span class="hljs-name">h1</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">body</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">html</span>&gt;</span><br><br></code></pre></td></tr></table></figure></li><li><p>在浏览器中访问成功</p></li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/page1.png"></p><h1 id="三、安装Hexo"><a href="#三、安装Hexo" class="headerlink" title="三、安装Hexo"></a>三、安装Hexo</h1><ol><li><p>安装Hexo(cmd管理员模式下)和查看版本</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs avrasm">npm install -g hexo-<span class="hljs-keyword">cli</span><br>hexo -v<br></code></pre></td></tr></table></figure><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo_download.png"></p></li><li><p>创建一个项目并初始化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo init hexo-blog<br><span class="hljs-built_in">cd</span> hexo-blog<br>npm install<br>//cmd管理员模式<br></code></pre></td></tr></table></figure><p>as</p></li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo_init_1.png"></p><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/npm_install.png"></p><p>其内部文件结构如下（红框内为npm install 安装的）：</p><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo-doc.png"></p><ol start="3"><li>本地启动<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs css">hexo <span class="hljs-selector-tag">g</span><br>hexo s<br></code></pre></td></tr></table></figure><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo_start.png"></li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo-doc-1.png"></p><p>如上所示，public文件夹内为新生成的web文件，共计11项。</p><ol start="4"><li>浏览器访问 <a href="http://localhost:4000/">http://localhost:4000</a></li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-1.png"></p><h1 id="四、更换主题"><a href="#四、更换主题" class="headerlink" title="四、更换主题"></a>四、更换主题</h1><h2 id="1、NexT主题"><a href="#1、NexT主题" class="headerlink" title="1、NexT主题"></a>1、NexT主题</h2><ol><li>下载NexT主题</li></ol><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs autoit">git clone git<span class="hljs-symbol">@github</span>.com:iissnan/hexo-theme-<span class="hljs-keyword">next</span>.git themes/<span class="hljs-keyword">next</span><br></code></pre></td></tr></table></figure><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/clone_next.png"></p><ol start="2"><li>打开 _config.yml 文件，该文件为站点配置文件</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/theme_set_next.png"></p><ol start="3"><li>本地启动，出现以下显示问题</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/next_error.png"></p><ol start="4"><li><p>原因是hexo在5.0之后把swig给删除了需要自己手动安装</p><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs coffeescript"><span class="hljs-built_in">npm</span> i hexo-renderer-swig<br></code></pre></td></tr></table></figure><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/npm_swig.png"></p></li><li><p>重新启动</p></li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-next.png"></p><p>参考：<a href="https://github.com/iissnan/hexo-theme-next/issues/2253">https://github.com/iissnan/hexo-theme-next/issues/2253</a></p><h2 id="2、Fluid主题"><a href="#2、Fluid主题" class="headerlink" title="2、Fluid主题"></a>2、Fluid主题</h2><ol><li>安装主题</li></ol><p>下载 <a href="https://github.com/fluid-dev/hexo-theme-fluid/releases">最新 release 版本</a> 解压到 <code>themes</code> 目录，并将解压出的文件夹重命名为 <code>fluid</code></p><ol start="2"><li>指定主题</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/theme_set_fluid.png"></p><ol start="3"><li><p>创建[关于页]</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs actionscript">hexo <span class="hljs-keyword">new</span> page about<br></code></pre></td></tr></table></figure><p>编辑博客目录下 <code>/source/about/index.md</code>，添加 <code>layout</code> 属性。</p></li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo_new_page_about.png"></p><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/page_about.png"></p><ol start="4"><li>重新启动</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-fluid.png"></p><h1 id="五、创建文章"><a href="#五、创建文章" class="headerlink" title="五、创建文章"></a>五、创建文章</h1><h2 id="1、文章创建和图片插入"><a href="#1、文章创建和图片插入" class="headerlink" title="1、文章创建和图片插入"></a>1、文章创建和图片插入</h2><ol><li>修改 Hexo 博客目录中的 <code>_config.yml</code>，打开这个配置是为了在生成文章的时候生成一个同名的资源目录用于存放图片文件。</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/config_set.png"></p><ol start="2"><li>创建一篇新文章，名为《测试文章》</li></ol><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">hexo <span class="hljs-built_in">new</span> <span class="hljs-built_in">post</span> 测试文章<br></code></pre></td></tr></table></figure><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/config_set_display.png"></p><ol start="3"><li>在md文件中插入图片的常用三种方式</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><br>&#123;<span class="hljs-string">%</span> <span class="hljs-string">asset_img</span> <span class="hljs-string">test.png</span> <span class="hljs-string">图片引用方法一</span> <span class="hljs-string">%</span>&#125;<br><br><br><span class="hljs-type">![</span><span class="hljs-string">图片引用方法二](test.png)</span><br><br><span class="hljs-string">需要在配置文件添加：</span><br><span class="hljs-attr">marked:</span><br>  <span class="hljs-attr">prependRoot:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">postAsset:</span> <span class="hljs-literal">true</span><br><br><br><br><span class="hljs-type">![</span><span class="hljs-string">图片引用方法三](/images/test.png)</span><br><span class="hljs-string">//images文件在在\source\images目录下</span><br><br></code></pre></td></tr></table></figure><p>如下分别是：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs maxima">![](/测试文章/BA.jpg)<br>![](BA.jpg)<br>&#123;<span class="hljs-symbol">%</span> asset_img BA.jpg This <span class="hljs-built_in">is</span> an <span class="hljs-built_in">example</span> <span class="hljs-built_in">image</span> <span class="hljs-symbol">%</span>&#125;<br>&#123;<span class="hljs-symbol">%</span> asset_img <span class="hljs-string">&quot;BA.jpg&quot;</span> <span class="hljs-string">&quot;spaced title&quot;</span> <span class="hljs-symbol">%</span>&#125;<br></code></pre></td></tr></table></figure><p>对于如上四种中的第二种图片引用方法，需要对配置文件进行以下修改：</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-params">post_asset_folder:</span> <span class="hljs-literal">true</span><br><span class="hljs-params">marked:</span><br>  <span class="hljs-params">prependRoot:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-params">postAsset:</span> <span class="hljs-literal">true</span><br><br></code></pre></td></tr></table></figure><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-2-1.png"></p><h2 id="2、md文件图片同步显示"><a href="#2、md文件图片同步显示" class="headerlink" title="2、md文件图片同步显示"></a>2、md文件图片同步显示</h2><p>如果需要同时在md文件显示图片，需要安装<strong>hexo-asset-image</strong> 插件</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">npm install hexo-asset-<span class="hljs-selector-tag">image</span> <span class="hljs-attr">--save</span><br></code></pre></td></tr></table></figure><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo_asset_image_dow.png"></p><p>重新启动</p><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-2-2.png"></p><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-2.png"></p><p>为什么会出现以下 &#x2F;.com&#x2F;&#x2F; 情况，其实是 <strong>hexo-asset-image</strong> 插件的bug</p><h3 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs verilog">npm uninstall hexo-asset-image<br>npm install https:<span class="hljs-comment">//github.com/CodeFalling/hexo-asset-image</span><br>hexo clean<br>hexo <span class="hljs-keyword">generate</span><br>hexo server <br></code></pre></td></tr></table></figure><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo_asset_image_dow_1.png"></p><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-2-3.png"></p><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-2-3-1.png"></p><p>此时可以看出其他三种方法的解析路径出现多余的&#x2F;02&#x2F;10</p><h3 id="方案二"><a href="#方案二" class="headerlink" title="方案二"></a>方案二</h3><p>对hexo-asset-image（第一版）文件夹中的index.js中的文件进行如下修改：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-comment">//$(this).attr(&#x27;src&#x27;, config.root + link + src); </span><br>$(this)<span class="hljs-selector-class">.attr</span>(<span class="hljs-string">&#x27;src&#x27;</span>, data<span class="hljs-selector-class">.permalink</span><span class="hljs-selector-class">.split</span>(<span class="hljs-string">&#x27;example.com&#x27;</span>)<span class="hljs-selector-attr">[1]</span>+ <span class="hljs-attribute">src</span>);<br></code></pre></td></tr></table></figure><p>其问题如上图所示</p><p>参考：<a href="https://blog.csdn.net/sluck_0430/article/details/136431303">hexo图片显示不出且图片路径错误&#x2F;.com&#x2F;&#x2F;_hexo显示不了图片-CSDN博客</a></p><h3 id="方案三"><a href="#方案三" class="headerlink" title="方案三"></a>方案三</h3><p>对hexo-asset-image（第一版）文件夹中的index.js中的文件进行如下修改（第23行的else分支）：</p><figure class="highlight cal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cal">//<span class="hljs-keyword">var</span> endPos = link.lastIndexOf(<span class="hljs-string">&#x27;.&#x27;</span>);<br><span class="hljs-keyword">var</span> endPos = link.lastIndexOf(<span class="hljs-string">&#x27;/&#x27;</span>);<br></code></pre></td></tr></table></figure><p>其问题依然如上图所示</p><p>参考：<a href="https://blog.asroads.com/post/95d84581.html">Hexo生成博文插入图片 | Asroads’Blog</a></p><p>最终决定卸载hexo-asset-image插件</p><h1 id="六、个性化页面展示"><a href="#六、个性化页面展示" class="headerlink" title="六、个性化页面展示"></a>六、个性化页面展示</h1><h2 id="博客标题"><a href="#博客标题" class="headerlink" title="博客标题"></a>博客标题</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-params">navbar:</span><br>  <span class="hljs-comment"># 导航栏左侧的标题，为空则按 hexo config 中 `title` 显示</span><br>  <span class="hljs-comment"># The title on the left side of the navigation bar. If empty, it is based on `title` in hexo config</span><br>  <span class="hljs-comment">#blog_title: &quot;Fluid&quot;</span><br>  <span class="hljs-params">blog_title:</span> <span class="hljs-string">&quot;blackstarry6&quot;</span><br></code></pre></td></tr></table></figure><h2 id="主页正中间的文字"><a href="#主页正中间的文字" class="headerlink" title="主页正中间的文字"></a>主页正中间的文字</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-params">slogan:</span><br>  <span class="hljs-params">enable:</span> <span class="hljs-literal">true</span><br><br>  <span class="hljs-comment"># 为空则按 hexo config.subtitle 显示</span><br>  <span class="hljs-comment"># If empty, text based on `subtitle` in hexo config</span><br>  <span class="hljs-comment">#text: &quot;An elegant Material-Design theme for Hexo&quot;</span><br>  <span class="hljs-params">text:</span> <span class="hljs-string">&quot;XXX 个人博客&quot;</span><br></code></pre></td></tr></table></figure><h2 id="主页展示"><a href="#主页展示" class="headerlink" title="主页展示"></a>主页展示</h2><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-fluid-1.png"></p><h1 id="七、发布到Github-Pages"><a href="#七、发布到Github-Pages" class="headerlink" title="七、发布到Github Pages"></a>七、发布到Github Pages</h1><p>安装hexo-deployer-git</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ada">npm install hexo-deployer-git <span class="hljs-comment">--save</span><br><br></code></pre></td></tr></table></figure><p>修改配置文件，其中 <code>token</code> 为 <code>GitHub</code> 的 <code>Personal access tokens</code></p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">deploy:</span><br><span class="hljs-symbol">  type:</span> git<br><span class="hljs-symbol">  repo:</span> XXX<br><span class="hljs-symbol">  branch:</span> main<br><span class="hljs-symbol">  token:</span> XXX<br><br></code></pre></td></tr></table></figure><p>部署到github</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">hexo <span class="hljs-selector-tag">g</span> -d<br></code></pre></td></tr></table></figure><p>参考：</p><p><a href="https://blog.csdn.net/yaorongke/article/details/119089190">GitHub Pages + Hexo搭建个人博客网站，史上最全教程_hexo博客-CSDN博客</a></p><p><a href="https://blog.csdn.net/yaorongke/article/details/119085413">Git安装(Windows)_windows git安装包-CSDN博客</a></p><p><a href="https://blog.csdn.net/yaorongke/article/details/119084295">NodeJS安装及配置(Windows)_welcome to node.js v14.21.3. type “.help” for more-CSDN博客</a></p><p><a href="https://blog.csdn.net/zimeng303/article/details/112167688">NodeJs 的安装及配置环境变量_nodejs配置环境变量-CSDN博客</a></p><p><a href="https://blog.csdn.net/m0_43401436/article/details/107191688">hexo博客中插入图片失败——解决思路及个人最终解决办法_hexo 文章插入图片失败-CSDN博客</a></p><h1 id="附录：SSH配置"><a href="#附录：SSH配置" class="headerlink" title="附录：SSH配置"></a>附录：SSH配置</h1><ol><li>生成密钥对</li></ol><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs armasm"><span class="hljs-symbol">ssh</span>-keygen -t ed25519 -C <span class="hljs-string">&quot;your_email@example.com&quot;</span><br><br><span class="hljs-comment">//如果您使用的是不支持 Ed25519 算法的旧版系统，请使用</span><br><span class="hljs-symbol">ssh</span>-keygen -t rsa -<span class="hljs-keyword">b</span> <span class="hljs-number">4096</span> -C <span class="hljs-string">&quot;your_email@example.com&quot;</span><br></code></pre></td></tr></table></figure><ol start="2"><li><p>在.&#x2F;ssh文件下查看生成的密钥对，复制.pub内的公钥内容</p></li><li><p>Github账号上添加公钥</p></li></ol><p><code>Settings</code>&#x3D;&gt;<code>SSH and GPG keys</code>&#x3D;&gt;<code>New SSH key</code>&#x3D;&gt;<code>Key</code>，在title中简单描述一下</p><ol start="4"><li>验证是否成功，即连接github服务器，此时出现如下情况：</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/ssh-1.png"></p><ol start="5"><li>换成443端口：</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/ssh-2.png"></p><ol start="6"><li>对.&#x2F;ssh文件中的config文件（没有的话自己建立）进行如下修改：</li></ol><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># Add section below to it</span><br><span class="hljs-attribute">Host</span> github.com<br>  <span class="hljs-attribute">Hostname</span> ssh.github.com<br>  <span class="hljs-attribute">Port</span> <span class="hljs-number">443</span><br><br></code></pre></td></tr></table></figure><ol start="7"><li>重新连接github服务器：</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/ssh-3.png"></p><p>参考链接：</p><p><a href="https://blog.csdn.net/weixin_42310154/article/details/118340458">Github配置ssh key的步骤（大白话+包含原理解释）_github生成ssh key-CSDN博客</a></p><p><a href="https://githubdocs.cn/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent?platform=windows">在 SSH 代理中生成新的 SSH 密钥并添加它 - GitHub 文档 - GitHub 文档</a></p><p><a href="https://blog.csdn.net/misakivv/article/details/144929627">解决 ssh connect to host github.com port 22 Connection timed out_ssh: connect to host gitlab.sdlg.cn port 30022: co-CSDN博客</a></p><h2 id="可能出现的问题和解决方案"><a href="#可能出现的问题和解决方案" class="headerlink" title="可能出现的问题和解决方案"></a>可能出现的问题和解决方案</h2><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/git_solve_1.png"></p><p>来源：<a href="https://blog.csdn.net/fearlessxmm/article/details/90401690">https://blog.csdn.net/fearlessxmm/article/details/90401690</a>  </p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>Web</category>
      
    </categories>
    
    
    <tags>
      
      <tag>web</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试文章</title>
    <link href="/2025/02/10/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <url>/2025/02/10/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<p>1</p><p><img src="/2025/02/10/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/BA.jpg"></p><p>1</p><p>banner_img:&#x2F;测试文章&#x2F;BA.jpg</p><p>官方引用</p><img src="/2025/02/10/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/BA.jpg" class="" title="This is an example image"><img src="/2025/02/10/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/BA.jpg" class="" title="spaced title">]]></content>
    
    
    
    <tags>
      
      <tag>exam</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/02/10/hello-world/"/>
    <url>/2025/02/10/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>exam</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于《白话机器学习的数学》的知识整理（4）</title>
    <link href="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/"/>
    <url>/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>​ 在进行回归和分类时，为了进行预测，我们定义了函数fθ(x)，然后根据训练数据求出了函数的参数θ。</p><p>​但是，其实我们真正想要的是通过预测函数得到预测值。以回归的那个例子来说，就是关于投入的广告费能带来多少点击量的预测值。所以我们希望fθ(x)对未知数据x输出的预测值尽可能正确。所以我们需要能够<strong>定量地表示机器学习模型的精度</strong>。</p><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><h4 id="回归问题的验证"><a href="#回归问题的验证" class="headerlink" title="回归问题的验证"></a>回归问题的验证</h4><p>​把获取的全部训练数据分成两份：一份用于测试，一份用于训练。然后用前者来评估模型。</p><p>​对于回归的情况，只要在训练好的模型上计算测试数据的误差的平方，再取其平均值就可以了。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-1.png"></p><p>​这个值被称为均方误差或者MSE，全称Mean Square Error。这个误差越小，精度就越高，模型也就越好。</p><p>​另外，在分类问题中也会出现模型只能拟合训练数据的问题哦。</p><h4 id="分类问题的验证"><a href="#分类问题的验证" class="headerlink" title="分类问题的验证"></a>分类问题的验证</h4><p>​“数据的分配方法不要太极端其实会更好”这一点与回归的时候也是一样的。</p><p>​由于回归是连续值，所以可以从误差入手，但是在分类中我们必须要考虑分类的类别是否正确。</p><p>​在回归中要考虑的是答案不完全一致时的误差，而分类中要考虑的是答案是否正确。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-2.png"></p><p>​我们可以使用表里的4个记号来计算分类的精度。精度的英文是Accuracy，它的计算表达式是这样的：</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-3.png"></p><p>​</p><h4 id="精确率和召回率"><a href="#精确率和召回率" class="headerlink" title="精确率和召回率"></a>精确率和召回率</h4><p>​第一个指标——精确率。它的英文是Precision。根据表达式来看，它的含义是在被分类为Positive的数据中，实际就是Positive的数据所占的比例。 这个值越高，说明分类错误越少。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-4.png"></p><p>​第二个指标是召回率，英文是Recall。根据表达式来看，它的含义是在Positive数据中，实际被分类为Positive的数据所占的比例。 这个值越高，说明被正确分类的数据越多。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-5.png"></p><p>​精确率和召回率都很高的模型就是一个好模型了，不过一般来说，精确率和召回率会一个高一个低，需要我们取舍。</p><h4 id="F值"><a href="#F值" class="headerlink" title="F值"></a>F值</h4><p>​评定综合性能的指标F值，即Fmeasure：</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-6.png"></p><p>​有时称F值为F1值会更准确，这一点需要注意。除F1值之外，还有一个带权重的F值指标。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-7.png"></p><p>​F1值在数学上是精确率和召回率的调和平均值。</p><p>​之前介绍的精确率和召回率都是以TP为主进行计算的，而以TN为主来计算精确率和召回率的表达式是这样的:</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-8.png"></p><p>​当数据不平衡时，使用数量少的那个会更好。</p><p>​把全部训练数据分为测试数据和训练数据的做法称为交叉验证。这是非常重要的方法。</p><p>​交叉验证的方法中，尤为有名的是K折交叉验证：</p><p>● 把全部训练数据分为K份</p><p>● 将K-1份数据用作训练数据，剩下的1份用作测试数据</p><p>● 每次更换训练数据和测试数据，重复进行K次交叉验证</p><p>● 最后计算K个精度的平均值，把它作为最终的精度</p><p>​不切实际地增加K值会非常耗费时间，所以我们必须要确定一个合适的K值。</p><p>​</p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><h4 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h4><p>​之前我们提到过的模型只能拟合训练数据的状态被称为过拟合，英文是overfitting。</p><p>​过拟合不止在回归时出现，在分类时也经常发生</p><p>​有几种方法可以避免过拟合：</p><p>● 增加全部训练数据的数量</p><p>● 使用简单的模型</p><p>● 正则化</p><p>​</p><h4 id="正则化的方法"><a href="#正则化的方法" class="headerlink" title="正则化的方法"></a>正则化的方法</h4><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-9.png"></p><p>​我们要对这个新的目标函数进行最小化，这种方法就称为正则化。</p><p>​m是参数的个数,不过一般来说不对θ0应用正则化。所以仔细看会发现j的取值是从1开始的。</p><p>​假如预测函数的表达式为fθ(x)&#x3D;θ0+θ1x+θ2x2，那么m&#x3D;2就意味着正则化的对象参数为θ1和θ2。</p><p>​θ0这种只有参数的项称为偏置项，一般不对它进行正则化。</p><p>​λ是决定正则化项影响程度的正的常数。这个值需要我们自己来定。</p><h4 id="正则化的效果"><a href="#正则化的效果" class="headerlink" title="正则化的效果"></a>正则化的效果</h4><p>​把目标函数分成两个部分。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-10.png"></p><p>​C(θ)是本来就有的目标函数项，R(θ)是正则化项。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-11.png"></p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-12.png"></p><p>​与加正则化项之前相比，θ1更接近0了。</p><p>​这就是正则化的效果。它可以防止参数变得过大，有助于参数接近较小的值。虽然我们只考虑了θ1，但其他θj参数的情况也是类似的。</p><p>​因为参数的值变小，意味着该参数的影响也会相应地变小。</p><p>​通过减小不需要的参数的影响，将复杂模型替换为简单模型来防止过拟合的方式。</p><h4 id="分类的正则化"><a href="#分类的正则化" class="headerlink" title="分类的正则化"></a>分类的正则化</h4><p>​逻辑回归的目标函数是对数似然函数，分类也是在这个目标函数中增加正则化项就行了，道理是相同的。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-13.png"></p><p>​对数似然函数本来以最大化为目标。但是，这次我想让它变成和回归的目标函数一样的最小化问题，所以加了负号。这样就可以像处理回归一样处理它，所以只要加上正则化项就可以了。</p><p>​反转符号是为了将最大化问题替换为最小化问题。反转了符号之后，在更新参数时就要像回归一样，与微分的函数的符号反方向移动才行。</p><p>​其微分结果如下：</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-13-1.png"></p><p>​R(θ)对θ0微分的结果为0，所以j&#x3D;0时表达式4.3.14中的λθj就消失了。因此实际上我们需要像这样区分两种情况：</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-13-2.png"></p><p>​刚才我介绍的方法其实叫L2正则化。除L2正则化方法之外，还有L1正则化方法。它的正则化项R是这样的:</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-14.png"></p><p>​L1正则化的特征是被判定为不需要的参数会变为0，从而减少变量个数。而L2正则化不会把参数变为0。</p><p>​ L2正则化会抑制参数，使变量的影响不会过大，而L1会直接去除不要的变量。</p><h3 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h3><h4 id="欠拟合"><a href="#欠拟合" class="headerlink" title="欠拟合"></a>欠拟合</h4><p>​反过来又有一种叫作欠拟合的状态，用英文说是underfitting。在这种情况下模型的性能也会变差。</p><p>​比如用直线对图中这种拥有复杂边界线的数据进行分类的情况，无论怎样做都不能很好地分类，最终的精度会很差。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-15.png"></p><p>​</p><h4 id="区分过拟合与欠拟合"><a href="#区分过拟合与欠拟合" class="headerlink" title="区分过拟合与欠拟合"></a>区分过拟合与欠拟合</h4><p>​训练数据较少时训练好的模型难以预测未知的数据，所以精度很低；反过来说，训练数据变多时，预测精度就会一点点地变高。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-16-1.png"></p><p>​将两份数据的精度用图来展示后，如果是这种形状，就说明出现了欠拟合的状态。也有一种说法叫作高偏差。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-16-2.png"></p><p>​而在过拟合的情况下，图是下面这样的。这也叫作高方差。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-16-3.png"></p><p>​只对训练数据拟合得较好，这就是过拟合的特征。下图中需要注意的点在这里：</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-16-4.png"></p><p>​像这样展示了数据数量和精度的图称为<strong>学习曲线</strong>。</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI-Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于《白话机器学习的数学》的知识整理（3）</title>
    <link href="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/"/>
    <url>/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="设置问题"><a href="#设置问题" class="headerlink" title="设置问题"></a>设置问题</h3><p>​不去考虑图像本身的内容，只根据尺寸把它分类为纵向图像和横向图像，二分类问题。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3.png"></p><p>​只要找到这条线，就可以根据点在线的哪一边来判断图像是横向还是纵向的了。</p><h3 id="内积"><a href="#内积" class="headerlink" title="内积"></a>内积</h3><p>​分类用图形来解释更容易理解，所以把它想象为有大小和方向的、带箭头的向量比较好。</p><p>​那条线，是使权重向量成为法线向量的直线。</p><p>​设权重向量为w，那么那条直线的表达式就是w*x&#x3D;0。</p><p>​w是权重一词的英文——weight的首字母。上次学习回归时，我们为了求未知参数θ做了很多事情，而w和θ是一样的。</p><p>​实向量空间的内积是各相应元素乘积的和，所以刚才的表达式也可以写成这样：</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-1.png"></p><p>​现在要考虑的是有宽和高的二维情况，所以n&#x3D;2就可以了。</p><p>​法线是与某条直线相垂直的向量。</p><p>​设权重向量为w&#x3D;(1,1)，w*x&#x3D;x1+x2，如下图：</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-2.png"></p><p>​这就是“使权重向量成为法线向量的直线”在图形上的解释。</p><ul><li>一开始并不存在你画的那种直线，而是要通过训练找到权重向量，然后才能得到与这个向量垂直的直线，最后根据这条直线就可以对数据进行分类了。</li></ul><h3 id="感知机（perceptron）"><a href="#感知机（perceptron）" class="headerlink" title="感知机（perceptron）"></a>感知机（perceptron）</h3><p>​感知机是接受多个输入后将每个值与各自的权重相乘，最后输出总和的模型。</p><p>![](C:\Users\10764\Desktop\Record\Reading Impression\白话机器学习的数学\image\3-3.png)</p><p>​</p><h4 id="训练前的准备"><a href="#训练前的准备" class="headerlink" title="训练前的准备"></a>训练前的准备</h4><p>​首先是训练数据。设表示宽的轴为x1、表示高的轴为x2，用y来表示图像是横向还是纵向的，横向的值为1、纵向的值为-1。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-4.png"></p><p>​接下来，根据参数向量x来判断图像是横向还是纵向的函数，即返回1或者-1的函数的定义如下。这个函数被称为判别函数。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-5.png"></p><p>​决定内积符号的是cos θ。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-6.png"></p><p>​内积是衡量向量之间相似程度的指标。结果为正，说明二者相似；为0则二者垂直；为负则说明二者不相似。</p><h4 id="权重向量的更新表达式"><a href="#权重向量的更新表达式" class="headerlink" title="权重向量的更新表达式"></a>权重向量的更新表达式</h4><p>​在这个基础上，我们可以这样定义权重向量的更新表达式。</p><p>![](C:\Users\10764\Desktop\Record\Reading Impression\白话机器学习的数学\image\3-7.png)</p><p>​i指的是训练数据的索引，而不是i次方的意思，这一点一定要注意。用这个表达式重复处理所有训练数据，更新权重向量。</p><ul><li>在图上随意画一个权重向量和直线。</li></ul><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-7-1.png"></p><ul><li>假设第一个训练数据是x(1)&#x3D;(125,30)，首先我们就用它来更新参数吧。</li></ul><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-7-2.png"></p><p>​现在权重向量w和训练数据的向量x(1)二者的方向几乎相反，w和x(1)之间的夹角θ的范围是90◦＜θ＜270◦，内积为负。也就是说，判别函数fw(x(1))的分类结果为-1。</p><p>​现在y(1)&#x3D;1，所以更新表达式是这样的，其实就是向量的加法——w+x(1)。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-7-3.png"></p><p>​这个w+x(1)就是下一个新的w，画一条与新的权重向量垂直的直线，相当于把原来的线旋转了一下。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-7-4.png"></p><p>​刚才x(1)与权重向量分居直线两侧，现在它们在同一侧了</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-7-5.png"></p><p>​这次θ＜90◦，所以内积为正，判别函数fw(x)的分类结果为1。而且x(1)的标签也为1，说明分类成功了。</p><ul><li>刚才处理的是标签值y&#x3D;1的情况，而对于y&#x3D;-1的情况，只是更新表达式的向量加法变成了减法而已，做的事情是一样的。</li><li>也就是说，虽然有加法和减法的区别，但它们的做法都是在分类失败时更新权重向量，使得直线旋转相应的角度。</li><li>像这样重复更新所有的参数，就是<strong>感知机的学习方法</strong>。</li></ul><h3 id="线性可分"><a href="#线性可分" class="headerlink" title="线性可分"></a>线性可分</h3><p>​如果要对时装的照片进行分类的话用感知机是做不到的，因为感知机只是简单的模型。</p><ul><li>感知机的最大的缺点就是它只能解决线性可分的问题。</li></ul><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-8.png"></p><p>​假设有上面这张图里的数据，其中圆点为1，叉号为-1，如果只用一条直线对这些数据进行分类，应该画一条什么样的线呢？答案是不可能的。</p><p>​线性可分指的就是能够使用直线分类的情况，像这样不能用直线分类的就不是线性可分。</p><p>​像照片这类的图像分类就不是线性可分了。这类图像数据的维度一般会很高，所以无法可视化。</p><ul><li>之前提到的感知机也被称为简单感知机或单层感知机，真的是很弱的模型。不过，既然有单层感知机，那么就会有多层感知机。实际上多层感知机就是神经网络了，它是表现力非常高的模型。</li></ul><p>​有一个不同于感知机的算法能够很好地应用于线性不可分问题。这个算法更实用。</p><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p>​接下来要讲的算法与感知机的不同之处在于，它是把分类作为概率来考虑的。另外，这里设横向的值为1、纵向的值为0。</p><p>​只要是两个不同的值，用什么都可以。在学习感知机时之所以设置值为1和-1，是因为这样会使参数更新表达式看起来更简洁，而现在则是设置为1和0会更简洁。</p><p>​</p><h4 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h4><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-9.png"></p><p>​exp的全称是exponential，即指数函数。exp(x)与ex含义相同，只是写法不同。它的图形如下：</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-9-1.png"></p><ul><li><p>θTx&#x3D;0时fθ(x)&#x3D;0.5，以及0＜fθ(x)＜1是sigmoid函数的两个特征。</p><p>因为sigmoid函数的取值范围是0＜fθ(x)＜1，所以它可以作为概率来使用。</p></li></ul><h4 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h4><p>​把未知数据x是横向图像的概率作为fθ(x)。其表达式是这样的：</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-10.png"></p><p>​这是在给出x数据时y&#x3D;1，即图像为横向的概率。</p><p>​结合sigmoid函数，有：</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-10-1.png"></p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-10-2.png"></p><p>​为了求得正确的参数θ而定义目标函数，进行微分，然后求参数的更新表达式。这种算法就称为<strong>逻辑回归</strong>。</p><h3 id="似然函数"><a href="#似然函数" class="headerlink" title="似然函数"></a>似然函数</h3><p>​逻辑回归的目标函数与之前的不一样哦。</p><p>● y&#x3D;1的时候，我们希望概率P(y&#x3D;1|x)是最大的；</p><p>● y&#x3D;0的时候，我们希望概率P(y&#x3D;0|x)是最大的。</p><p>​联合概率的表达式一般化的写法如下：</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-11.png"></p><ul><li><p>利用了任何数字的0次方都是1的特性；</p></li><li><p>回归的时候处理的是误差，所以要最小化，而现在考虑的是联合概率，我们希望概率尽可能大，所以要最大化。</p></li><li><p>这里的目标函数L(θ)也被称为似然，函数的名字L取自似然的英文单词Likelihood的首字母。</p></li><li><p>我们可以认为似然函数L(θ)中，使其值最大的参数θ能够最近似地说明训练数据。</p></li></ul><h3 id="对数似然函数"><a href="#对数似然函数" class="headerlink" title="对数似然函数"></a>对数似然函数</h3><ul><li><p>直接对似然函数进行微分有点困难，在此之前要把函数变形。</p><ol><li>首先它是联合概率。概率都是1以下的数，所以像联合概率这种概率乘法的值会越来越小。</li><li>另外还有一个，那就是乘法。与加法相比，乘法的计算量要大得多。</li></ol></li><li><p>只要取似然函数的对数就好了。像这样在等式两边加上log即可：</p></li></ul><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-11-1.png"></p><p>​我们现在考察的似然函数也是在L(θ1)＜L(θ2)时，有log L(θ1)＜log L(θ2)成立。也就是说，使L(θ)最大化等价于使log L(θ)最大化。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-11-2.png"></p><p>​现在我们考虑的只有y&#x3D;1和y&#x3D;0两种情况，所以应有P(y(i)&#x3D;0|x(i))+P(y(i)&#x3D;1|x(i))&#x3D;1。</p><h4 id="似然函数的微分"><a href="#似然函数的微分" class="headerlink" title="似然函数的微分"></a>似然函数的微分</h4><p>​最小化时要按照与微分结果的符号相反的方向移动，而最大化时要与微分结果的符号同向移动。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-12-1.png"></p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-12-2.png"></p><h3 id="线性不可分"><a href="#线性不可分" class="headerlink" title="线性不可分"></a>线性不可分</h3><p>​通过随意地增加次数，就可以得到复杂形状的决策边界了。</p><p>​还有一个名为SVM，也就是支持向量机的分类算法也很有名。此外，还有多分类的做法。</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI-Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于《白话机器学习的数学》的知识整理（2）</title>
    <link href="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/"/>
    <url>/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="设置问题"><a href="#设置问题" class="headerlink" title="设置问题"></a>设置问题</h3><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2.png"></p><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>​不可能让所有点的误差都等于0。所以我们要做的是让所有点的误差之和尽可能地小。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-0.png"></p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-1.png"></p><p>​我们只要想办法缩小误差虚线的高度，就能预测正确的点击量了。</p><p>​我们来把刚才说的内容用表达式展现出来。假设有n个训练数据，那么它们的误差之和可以用这样的表达式表示。这个表达式称为目标函数，E(θ)的E是误差的英语单词Error的首字母。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-2.png"></p><ul><li>x(i)和y(i)中的i不是i次幂的意思，而是指第i个训练数据；</li><li>我们对每个训练数据的误差取平方之后，全部相加，然后乘以1&#x2F;2。这么做是为了找到使E(θ)的值最小的θ。这样的问题称为最优化问题；</li><li>如果只是简单地计算差值，我们就得考虑误差为负值的情况，所以要计算误差的平方；</li><li>一般不用绝对值，而用平方。因为之后要对目标函数进行微分，比起绝对值，平方的微分更加简单；</li></ul><h4 id="最速下降法"><a href="#最速下降法" class="headerlink" title="最速下降法"></a>最速下降法</h4><ul><li>最速下降法或梯度下降法；</li></ul><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-3.png"></p><ol><li>A:&#x3D;B这种写法，它的意思是通过B来定义A。</li><li>η是称为学习率的正的常数，读作“伊塔”。根据学习率的大小，到达最小值的更新次数也会发生变化。换种说法就是收敛速度会不同。有时候甚至会出现完全无法收敛，一直发散的情况。</li><li>如果η较大，那么x:&#x3D;x-η(2x-2)会在两个值上跳来跳去，甚至有可能远离最小值。这就是发散状态。而当η较小时，移动量也变小，更新次数就会增加，但是值确实是会朝着收敛的方向而去。</li></ol><p>​如 图2-0 所示，fθ(x)拥有θ0和θ1两个参数。也就是说这个目标函数是拥有θ0和θ1的双变量函数，所以不能用普通的微分，而要用偏微分。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-4.png"></p><p>令<img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-5-1.png"></p><p>有<img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-5-2.png"></p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-5-3.png"></p><p>即<img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-5-4.png"></p><p>同理<img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-5-5.png"></p><p>综上所述：</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-5-6.png"></p><h3 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h3><p>​对于一开始我在图中添加的数据点来说，其实曲线比直线拟合得更好。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-6.png"></p><p>​当我们把fθ(x)定义为二次函数，就能用它来表示这条曲线了。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-7.png"></p><p>​或者用更大次数的表达式也可以。这样就能表示更复杂的曲线了。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-8.png"></p><p>​虽然次数越大拟合得越好，但难免也会出现过拟合的问题。</p><p>​对二次函数进行微分，最终得到：</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-9.png"></p><p>​像这样增加函数中多项式的次数，然后再使用函数的分析方法被称为<strong>多项式回归</strong>。</p><h3 id="多重回归"><a href="#多重回归" class="headerlink" title="多重回归"></a>多重回归</h3><p>​实际中要解决的很多问题是变量超过2个的复杂问题。</p><p>​如之前只是根据广告费来预测点击量，现在呢，决定点击量的除了广告费之外，还有广告的展示位置和广告版面的大小等多个要素。现在变量达到了3个以上，就无法可视化了。</p><p>​这次我们只考虑广告版面的大小，设广告费为x1、广告栏的宽为x2、广告栏的高为x3，那么fθ可以表示如下：</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-10.png"></p><p>​</p><p>​当有n个变量时：</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-11-1.png"></p><p>​把θ和x用列向量来定义：</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-11-2.png"></p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-11-3.png"></p><p>即：<img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-11-4.png"></p><p>​设u&#x3D;E(θ)、v&#x3D;fθ(x)的部分是一样的。为了一般化，我们可以考虑对第j个元素θj偏微分的表达式。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-12-1.png"></p><p>​u对v微分的部分是一样的，所以只需要求v对θj的微分就好了。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-12-2.png"></p><p>​第j个参数的更新表达式就是这样的:</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-12-3.png"></p><p>​想这样包含了多个变量的回归称为<strong>多重回归</strong>。</p><p>​所谓的最速下降法就是对所有的训练数据都重复进行计算。现在可以收集大量的数据，但是训练数据越多，循环次数也就越多，那么计算起来不就非常花时间了吗？为此：</p><h3 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h3><p>​最速下降法除了计算花时间以外，还有一个缺点——那就是容易陷入局部最优解。</p><p>​选用随机数作为初始值的情况比较多。不过这样每次初始值都会变，进而导致陷入局部最优解的问题</p><p>​如 图2-12-3 所示表达式使用了所有训练数据的误差，在随机梯度下降法中会随机选择一个训练数据，并使用它来更新参数。这个表达式中的k就是被随机选中的数据索引。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-13.png"></p><p>​ 最速下降法更新1次参数的时间，随机梯度下降法可以更新n次。此外，随机梯度下降法由于训练数据是随机选择的，更新参数时使用的又是选择数据时的梯度，所以不容易陷入目标函数的局部最优解。</p><p>​除了随机选择1个训练数据的做法，此外还有随机选择m个训练数据来更新参数的做法。设随机选择m个训练数据的索引的集合为K，那么我们这样来更新参数。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-14.png"></p><p>​ 假设训练数据有100个，那么在m&#x3D;10时，创建一个有10个随机数的索引的集合，例如K&#x3D;{61, 53, 59, 16, 30, 21, 85, 31, 51, 10}，然后重复更新参数。这种做法被称为<strong>小批量（mini-batch）梯度下降法</strong>。</p><p>​不管是随机梯度下降法还是小批量梯度下降法，我们都必须考虑学习率η。把η设置为合适的值是很重要的。</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI-Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于《白话机器学习的数学》的知识整理（1）</title>
    <link href="/2024/07/22/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/"/>
    <url>/2024/07/22/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><ul><li>经常听到的是鉴别垃圾邮件、用图像进行人脸识别、电商网站的推荐功能之类的。</li><li>虽然它的应用场景很多，但它不是万能的。了解机器学习适合的应用场景，明确它能做什么、不能做什么也很重要。</li></ul><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul><li>无论是过去还是现在，计算机都特别擅长处理重复的任务。所以计算机能够比人类更高效地读取大量的数据、学习数据的特征并从中找出数据的模式。这样的任务也被称为<strong>机器学习或者模式识别</strong>。</li><li>受益于 <ol><li>具备了能够收集大量数据的环境</li><li>具备了能够处理大量数据的环境</li></ol></li><li>当我们打算用机器学习做什么事情的时候，首先需要的就是数据。因为机器学习就是从数据中找出特征和模式的技术。</li><li>比起可以学习到大量知识，计算机能够更快地处理数据这一点更令人激动。现在可以使用GPU进行数值计算，Hadoop、Spark之类的分布式处理技术也逐渐成熟，所以才说现在“具备了能够处理大量数据的环境”。</li></ul><h3 id="相关算法"><a href="#相关算法" class="headerlink" title="相关算法"></a>相关算法</h3><ul><li><p>回归-regression</p><p>简单易懂地说，回归就是在处理连续数据如时间序列数据时使用的技术。如股价时间表。</p></li><li><p>分类-classification</p><p>只有两个类别的问题称为二分类，有三个及以上的问题称为多分类，比如数字的识别就属于多分类问题。</p><p>这种技术可以用来自动识别明信片上手写的邮件编码。有一个很有名的数据集叫MNIST，其中收集了大量手写的数字图片，以及图片实际的数字信息。</p></li><li><p>聚类-clustering</p><p>聚类与分类相似，却又有些不同。聚类考虑的问题是：假设在有100名学生的学校进行摸底考试，然后根据考试成绩把100名学生分为几组，根据分组结果，我们能得出某组偏重理科、某组偏重文科这样有意义的结论。</p><p>它与分类的区别在于数据带不带标签。也有人把标签称为正确答案数据。</p></li></ul><p>​使用有标签的数据进行的学习称为<strong>有监督学习</strong>，与之相反，使用没有标签的数据进行的学习称为<strong>无监督学习</strong>。回归和分类是有监督学习，而聚类是无监督学习。</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI-Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
