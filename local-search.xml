<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>基于鱼书（第一册）的知识整理（2）</title>
    <link href="/2025/03/10/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/"/>
    <url>/2025/03/10/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="神经网络的学习"><a href="#神经网络的学习" class="headerlink" title="神经网络的学习"></a>神经网络的学习</h1><p>​这里所说的“学习”是指从训练数据中自动获取最优权重参数的过程。</p><p>​为了使神经网络能进行学习，将导入损失函数这一指标。而学习的目的就是以该损失函数为基准，找出能使它的值达到最小的权重参数。</p><h2 id="从数据中学习"><a href="#从数据中学习" class="headerlink" title="从数据中学习"></a>从数据中学习</h2><p>​神经网络的特征就是可以从数据中学习。所谓“从数据中学习”，是指可以由数据自动决定权重参数的值。</p><p>​对于线性可分问题，感知机是可以利用数据自动学习的。根据“感知机收敛定理”，通过有限次数的学习，线性可分问题是可解的。但是，非线性可分问题则无法通过（自动）学习来解决。</p><h3 id="数据驱动"><a href="#数据驱动" class="headerlink" title="数据驱动"></a>数据驱动</h3><p>​从数据中寻找答案、从数据中发现模式、根据数据讲故事……这些机器学习所做的事情，如果没有数据的话，就无从谈起。因此，<code>数据是机器学习的核心</code>。</p><p>​人们以自己的经验和直觉为线索，通过反复试验推进工作。而机器学习的方法则极力避免人为介入，尝试从收集到的数据中发现答案（模式）。神经网络或深度学习则比以往的机器学习方法更能避免人为介入。</p><p>​如果让我们自己来设计一个能将5正确分类的程序，就会意外地发现这是一个很难的问题。人可以简单地识别出5，但却很难明确说出是基于何种规律而识别出了5。此外，每个人都有不同的写字习惯，要发现其中的规律是一件非常难的工作。</p><p>​一种方案是，先从图像中提取特征量，再用机器学习技术学习这些特征量的模式。</p><ul><li>这里所说的“特征量”是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。</li><li>图像的特征量通常表示为向量的形式。</li><li>在计算机视觉领域，常用的特征量包括SIFT、SURF和HOG等。</li><li>使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、KNN等分类器进行学习。</li></ul><p>​机器学习的方法中，由机器从收集到的数据中找出规律性。与从零开始想出算法相比，这种方法可以更高效地解决问题，也能减轻人的负担。</p><p>​但是需要注意的是，将图像转换为向量时使用的特征量仍是由人设计的。对于不同的问题，必须使用合适的特征量（必须设计专门的特征量），才能得到好的结果。</p><p>​也就是说，即使使用特征量和机器学习的方法，也需要针对不同的问题人工考虑合适的特征量。</p><p><img src="/2025/03/10/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/%E4%B8%89%E7%B1%BB%E8%AF%86%E5%88%AB%E7%AE%97%E6%B3%95.png"></p><p>​深度学习有时也称为端到端机器学习(end-to-end machine learning)。这里所说的端到端是指从一端到另一端的意思，也就是从原始数据（输入）中获得目标结果（输出）的意思。</p><p>​神经网络的优点是对所有的问题都可以用同样的流程来解决。也就是说，与待处理的问题无关，神经网络可以将数据直接作为原始数据，进行“端对端”的学习。</p><h3 id="训练数据和测试数据"><a href="#训练数据和测试数据" class="headerlink" title="训练数据和测试数据"></a>训练数据和测试数据</h3><p>​机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验等。首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。</p><p>​为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。另外，训练数据也可以称为监督数据。</p><p>​泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。获得泛化能力是机器学习的最终目标。</p><p>​只对某个数据集过度拟合的状态称为过拟合(over fitting)。避免过拟合也是机器学习的一个重要课题。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>​神经网络的学习通过某个指标表示现在的状态。然后，以这个指标为基准，寻找最优权重参数。神经网络的学习中所用的指标称为损失函数(loss function)。这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等。</p><p>​损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。以“性能的恶劣程度”为指标可能会使人感到不太自然，但是如果给损失函数乘上一个负值，就可以解释为“在多大程度上不坏”，即“性能有多好”。并且，“使性能的恶劣程度达到最小”和“使性能的优良程度达到最大”是等价的，不管是用“恶劣程度”还是“优良程度”，做的事情本质上都是一样的。</p><h3 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a>均方误差</h3><p>$$<br>\text{MSE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2<br>$$</p><ul><li><p>前者为真实值，后者为预测值。</p></li><li><p>n 是样本的数量。</p></li></ul><p>​均方误差用于衡量预测值与实际值之间的平均平方差，通常用于回归问题的评估指标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_squared_error</span>(<span class="hljs-params">y, t</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * np.<span class="hljs-built_in">sum</span>((y-t)**<span class="hljs-number">2</span>)<br><br></code></pre></td></tr></table></figure><h3 id="交叉熵误差"><a href="#交叉熵误差" class="headerlink" title="交叉熵误差"></a>交叉熵误差</h3><p>$$<br>\text{Cross-Entropy} &#x3D; - \sum_{i&#x3D;1}^{n} \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right)<br>$$</p><ul><li>前者是真实标签（通常为 0 或 1）。后者第 i个预测概率（预测类别为 1 的概率）。</li><li>n是样本的数量。</li></ul><p>​实际上只计算对应正确解标签的输出的自然对数。比如，假设正确解标签的索引是“2”，与之对应的神经网络的输出是0.6，则交叉熵误差是-log 0.6 &#x3D; 0.51；若“2”对应的输出是0.1，则交叉熵误差为-log 0.1 &#x3D; 2.30。也就是说，交叉熵误差的值是由正确解标签所对应的输出结果决定的。</p><p>​交叉熵误差常用于分类问题中，衡量模型的预测概率与实际标签之间的差异。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cross_entropy_error</span>(<span class="hljs-params">y, t</span>):<br>    delta = <span class="hljs-number">1e-7</span><br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(t * np.log(y + delta))<br><br></code></pre></td></tr></table></figure><p>​函数内部在计算np.log时，加上了一个微小值delta。这是因为，当出现np.log(0)时，np.log(0)会变为负无限大的-inf，这样一来就会导致后续计算无法进行。作为保护性对策，添加一个微小值可以防止负无限大的发生。</p><h3 id="mini-batch学习"><a href="#mini-batch学习" class="headerlink" title="mini-batch学习"></a>mini-batch学习</h3><p>​使用训练数据进行学习，严格来说，就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。因此，计算损失函数时必须将所有的训练数据作为对象。也就是说，如果训练数据有100个的话，我们就要把这100个损失函数的总和作为学习的指标。</p><p>​通过除以N，可以求单个数据的“平均损失函数”。通过这样的平均化，可以获得和训练数据的数量无关的统一指标。比如，即便训练数据有1000个或10000个，也可以求得单个数据的平均损失函数。</p><p>​另外，MNIST数据集的训练数据有60000个，如果以全部数据为对象求损失函数的和，则计算过程需要花费较长的时间。再者，如果遇到大数据，数据量会有几百万、几千万之多，这种情况下以全部数据为对象计算损失函数是不现实的。因此，我们从全部数据中选出一部分，作为全部数据的“近似”。</p><p>​</p><p>​神经网络的学习也是从训练数据中选出一批数据（称为mini-batch,小批量），然后对每个mini-batch进行学习。比如，从60000个训练数据中随机选择100笔，再用这100笔数据进行学习。这种学习方式称为mini-batch学习。</p><h3 id="mini-batch版交叉熵误差的实现"><a href="#mini-batch版交叉熵误差的实现" class="headerlink" title="mini-batch版交叉熵误差的实现"></a>mini-batch版交叉熵误差的实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cross_entropy_error</span>(<span class="hljs-params">y, t</span>):<br>    <span class="hljs-keyword">if</span> y.ndim == <span class="hljs-number">1</span>:<br>        t = t.reshape(<span class="hljs-number">1</span>, t.size)<br>        y = y.reshape(<span class="hljs-number">1</span>, y.size)<br><br>    batch_size = y.shape[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(t * np.log(y + <span class="hljs-number">1e-7</span>)) / batch_size<br><br></code></pre></td></tr></table></figure><p>​y是神经网络的输出，t是监督数据。y的维度为1时，即求单个数据的交叉熵误差时，需要改变数据的形状。并且，当输入为mini-batch时，要用batch的个数进行正规化，计算单个数据的平均交叉熵误差。</p><p>​当监督数据是标签形式（非one-hot表示，而是像“2”“7”这样的标签）时，交叉熵误差可通过如下代码实现.。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cross_entropy_error</span>(<span class="hljs-params">y, t</span>):<br>    <span class="hljs-keyword">if</span> y.ndim == <span class="hljs-number">1</span>:<br>        t = t.reshape(<span class="hljs-number">1</span>, t.size)<br>        y = y.reshape(<span class="hljs-number">1</span>, y.size)<br><br>    batch_size = y.shape[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(np.log(y[np.arange(batch_size), t] + <span class="hljs-number">1e-7</span>)) / batch_size<br><br></code></pre></td></tr></table></figure><p>​实现的要点是，由于one-hot表示中t为0的元素的交叉熵误差也为0，因此针对这些元素的计算可以忽略。换言之，如果可以获得神经网络在正确解标签处的输出，就可以计算交叉熵误差。因此，t为one-hot表示时通过t *np.log(y)计算的地方，在t为标签形式时，可用np.log( y[np.arange (batch_size), t] )实现相同的处理（为了便于观察，这里省略了微小值1e-7）。</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">np</span>.arange (batch_size)会生成一个从<span class="hljs-number">0</span>到batch_size-<span class="hljs-number">1</span>的数组。比如当batch_size为<span class="hljs-number">5</span>时，np.arange(batch_size)会生成一个NumPy数组[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>,<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]。因为t中标签是以[<span class="hljs-number">2</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">9</span>, <span class="hljs-number">4</span>]的形式存储的，所以y[np.arange(batch_size), t]能抽出各个数据的正确解标签对应的神经网络的输出（在这个例子中，y[np.arange(batch_size), t]会生成NumPy数组[y[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>], y[<span class="hljs-number">1</span>,<span class="hljs-number">7</span>], y[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>], y[<span class="hljs-number">3</span>,<span class="hljs-number">9</span>], y[<span class="hljs-number">4</span>,<span class="hljs-number">4</span>]]）。<br></code></pre></td></tr></table></figure><h3 id="为何要设定损失函数"><a href="#为何要设定损失函数" class="headerlink" title="为何要设定损失函数"></a>为何要设定损失函数</h3><p>​在神经网络的学习中，寻找最优参数（权重和偏置）时，要寻找使损失函数的值尽可能小的参数。为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值。</p><p>​在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为0。</p><p>​因为即便识别精度有所改善，它的值也不会像32.0123 …%这样连续变化，而是变为33%、34%这样的不连续的、离散的值。而如果把损失函数作为指标，则当前损失函数的值可以表示为0.92543 …这样的值。并且，如果稍微改变一下参数的值，对应的损失函数也会像0.93432 …这样发生连续性的变化。</p><p>​识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值也是不连续地、突然地变化。作为激活函数的阶跃函数也有同样的情况。出于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行。</p><h2 id="数值微分"><a href="#数值微分" class="headerlink" title="数值微分"></a>数值微分</h2><p>​梯度法使用梯度的信息决定前进的方向。</p><p>​如果按照导数定义进行计算，会产生以下问题：</p><ol><li>因为想把尽可能小的值赋给h，所以h使用了10e-50这个微小值。但是，这样反而产生了舍入误差(rounding error)。所谓舍入误差，是指因省略小数的精细部分的数值（比如，小数点第8位以后的数值）而造成最终的计算结果上的误差。</li><li>真的导数（真的切线）和定义中得到的导数的值在严格意义上并不一致。这个差异的出现是因为h不可能无限接近0。</li></ol><p>​数值微分含有误差。为了减小这个误差，我们可以计算函数f在(x+h)和(x-h)之间的差分。因为这种计算方法以x为中心，计算它左右两边的差分，所以也称为中心差分（而(x+h)和x之间的差分称为前向差分）。</p><p>​利用微小的差分求导数的过程称为数值微分(numerical differentiation)。而基于数学式的推导求导数的过程，则用“解析性”(analytic)一词，称为“解析性求解”或者“解析性求导”。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">numerical_diff</span>(<span class="hljs-params">f, x</span>):<br>    h = <span class="hljs-number">1e-4</span> <span class="hljs-comment"># 0.0001</span><br>    <span class="hljs-keyword">return</span> (f(x+h) - f(x-h)) / (<span class="hljs-number">2</span>*h)<br><br></code></pre></td></tr></table></figure><p>​偏导数和单变量的导数一样，都是求某个地方的斜率。不过，偏导数需要将多个变量中的某一个变量定为目标变量，并将其他变量固定为某个值。</p><h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><p>​由全部变量的偏导数汇总而成的向量称为梯度(gradient)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">numerical_gradient</span>(<span class="hljs-params">f, x</span>):<br>    h = <span class="hljs-number">1e-4</span> <span class="hljs-comment"># 0.0001</span><br>    grad = np.zeros_like(x) <span class="hljs-comment"># 生成和x形状相同的数组</span><br><br>    <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(x.size):<br>        tmp_val = x[idx]<br>        <span class="hljs-comment"># f(x+h)的计算</span><br>        x[idx] = tmp_val + h<br>        fxh1 = f(x)<br><br>        <span class="hljs-comment"># f(x-h)的计算</span><br>        x[idx] = tmp_val - h<br>        fxh2 = f(x)<br><br>        grad[idx] = (fxh1 - fxh2) / (<span class="hljs-number">2</span>*h)<br>        x[idx] = tmp_val <span class="hljs-comment"># 还原值</span><br><br>    <span class="hljs-keyword">return</span> grad<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">function_2</span>(<span class="hljs-params">x</span>):<br><span class="hljs-keyword">return</span> x[<span class="hljs-number">0</span>]**<span class="hljs-number">2</span> + x[<span class="hljs-number">1</span>]**<span class="hljs-number">2</span><br><span class="hljs-comment"># 或者return np.sum(x**2)</span><br><br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>numerical_gradient(function_2, np.array([<span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>]))<br>array([ <span class="hljs-number">6.</span>,  <span class="hljs-number">8.</span>])<span class="hljs-number">2</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>numerical_gradient(function_2, np.array([<span class="hljs-number">0.0</span>, <span class="hljs-number">2.0</span>]))<br>array([ <span class="hljs-number">0.</span>,  <span class="hljs-number">4.</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>numerical_gradient(function_2, np.array([<span class="hljs-number">3.0</span>, <span class="hljs-number">0.0</span>]))<br>array([ <span class="hljs-number">6.</span>,  <span class="hljs-number">0.</span>])<br></code></pre></td></tr></table></figure><p>​函数numerical_gradient(f, x)中，参数f为函数，x为NumPy数组，该函数对NumPy数组x的各个元素求数值微分。</p><p>​实际上，梯度会指向各点处的函数值降低的方向。更严格地讲，梯度指示的方向是各点处的函数值减小最多的方向</p><h3 id="梯度法"><a href="#梯度法" class="headerlink" title="梯度法"></a>梯度法</h3><p>​机器学习的主要任务是在学习时寻找最优参数。同样地，神经网络也必须在学习时找到最优参数（权重和偏置）。</p><p>​这里所说的最优参数是指损失函数取最小值时的参数。但是，一般而言，损失函数很复杂，参数空间庞大，我们不知道它在何处能取得最小值。而通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">梯度表示的是各点处的函数值减小最多的方向。因此，无法保证梯度所指的方向就是函数的最小值或者真正应该前进的方向。实际上，在复杂的函数中，梯度指示的方向基本上都不是函数值最小处。<br></code></pre></td></tr></table></figure><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs arcade">函数的极小值、最小值以及被称为鞍点(saddle <span class="hljs-built_in">point</span>)的地方，梯度为<span class="hljs-number">0</span>。极小值是局部最小值，也就是限定在某个范围内的最小值。鞍点是从某个方向上看是极大值，从另一个方向上看则是极小值的点。<br>虽然梯度法是要寻找梯度为<span class="hljs-number">0</span>的地方，但是那个地方不一定就是最小值（也有可能是极小值或者鞍点）。此外，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入被称为“学习高原”的无法前进的停滞期。<br></code></pre></td></tr></table></figure><p>​虽然梯度的方向并不一定指向最小值，但沿着它的方向能够最大限度地减小函数的值。因此，在寻找函数的最小值（或者尽可能小的值）的位置的任务中，要以梯度的信息为线索，决定前进的方向。</p><p><img src="/2025/03/10/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/%E6%A2%AF%E5%BA%A6%E6%B3%95.png"></p><p>​η表示更新量，在神经网络的学习中，称为学习率(learning rate)。学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数。</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dns">学习率需要事先确定为某个值，比如<span class="hljs-number">0</span>.<span class="hljs-number">01或0.001</span>。一般而言，这个值过大或过小，都无法抵达一个“好的位置”。在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了。<br></code></pre></td></tr></table></figure><p>​上面的是表示更新一次的式子，这个步骤会反复执行。也就是说，每一步都按该式更新变量的值，通过反复执行此步骤，逐渐减小函数值。虽然这里只展示了有两个变量时的更新过程，但是即便增加变量的数量，也可以通过类似的式子（各个变量的偏导数）进行更新。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient_descent</span>(<span class="hljs-params">f, init_x, lr=<span class="hljs-number">0.01</span>, step_num=<span class="hljs-number">100</span></span>):<br>    x = init_x<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(step_num):<br>        grad = numerical_gradient(f, x)<br>        x -= lr * grad<br><br>    <span class="hljs-keyword">return</span> x<br><br></code></pre></td></tr></table></figure><p>​像学习率这样的参数称为超参数。这是一种和神经网络的参数（权重和偏置）性质不同的参数。相对于神经网络的权重参数是通过训练数据和学习算法自动获得的，学习率这样的超参数则是人工设定的。一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定。</p><h3 id="神经网络的梯度"><a href="#神经网络的梯度" class="headerlink" title="神经网络的梯度"></a>神经网络的梯度</h3><p>​神经网络的学习也要求梯度。这里所说的梯度是指损失函数关于权重参数的梯度。</p><h2 id="学习算法的实现"><a href="#学习算法的实现" class="headerlink" title="学习算法的实现"></a>学习算法的实现</h2><p>​神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。神经网络的学习分成下面4个步骤。</p><ul><li>步骤1（mini-batch）：从训练数据中随机选出一部分数据，这部分数据称为mini-batch。我们的目标是减小mini-batch的损失函数的值。</li><li>步骤2（计算梯度）：为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度。梯度表示损失函数的值减小最多的方向。</li><li>步骤3（更新参数）：将权重参数沿梯度方向进行微小更新。</li><li>步骤4（重复0：重复步骤1、步骤2、步骤3。</li></ul><p>​这个方法通过梯度下降法更新参数，不过因为这里使用的数据是随机选择的mini batch数据，所以又称为随机梯度下降法(stochastic gradient descent)。“随机”指的是“随机选择的”的意思，因此，随机梯度下降法是“对随机选择的数据进行的梯度下降法”。</p><p>​深度学习的很多框架中，随机梯度下降法一般由一个名为SGD的函数来实现。SGD来源于随机梯度下降法的英文名称的首字母。</p><h3 id="2层神经网络的类"><a href="#2层神经网络的类" class="headerlink" title="2层神经网络的类"></a>2层神经网络的类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir)<br><span class="hljs-keyword">from</span> common.functions <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> common.gradient <span class="hljs-keyword">import</span> numerical_gradient<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TwoLayerNet</span>:<br><br>    <span class="hljs-comment">#输入层、隐藏层、输出层的神经元数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size,</span><br><span class="hljs-params">                 weight_init_std=<span class="hljs-number">0.01</span></span>):<br>        <span class="hljs-comment"># 初始化权重</span><br>        <span class="hljs-variable language_">self</span>.params = &#123;&#125;<br>        <br>        <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W1&#x27;</span>] = weight_init_std * \<br>                            np.random.randn(input_size, hidden_size)<br>        <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b1&#x27;</span>] = np.zeros(hidden_size)<br>        <br>        <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W2&#x27;</span>] = weight_init_std * \<br>                            np.random.randn(hidden_size, output_size)<br>        <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b2&#x27;</span>] = np.zeros(output_size)<br><br>        <br>        <span class="hljs-comment">#进行识别（推理），x是图像数据</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, x</span>):<br>        W1, W2 = <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W1&#x27;</span>], <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W2&#x27;</span>]<br>        b1, b2 = <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b1&#x27;</span>], <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b2&#x27;</span>]<br><br>        a1 = np.dot(x, W1) + b1<br>        z1 = sigmoid(a1)<br>        a2 = np.dot(z1, W2) + b2<br>        y = softmax(a2)<br><br>        <span class="hljs-keyword">return</span> y<br><br>    <span class="hljs-comment">#计算损失函数的值</span><br>    <span class="hljs-comment"># x:输入数据, t:监督数据（正确解标签）</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, x, t</span>):<br>        y = <span class="hljs-variable language_">self</span>.predict(x)<br><br>        <span class="hljs-keyword">return</span> cross_entropy_error(y, t)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">accuracy</span>(<span class="hljs-params">self, x, t</span>):<br>        y = <span class="hljs-variable language_">self</span>.predict(x)<br>        y = np.argmax(y, axis=<span class="hljs-number">1</span>)<br>        t = np.argmax(t, axis=<span class="hljs-number">1</span>)<br><br>        accuracy = np.<span class="hljs-built_in">sum</span>(y == t) / <span class="hljs-built_in">float</span>(x.shape[<span class="hljs-number">0</span>])<br>        <span class="hljs-keyword">return</span> accuracy<br><br>    <span class="hljs-comment">#计算权重参数的梯度</span><br>    <span class="hljs-comment"># x:输入数据, t:监督数据</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">numerical_gradient</span>(<span class="hljs-params">self, x, t</span>):<br>        loss_W = <span class="hljs-keyword">lambda</span> W: <span class="hljs-variable language_">self</span>.loss(x, t)<br><br>        grads = &#123;&#125;<br>        grads[<span class="hljs-string">&#x27;W1&#x27;</span>] = numerical_gradient(loss_W, <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W1&#x27;</span>])<br>        grads[<span class="hljs-string">&#x27;b1&#x27;</span>] = numerical_gradient(loss_W, <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b1&#x27;</span>])<br>        grads[<span class="hljs-string">&#x27;W2&#x27;</span>] = numerical_gradient(loss_W, <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W2&#x27;</span>])<br>        grads[<span class="hljs-string">&#x27;b2&#x27;</span>] = numerical_gradient(loss_W, <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b2&#x27;</span>])<br><br>        <span class="hljs-keyword">return</span> grads<br><br><br></code></pre></td></tr></table></figure><h3 id="mini-batch的实现"><a href="#mini-batch的实现" class="headerlink" title="mini-batch的实现"></a>mini-batch的实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># coding: utf-8</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir)  <span class="hljs-comment"># 为了导入父目录的文件而进行的设定</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> dataset.mnist <span class="hljs-keyword">import</span> load_mnist<br><span class="hljs-keyword">from</span> two_layer_net <span class="hljs-keyword">import</span> TwoLayerNet<br><br><span class="hljs-comment"># 读入数据</span><br>(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="hljs-literal">True</span>, one_hot_label=<span class="hljs-literal">True</span>)<br><br>network = TwoLayerNet(input_size=<span class="hljs-number">784</span>, hidden_size=<span class="hljs-number">50</span>, output_size=<span class="hljs-number">10</span>)<br><br>iters_num = <span class="hljs-number">10000</span>  <span class="hljs-comment"># 适当设定循环的次数</span><br>train_size = x_train.shape[<span class="hljs-number">0</span>]<br>batch_size = <span class="hljs-number">100</span><br>learning_rate = <span class="hljs-number">0.1</span><br><br>train_loss_list = []<br>train_acc_list = []<br>test_acc_list = []<br><br>iter_per_epoch = <span class="hljs-built_in">max</span>(train_size / batch_size, <span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iters_num):<br>    batch_mask = np.random.choice(train_size, batch_size)<br>    x_batch = x_train[batch_mask]<br>    t_batch = t_train[batch_mask]<br>    <br>    <span class="hljs-comment"># 计算梯度</span><br>    <span class="hljs-comment">#grad = network.numerical_gradient(x_batch, t_batch)</span><br>    grad = network.gradient(x_batch, t_batch)<br>    <br>    <span class="hljs-comment"># 更新参数</span><br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> (<span class="hljs-string">&#x27;W1&#x27;</span>, <span class="hljs-string">&#x27;b1&#x27;</span>, <span class="hljs-string">&#x27;W2&#x27;</span>, <span class="hljs-string">&#x27;b2&#x27;</span>):<br>        network.params[key] -= learning_rate * grad[key]<br>    <br>    loss = network.loss(x_batch, t_batch)<br>    train_loss_list.append(loss)<br>    <br>    <span class="hljs-keyword">if</span> i % iter_per_epoch == <span class="hljs-number">0</span>:<br>        train_acc = network.accuracy(x_train, t_train)<br>        test_acc = network.accuracy(x_test, t_test)<br>        train_acc_list.append(train_acc)<br>        test_acc_list.append(test_acc)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;train acc, test acc | &quot;</span> + <span class="hljs-built_in">str</span>(train_acc) + <span class="hljs-string">&quot;, &quot;</span> + <span class="hljs-built_in">str</span>(test_acc))<br><br><span class="hljs-comment"># 绘制图形</span><br>markers = &#123;<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;s&#x27;</span>&#125;<br>x = np.arange(<span class="hljs-built_in">len</span>(train_acc_list))<br>plt.plot(x, train_acc_list, label=<span class="hljs-string">&#x27;train acc&#x27;</span>)<br>plt.plot(x, test_acc_list, label=<span class="hljs-string">&#x27;test acc&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;epochs&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;accuracy&quot;</span>)<br>plt.ylim(<span class="hljs-number">0</span>, <span class="hljs-number">1.0</span>)<br>plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p>​mini-batch的大小为100，需要每次从60000个训练数据中随机取出100个数据（图像数据和正确解标签数据）。然后，对这个包含100笔数据的mini-batch求梯度，使用随机梯度下降法(SGD)更新参数。这里，梯度法的更新次数（循环的次数）为10000。每更新一次，都对训练数据计算损失函数的值，并把该值添加到数组中。</p><h2 id="基于测试数据的评价"><a href="#基于测试数据的评价" class="headerlink" title="基于测试数据的评价"></a>基于测试数据的评价</h2><p>​通过反复学习可以使损失函数的值逐渐减小。不过这个损失函数的值，严格地讲是“对训练数据的某个mini-batch的损失函数”的值。训练数据的损失函数值减小，虽说是神经网络的学习正常进行的一个信号，但光看这个结果还不能说明该神经网络在其他数据集上也一定能有同等程度的表现。</p><p>​神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数据，即确认是否会发生过拟合。过拟合是指，虽然训练数据中的数字图像能被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象。</p><p>​神经网络学习的最初目标是掌握泛化能力，因此，要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据。</p><p>​epoch是一个单位。一个epoch表示学习中所有训练数据均被使用过一次时的更新次数。比如，对于10000笔训练数据，用大小为100笔数据的mini-batch进行学习时，重复随机梯度下降法100次，所有的训练数据就都被“看过”了。此时，100次就是一个epoch。</p><p>​在上面的例子中，每经过一个epoch，就对所有的训练数据和测试数据计算识别精度，并记录结果。之所以要计算每一个epoch的识别精度，是因为如果在for语句的循环中一直计算识别精度，会花费太多时间。并且，也没有必要那么频繁地记录识别精度（只要从大方向上大致把握识别精度的推移就可以了）。因此，我们才会每经过一个epoch就记录一次训练数据的识别精度。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>​首先，为了能顺利进行神经网络的学习，我们导入了损失函数这个指标。以这个损失函数为基准，找出使它的值达到最小的权重参数，就是神经网络学习的目标。为了找到尽可能小的损失函数值，我们介绍了使用函数斜率的梯度法。</p><ul><li>机器学习中使用的数据集分为训练数据和测试数据。</li><li>神经网络用训练数据进行学习，并用测试数据评价学习到的模型的泛化能力。</li><li>神经网络的学习以损失函数为指标，更新权重参数，以使损失函数的值减小。</li><li>利用某个给定的微小值的差分求导数的过程，称为数值微分。利</li><li>用数值微分，可以计算权重参数的梯度。</li><li>数值微分虽然费时间，但是实现起来很简单。</li></ul><h1 id="补充：利用PyTorch实现高速计算"><a href="#补充：利用PyTorch实现高速计算" class="headerlink" title="补充：利用PyTorch实现高速计算"></a>补充：利用PyTorch实现高速计算</h1><h2 id="2层神经网络的类-1"><a href="#2层神经网络的类-1" class="headerlink" title="2层神经网络的类"></a>2层神经网络的类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> itertools<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TwoLayerNet</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size, device=<span class="hljs-string">&#x27;cuda&#x27;</span>, weight_init_std=<span class="hljs-number">0.01</span></span>):<br>        <span class="hljs-variable language_">self</span>.device = device<br><br>        <span class="hljs-comment"># 参数初始化 (替换为 PyTorch 张量)</span><br>        <span class="hljs-variable language_">self</span>.params = &#123;<br>            <span class="hljs-string">&#x27;W1&#x27;</span>: torch.nn.Parameter(weight_init_std * torch.randn(input_size, hidden_size, device=device)),<br>            <span class="hljs-string">&#x27;b1&#x27;</span>: torch.nn.Parameter(torch.zeros(hidden_size, device=device)),<br>            <span class="hljs-string">&#x27;W2&#x27;</span>: torch.nn.Parameter(weight_init_std * torch.randn(hidden_size, output_size, device=device)),<br>            <span class="hljs-string">&#x27;b2&#x27;</span>: torch.nn.Parameter(torch.zeros(output_size, device=device))<br>        &#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 输入类型转换</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(x, torch.Tensor):<br>            x = torch.tensor(x, dtype=torch.float32, device=<span class="hljs-variable language_">self</span>.device)<br><br>        W1, W2 = <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W1&#x27;</span>], <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W2&#x27;</span>]<br>        b1, b2 = <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b1&#x27;</span>], <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b2&#x27;</span>]<br><br>        <span class="hljs-comment"># 严格保持原始计算流程</span><br>        a1 = torch.matmul(x, W1) + b1<br>        z1 = torch.sigmoid(a1)<br>        a2 = torch.matmul(z1, W2) + b2<br>        y = F.softmax(a2, dim=<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">return</span> y<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, x, t</span>):<br>        <span class="hljs-comment"># 转换监督数据</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(t, torch.Tensor):<br>            t = torch.tensor(t, dtype=torch.float32, device=<span class="hljs-variable language_">self</span>.device)<br><br>        y = <span class="hljs-variable language_">self</span>.predict(x)<br><br>        <span class="hljs-comment"># 手工实现交叉熵误差 (与原始逻辑一致)</span><br>        delta = <span class="hljs-number">1e-7</span>  <span class="hljs-comment"># 防止log(0)</span><br>        batch_size = y.shape[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">return</span> -torch.<span class="hljs-built_in">sum</span>(t * torch.log(y + delta)) / batch_size<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">accuracy</span>(<span class="hljs-params">self, x, t</span>):<br>        <span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment"># 禁用梯度跟踪</span><br>            y = <span class="hljs-variable language_">self</span>.predict(x)<br>            y_labels = torch.argmax(y, dim=<span class="hljs-number">1</span>)<br><br>            <span class="hljs-comment"># 兼容 one-hot 和普通标签</span><br>            <span class="hljs-keyword">if</span> t.dim() &gt; <span class="hljs-number">1</span>:<br>                t_labels = torch.argmax(t, dim=<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">else</span>:<br>                t_labels = t.long()<br><br>            <span class="hljs-keyword">return</span> (y_labels == t_labels).<span class="hljs-built_in">float</span>().mean().item()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient</span>(<span class="hljs-params">self, x, t</span>):<br>        h = <span class="hljs-number">1e-4</span>  <span class="hljs-comment"># 保持原始步长</span><br>        grads = &#123;&#125;<br><br>        <span class="hljs-comment"># 确保输入为张量</span><br>        x = x <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(x, torch.Tensor) <span class="hljs-keyword">else</span> torch.tensor(x, device=<span class="hljs-variable language_">self</span>.device)<br>        t = t <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(t, torch.Tensor) <span class="hljs-keyword">else</span> torch.tensor(t, device=<span class="hljs-variable language_">self</span>.device)<br><br>        <span class="hljs-comment"># 遍历所有参数</span><br>        <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.params:<br>            param = <span class="hljs-variable language_">self</span>.params[key]<br>            grad = torch.zeros_like(param)<br><br>            <span class="hljs-comment"># 生成所有可能的多维索引</span><br>            dims = param.shape<br>            indices = itertools.product(*[<span class="hljs-built_in">range</span>(dim) <span class="hljs-keyword">for</span> dim <span class="hljs-keyword">in</span> dims])<br><br>            <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> indices:<br>                <span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment"># 禁用梯度追踪</span><br>                    original_val = param[idx].item()<br>                    param[idx] = original_val + h<br><br>                fxh1 = <span class="hljs-variable language_">self</span>.loss(x, t).item()<br><br>                <span class="hljs-keyword">with</span> torch.no_grad():<br>                    param[idx] = original_val - h<br><br>                fxh2 = <span class="hljs-variable language_">self</span>.loss(x, t).item()<br><br>                <span class="hljs-keyword">with</span> torch.no_grad():<br>                    param[idx] = original_val  <span class="hljs-comment"># 恢复原始值</span><br><br>                grad[idx] = (fxh1 - fxh2) / (<span class="hljs-number">2</span> * h)<br><br>            grads[key] = grad<br><br>        <span class="hljs-keyword">return</span> grads<br></code></pre></td></tr></table></figure><p>​不建议测试，理解梯度的前向传播即可，没有反向传播的梯度变化耗时过长。</p><h2 id="mini-batch的实现-1"><a href="#mini-batch的实现-1" class="headerlink" title="mini-batch的实现"></a>mini-batch的实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># cs.py</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir) <span class="hljs-comment"># 为了导入父目录中的文件而进行的设定</span><br><span class="hljs-keyword">from</span> dataset.mnist <span class="hljs-keyword">import</span> load_mnist<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader, TensorDataset<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-keyword">from</span> two_layer_net_torch <span class="hljs-keyword">import</span> TwoLayerNet<br><br><span class="hljs-comment"># 数据加载与转换</span><br>(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="hljs-literal">True</span>, one_hot_label=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 转换为 PyTorch 张量并发送到设备</span><br>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span><br><br>x_train = torch.tensor(x_train, dtype=torch.float32, device=device)<br>t_train = torch.tensor(t_train, dtype=torch.float32, device=device)<br>x_test = torch.tensor(x_test, dtype=torch.float32, device=device)<br>t_test = torch.tensor(t_test, dtype=torch.float32, device=device)<br><br><span class="hljs-comment"># 创建数据加载器</span><br>train_dataset = TensorDataset(x_train, t_train)<br>train_loader = DataLoader(train_dataset, batch_size=<span class="hljs-number">100</span>, shuffle=<span class="hljs-literal">True</span>)<br><br>test_dataset= TensorDataset(x_test,t_test)<br>test_loader= DataLoader(test_dataset, batch_size=<span class="hljs-number">100</span>, shuffle=<span class="hljs-literal">True</span>)<br><br><br>network = TwoLayerNet(input_size=<span class="hljs-number">784</span>, hidden_size=<span class="hljs-number">50</span>, output_size=<span class="hljs-number">10</span>, device=device)<br><br><span class="hljs-comment">#迭代次数与epoch设置</span><br>iters_num=<span class="hljs-number">10000</span><br>train_size = x_train.shape[<span class="hljs-number">0</span>]<br>batch_size = <span class="hljs-number">100</span><br>learning_rate = <span class="hljs-number">0.1</span><br>iter_per_epoch = <span class="hljs-built_in">max</span>(train_size / batch_size, <span class="hljs-number">1</span>)<br><br><span class="hljs-comment">#中间数据的记录</span><br>train_loss_list = []<br>train_acc_list = []<br>test_acc_list = []<br><br>start_time=time.time()<br><br><span class="hljs-comment"># 主训练循环</span><br>train_iter=<span class="hljs-built_in">iter</span>(train_loader);<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iters_num):<br>    <span class="hljs-comment"># 随机获取批量数据</span><br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-comment"># 获取下一个批次 (自动维持迭代状态)</span><br>        x_batch, t_batch = <span class="hljs-built_in">next</span>(train_iter)<br>    <span class="hljs-keyword">except</span> StopIteration:<br>        <span class="hljs-comment"># 当遍历完所有数据后，重新创建打乱的迭代器</span><br>        train_iter = <span class="hljs-built_in">iter</span>(train_loader)  <br>        x_batch, t_batch = <span class="hljs-built_in">next</span>(train_iter)<br><br>    <span class="hljs-comment"># 设备转移</span><br>    x_batch = x_batch.to(device, non_blocking=<span class="hljs-literal">True</span>)<br>    t_batch = t_batch.to(device, non_blocking=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-comment"># 计算梯度</span><br>    grads = network.gradient(x_batch, t_batch)<br><br>    <span class="hljs-comment"># 参数更新</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> network.params:<br>            network.params[key] -= learning_rate * grads[key]<br><br>    <span class="hljs-comment"># 记录损失</span><br>    loss = network.loss(x_batch, t_batch)<br>    train_loss_list.append(loss.item())<br><br>    <span class="hljs-comment"># 周期验证</span><br>    <span class="hljs-keyword">if</span> i % iter_per_epoch == <span class="hljs-number">0</span>:  <span class="hljs-comment"># 每epoch验证一次（MNIST共600 batch/epoch）</span><br>        <span class="hljs-comment"># 训练集精度</span><br>        correct_train = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> x, t <span class="hljs-keyword">in</span> train_loader:<br>            x = x.to(device)<br>            t = t.to(device)<br>            correct_train += (network.accuracy(x, t) * x.shape[<span class="hljs-number">0</span>])<br>        train_acc = correct_train / <span class="hljs-built_in">len</span>(train_dataset)<br><br>        <span class="hljs-comment"># 测试集精度</span><br>        correct_test = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> x, t <span class="hljs-keyword">in</span> test_loader:<br>            x = x.to(device)<br>            t = t.to(device)<br>            correct_test += (network.accuracy(x, t) * x.shape[<span class="hljs-number">0</span>])<br>        test_acc = correct_test / <span class="hljs-built_in">len</span>(test_dataset)<br><br>        train_acc_list.append(train_acc)<br>        test_acc_list.append(test_acc)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Iter <span class="hljs-subst">&#123;i&#125;</span>: Train Acc <span class="hljs-subst">&#123;train_acc:<span class="hljs-number">.4</span>f&#125;</span>, Test Acc <span class="hljs-subst">&#123;test_acc:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br>        <br>        end_time = time.time()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Cost:&#123;:.2f&#125;秒&quot;</span>.<span class="hljs-built_in">format</span>(end_time - start_time))<br>        start_time = time.time()<br>        <br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">for epoch in range(10):  # 更改为 epoch 循环</span><br><span class="hljs-string">    # 训练阶段</span><br><span class="hljs-string">    for x_batch, t_batch in train_loader:</span><br><span class="hljs-string">        grads = network.gradient(x_batch, t_batch)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        # 使用优化器更新参数</span><br><span class="hljs-string">        with torch.no_grad():</span><br><span class="hljs-string">            for name, param in network.params.items():</span><br><span class="hljs-string">                param -= learning_rate * grads[name]</span><br><span class="hljs-string"></span><br><span class="hljs-string">        # 记录损失</span><br><span class="hljs-string">        loss = network.loss(x_batch, t_batch)</span><br><span class="hljs-string">        train_loss_list.append(loss.item())</span><br><span class="hljs-string"></span><br><span class="hljs-string">    # 验证阶段</span><br><span class="hljs-string">    train_acc = network.accuracy(x_train, t_train)</span><br><span class="hljs-string">    test_acc = network.accuracy(x_test, t_test)</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    train_acc_list.append(train_acc)</span><br><span class="hljs-string">    test_acc_list.append(test_acc)</span><br><span class="hljs-string">    print(f&quot;Epoch &#123;epoch + 1&#125;: Train Acc &#123;train_acc:.4f&#125;, Test Acc &#123;test_acc:.4f&#125;&quot;)</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    end_time = time.time()</span><br><span class="hljs-string">    print(&quot;Cost:&#123;:.2f&#125;秒&quot;.format(end_time - start_time))</span><br><span class="hljs-string">    start_time=time.time()</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># 可视化结果</span><br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">#loss迭代结果展示</span><br><span class="hljs-string">x = np.arange(len(train_loss_list))</span><br><span class="hljs-string">plt.plot(x, train_loss_list, label=&#x27;loss&#x27;)</span><br><span class="hljs-string"></span><br><span class="hljs-string">plt.xlabel(&quot;iteration&quot;)</span><br><span class="hljs-string">plt.ylabel(&quot;loss&quot;)</span><br><span class="hljs-string"></span><br><span class="hljs-string">plt.ylim(0,3)</span><br><span class="hljs-string">plt.gca().margins(x=0)</span><br><span class="hljs-string"></span><br><span class="hljs-string">plt.show()</span><br><span class="hljs-string"></span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>markers = &#123;<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;s&#x27;</span>&#125;<br>x = np.arange(<span class="hljs-built_in">len</span>(train_acc_list))<br><br>plt.plot(x, train_acc_list, label=<span class="hljs-string">&#x27;train acc&#x27;</span>)<br>plt.plot(x, test_acc_list, label=<span class="hljs-string">&#x27;test acc&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>)<br><br>plt.xlabel(<span class="hljs-string">&quot;epochs&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;accuracy&quot;</span>)<br>plt.ylim(<span class="hljs-number">0</span>, <span class="hljs-number">1.0</span>)<br>plt.gca().margins(x=<span class="hljs-number">0</span>)<br><br>plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># cs.py</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir) <span class="hljs-comment"># 为了导入父目录中的文件而进行的设定</span><br><span class="hljs-keyword">from</span> dataset.mnist <span class="hljs-keyword">import</span> load_mnist<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader, TensorDataset<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> two_layer_net_torch <span class="hljs-keyword">import</span> TwoLayerNet<br><br><span class="hljs-comment"># 数据加载与转换</span><br>(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="hljs-literal">True</span>, one_hot_label=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 转换为 PyTorch 张量并发送到设备</span><br>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span><br><br>x_train = torch.tensor(x_train, dtype=torch.float32, device=device)<br>t_train = torch.tensor(t_train, dtype=torch.float32, device=device)<br>x_test = torch.tensor(x_test, dtype=torch.float32, device=device)<br>t_test = torch.tensor(t_test, dtype=torch.float32, device=device)<br><br><span class="hljs-comment"># 创建数据加载器</span><br>train_dataset = TensorDataset(x_train, t_train)<br>train_loader = DataLoader(train_dataset, batch_size=<span class="hljs-number">100</span>, shuffle=<span class="hljs-literal">True</span>)<br><br>network = TwoLayerNet(input_size=<span class="hljs-number">784</span>, hidden_size=<span class="hljs-number">50</span>, output_size=<span class="hljs-number">10</span>, device=device)<br><br><br>train_loss_list = []<br>train_acc_list = []<br>test_acc_list = []<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):  <span class="hljs-comment"># 更改为 epoch 循环</span><br>    <span class="hljs-comment"># 训练阶段</span><br>    <span class="hljs-comment">#network.train()</span><br>    <span class="hljs-keyword">for</span> x_batch, t_batch <span class="hljs-keyword">in</span> train_loader:<br>        grads = network.gradient(x_batch, t_batch)<br><br>        <span class="hljs-comment"># 使用优化器更新参数</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> network.params.items():<br>                param -= <span class="hljs-number">0.1</span> * grads[name]<br><br>        <span class="hljs-comment"># 记录损失</span><br>        loss = network.loss(x_batch, t_batch)<br>        train_loss_list.append(loss.item())<br><br>    <span class="hljs-comment"># 验证阶段</span><br>    <span class="hljs-comment">#network.eval()</span><br>    train_acc = network.accuracy(x_train, t_train)<br>    test_acc = network.accuracy(x_test, t_test)<br>    train_acc_list.append(train_acc)<br>    test_acc_list.append(test_acc)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span>: Train Acc <span class="hljs-subst">&#123;train_acc:<span class="hljs-number">.4</span>f&#125;</span>, Test Acc <span class="hljs-subst">&#123;test_acc:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 可视化结果</span><br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">x = np.arange(len(train_loss_list))</span><br><span class="hljs-string">plt.plot(x, train_loss_list, label=&#x27;loss&#x27;)</span><br><span class="hljs-string"></span><br><span class="hljs-string">plt.xlabel(&quot;iteration&quot;)</span><br><span class="hljs-string">plt.ylabel(&quot;loss&quot;)</span><br><span class="hljs-string"></span><br><span class="hljs-string">plt.ylim(0,3)</span><br><span class="hljs-string">plt.gca().margins(x=0)</span><br><span class="hljs-string"></span><br><span class="hljs-string">plt.show()</span><br><span class="hljs-string"></span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>markers = &#123;<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;s&#x27;</span>&#125;<br>x = np.arange(<span class="hljs-built_in">len</span>(train_acc_list))<br><br>plt.plot(x, train_acc_list, label=<span class="hljs-string">&#x27;train acc&#x27;</span>)<br>plt.plot(x, test_acc_list, label=<span class="hljs-string">&#x27;test acc&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>)<br><br>plt.xlabel(<span class="hljs-string">&quot;epochs&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;accuracy&quot;</span>)<br>plt.ylim(<span class="hljs-number">0</span>, <span class="hljs-number">1.0</span>)<br>plt.gca().margins(x=<span class="hljs-number">0</span>)<br><br>plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI-Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于鱼书（第一册）的知识整理（1）</title>
    <link href="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/"/>
    <url>/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p>​对于复杂的函数，感知机也隐含着能够表示它的可能性。即便是计算机进行的复杂处理，感知机（理论上）也可以将其表示出来。但坏消息是，设定权重的工作，即确定合适的、能符合预期的输入与输出的权重，现在还是由人工进行的。</p><p>​神经网络的出现就是为了解决刚才的坏消息。具体地讲，神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数。</p><h2 id="简单例子"><a href="#简单例子" class="headerlink" title="简单例子"></a>简单例子</h2><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AE%80%E5%8D%95%E4%BE%8B%E5%AD%90.png"></p><p>​中间层有时也称为隐藏层。“隐藏”一词的意思是，隐藏层的神经元（和输入层、输出层不同）肉眼看不见。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>​如“激活”一词所示，激活函数的作用在于决定如何来激活输入信号的总和。</p><p>​“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数的模型。“多层感知机”是指神经网络，即使用sigmoid函数等平滑的激活函数的多层网络。</p><h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><p>$$<br>\sigma(x) &#x3D; \frac{1}{1 + e^{-x}}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pylab <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br>x = np.arange(-<span class="hljs-number">5.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">0.1</span>)<br>y = sigmoid(x)<br>plt.plot(x, y)<br>plt.ylim(-<span class="hljs-number">0.1</span>, <span class="hljs-number">1.1</span>) <span class="hljs-comment"># 指定y轴的范围</span><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/sigmoid%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%BE%E5%BD%A2.png"></p><p>​感知机中神经元之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数值信号。</p><p>​神经网络的激活函数必须使用非线性函数。换句话说，激活函数不能使用线性函数。因为线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。因此，为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。</p><h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><p>$$<br>\text{ReLU}(x) &#x3D; \max(0, x)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pylab <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> np.maximum(<span class="hljs-number">0</span>, x)<br><br>x = np.arange(-<span class="hljs-number">5.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">0.1</span>)<br>y = sigmoid(x)<br>plt.plot(x, y)<br>plt.ylim(-<span class="hljs-number">1</span>, <span class="hljs-number">5</span>) <span class="hljs-comment"># 指定y轴的范围</span><br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/ReLU%E5%87%BD%E6%95%B0%E7%9A%84%E5%9B%BE%E5%BD%A2.png"></p><h2 id="3层神经网络的实现"><a href="#3层神经网络的实现" class="headerlink" title="3层神经网络的实现"></a>3层神经网络的实现</h2><h3 id="矩阵的运算"><a href="#矩阵的运算" class="headerlink" title="矩阵的运算"></a>矩阵的运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#矩阵的点积/内积</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>np.dot(A,B)<br></code></pre></td></tr></table></figure><p>​巧妙地使用NumPy数组，可以用很少的代码完成神经网络的前向处理。</p><h3 id="符号确认"><a href="#符号确认" class="headerlink" title="符号确认"></a>符号确认</h3><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/%E6%9D%83%E9%87%8D%E7%9A%84%E7%AC%A6%E5%8F%B7.png"></p><h3 id="各层间信号传递的实现"><a href="#各层间信号传递的实现" class="headerlink" title="各层间信号传递的实现"></a>各层间信号传递的实现</h3><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/%E4%BB%8E%E8%BE%93%E5%85%A5%E5%B1%82%E5%88%B0%E7%AC%AC1%E5%B1%82%E7%9A%84%E4%BF%A1%E5%8F%B7%E4%BC%A0%E9%80%92.png"></p><p>​上图增加了表示偏置的神经元“1”。请注意，偏置的右下角的索引号只有一个。这是因为前一层的偏置神经元（神经元“1”）只有一个。</p><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/%E4%BB%8E%E8%BE%93%E5%85%A5%E5%B1%82%E5%88%B0%E7%AC%AC1%E5%B1%82%E7%9A%84%E4%BF%A1%E5%8F%B7%E4%BC%A0%E9%80%921.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">X = np.array([<span class="hljs-number">1.0</span>, <span class="hljs-number">0.5</span>])<br>W1 = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>]])<br>B1 = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>])<br><br><span class="hljs-built_in">print</span>(W1.shape) <span class="hljs-comment"># (2, 3)</span><br><span class="hljs-built_in">print</span>(X.shape) <span class="hljs-comment"># (2,)</span><br><span class="hljs-built_in">print</span>(B1.shape) <span class="hljs-comment"># (3,)</span><br><br>A1 = np.dot(X, W1) + B1<br></code></pre></td></tr></table></figure><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/%E7%AC%AC1%E5%B1%82%E4%B8%AD%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E8%AE%A1%E7%AE%97.png"></p><p>​隐藏层的加权和（加权信号和偏置的总和）用a表示，被激活函数转换后的信号用z表示。此外，图中h()表示激活函数，这里我们使用的是sigmoid函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">Z1 = sigmoid(A1)<br><br><span class="hljs-built_in">print</span>(A1) <span class="hljs-comment"># [0.3, 0.7, 1.1]</span><br><span class="hljs-built_in">print</span>(Z1) <span class="hljs-comment"># [0.57444252, 0.66818777, 0.75026011]</span><br></code></pre></td></tr></table></figure><p>​第1层到第2层的信号传递如下：</p><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/%E7%AC%AC1%E5%B1%82%E5%88%B0%E7%AC%AC2%E5%B1%82%E7%9A%84%E4%BF%A1%E5%8F%B7%E4%BC%A0%E9%80%92.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">W2 = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.4</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.6</span>]])<br>B2 = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>])<br><br><span class="hljs-built_in">print</span>(Z1.shape) <span class="hljs-comment"># (3,)</span><br><span class="hljs-built_in">print</span>(W2.shape) <span class="hljs-comment"># (3, 2)</span><br><span class="hljs-built_in">print</span>(B2.shape) <span class="hljs-comment"># (2,)</span><br><br>A2 = np.dot(Z1, W2) + B2<br>Z2 = sigmoid(A2)<br><br></code></pre></td></tr></table></figure><p>​最后是第2层到输出层的信号传递:</p><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/%E7%AC%AC2%E5%B1%82%E5%88%B0%E8%BE%93%E5%87%BA%E5%B1%82%E7%9A%84%E4%BF%A1%E5%8F%B7%E4%BC%A0%E9%80%92.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">identity_function</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> x<br><br>W3 = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>]])<br>B3 = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>])<br><br>A3 = np.dot(Z2, W3) + B3<br>Y = identity_function(A3) <span class="hljs-comment"># 或者Y = A3</span><br></code></pre></td></tr></table></figure><p>​这里我们定义了identity_function()函数（也称为“恒等函数”），并将其作为输出层的激活函数。恒等函数会将输入按原样输出，因此，这个例子中没有必要特意定义identity_function()。这里这样实现只是为了和之前的流程保持统一。另外，输出层的激活函数用σ()表示，不同于隐藏层的激活函数h()（σ读作sigma）。</p><h3 id="代码小结"><a href="#代码小结" class="headerlink" title="代码小结"></a>代码小结</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">identity_function</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> x<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_network</span>():<br>    network = &#123;&#125;<br>    network[<span class="hljs-string">&#x27;W1&#x27;</span>] = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.6</span>]])<br>    network[<span class="hljs-string">&#x27;b1&#x27;</span>] = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>])<br>    network[<span class="hljs-string">&#x27;W2&#x27;</span>] = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.4</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.5</span>], [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.6</span>]])<br>    network[<span class="hljs-string">&#x27;b2&#x27;</span>] = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>])<br>    network[<span class="hljs-string">&#x27;W3&#x27;</span>] = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.3</span>], [<span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>]])<br>    network[<span class="hljs-string">&#x27;b3&#x27;</span>] = np.array([<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>])<br><br>    <span class="hljs-keyword">return</span> network<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">network, x</span>):<br>    W1, W2, W3 = network[<span class="hljs-string">&#x27;W1&#x27;</span>], network[<span class="hljs-string">&#x27;W2&#x27;</span>], network[<span class="hljs-string">&#x27;W3&#x27;</span>]<br>    b1, b2, b3 = network[<span class="hljs-string">&#x27;b1&#x27;</span>], network[<span class="hljs-string">&#x27;b2&#x27;</span>], network[<span class="hljs-string">&#x27;b3&#x27;</span>]<br><br>    a1 = np.dot(x, W1) + b1<br>    z1 = sigmoid(a1)<br>    a2 = np.dot(z1, W2) + b2<br>    z2 = sigmoid(a2)<br>    a3 = np.dot(z2, W3) + b3<br>    y = identity_function(a3)<br><br>    <span class="hljs-keyword">return</span> y<br><br>network = init_network()<br>x = np.array([<span class="hljs-number">1.0</span>, <span class="hljs-number">0.5</span>])<br>y = forward(network, x)<br><span class="hljs-built_in">print</span>(y) <span class="hljs-comment"># [ 0.31682708 0.69627909]</span><br><br><br></code></pre></td></tr></table></figure><ul><li>init_network()函数进行权重和偏置的初始化，并将它们保存在字典变量network中。这个字典变量network中保存了每一层所需的参数（权重和偏置）。</li><li>forward()函数中封装了将输入信号转换为输出信号的处理过程。</li></ul><h2 id="输出层的设计"><a href="#输出层的设计" class="headerlink" title="输出层的设计"></a>输出层的设计</h2><p>​一般而言，回归问题用恒等函数，分类问题用softmax函数。</p><p>​机器学习的问题大致可以分为分类问题和回归问题。分类问题是数据属于哪一个类别的问题。而回归问题是根据某个输入预测一个（连续的）数值的问题。</p><h3 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h3><p>$$<br>\text{Softmax}(x_i) &#x3D; \frac{e^{x_i}}{\sum_{j} e^{x_j}}<br>$$</p><p>​exp(x)是表示的指数函数（e是纳皮尔常数2.7182 …）。上式表示假设输出层共有n个神经元，计算第k个神经元的输出。softmax函数的分子是输入信号的指数函数，分母是所有输入信号的指数函数的和。</p><p>​softmax函数的输出通过箭头与所有的输入信号相连。这是因为，从式子中可以看出，输出层的各个神经元都受到所有输入信号的影响。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">a</span>):<br>    exp_a = np.exp(a)<br>    sum_exp_a = np.<span class="hljs-built_in">sum</span>(exp_a)<br>    y = exp_a / sum_exp_a<br><br>    <span class="hljs-keyword">return</span> y<br><br></code></pre></td></tr></table></figure><h3 id="实现softmax函数时的注意事项"><a href="#实现softmax函数时的注意事项" class="headerlink" title="实现softmax函数时的注意事项"></a>实现softmax函数时的注意事项</h3><p>​softmax函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大。比如，e^10的值会超过20000，会变成一个后面有40多个0的超大值，e^100的结果会返回一个表示无穷大的inf。如果在这些超大值之间进行除法运算，结果会出现“不确定”的情况。</p><p><img src="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/softmax%E5%87%BD%E6%95%B0%E7%9A%84%E6%94%B9%E8%BF%9B.png"></p><p>​式(3.11)说明，在进行softmax的指数函数的运算时，加上（或者减去）某个常数并不会改变运算的结果。这里的C’可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">a</span>):<br>    c = np.<span class="hljs-built_in">max</span>(a)<br>    exp_a = np.exp(a - c) <span class="hljs-comment"># 溢出对策</span><br>    sum_exp_a = np.<span class="hljs-built_in">sum</span>(exp_a)<br>    y = exp_a / sum_exp_a<br><br>    <span class="hljs-keyword">return</span> y<br><br></code></pre></td></tr></table></figure><h3 id="softmax函数的特征"><a href="#softmax函数的特征" class="headerlink" title="softmax函数的特征"></a>softmax函数的特征</h3><p>​softmax函数的输出是0.0到1.0之间的实数。并且，softmax函数的输出值的总和是1。输出总和为1是softmax函数的一个重要性质。正因为有了这个性质，我们才可以把softmax函数的输出解释为“概率”。</p><p>​一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此，神经网络在进行分类时，输出层的softmax函数可以省略。</p><p>​在实际的问题中，由于指数函数的运算需要一定的计算机运算量，因此输出层的softmax函数一般会被省略。</p><p>​求解机器学习问题的步骤可以分为“学习”和“推理”两个阶段。首先，在学习阶段进行模型的学习，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。</p><p>​如前所述，推理阶段一般会省略输出层的softmax函数。在输出层使用softmax函数是因为它和神经网络的学习有关系。</p><h3 id="输出层的神经元数量"><a href="#输出层的神经元数量" class="headerlink" title="输出层的神经元数量"></a>输出层的神经元数量</h3><p>​对于分类问题，输出层的神经元数量一般设定为类别的数量。</p><h2 id="手写数字识别"><a href="#手写数字识别" class="headerlink" title="手写数字识别"></a>手写数字识别</h2><p>​假设学习已经全部结束，我们使用学习到的参数，先实现神经网络的“推理处理”。这个推理处理也称为神经网络的前向传播(forward propagation)。</p><p>​和求解机器学习问题的步骤（分成学习和推理两个阶段进行）一样，使用神经网络解决问题时，也需要首先使用训练数据（学习数据）进行权重参数的学习；进行推理时，使用刚才学习到的参数，对输入数据进行分类。</p><h3 id="MNIST数据集"><a href="#MNIST数据集" class="headerlink" title="MNIST数据集"></a>MNIST数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#需要科学上网下载数据集</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir) <span class="hljs-comment"># 为了导入父目录中的文件而进行的设定</span><br><span class="hljs-keyword">from</span> dataset.mnist <span class="hljs-keyword">import</span> load_mnist<br><br><span class="hljs-comment"># 第一次调用会花费几分钟……</span><br>(x_train, t_train), (x_test, t_test) = load_mnist(flatten=<span class="hljs-literal">True</span>,<br>normalize=<span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># 输出各个数据的形状</span><br><span class="hljs-built_in">print</span>(x_train.shape) <span class="hljs-comment"># (60000, 784)</span><br><span class="hljs-built_in">print</span>(t_train.shape) <span class="hljs-comment"># (60000,)</span><br><span class="hljs-built_in">print</span>(x_test.shape) <span class="hljs-comment"># (10000, 784)</span><br><span class="hljs-built_in">print</span>(t_test.shape) <span class="hljs-comment"># (10000,)</span><br>·<br></code></pre></td></tr></table></figure><p>​load_mnist 函数以“（训练图像,训练标签），（测试图像,测试标签）”的形式返回读入的MNIST数据。</p><p>​还可以像load_mnist(normalize&#x3D;True, flatten&#x3D;True, one_hot_label&#x3D;False)这样，设置3个参数。</p><ul><li>第1个参数normalize设置是否将输入图像正规化为0.0～1.0的值。如果将该参数设置为False，则输入图像的像素会保持原来的0～255。</li><li>第2个参数flatten设置是否展开输入图像（变成一维数组）。如果将该参数设置为False，则输入图像为1×28×28的三维数组；若设置为True，则输入图像会保存为由784个元素构成的一维数组。</li><li>第3个参数one_hot_label设置是否将标签保存为one-hot表示(one-hot representation)。one-hot表示是仅正确解标签为1，其余皆为0的数组，就像[0,0,1,0,0,0,0,0,0,0]这样。当one_hot_label为False时，只是像7、2这样简单保存正确解标签；当one_hot_label为True时，标签则保存为one-hot表示。</li></ul><p>​Python有pickle这个便利的功能。这个功能可以将程序运行中的对象保存为文件。如果加载保存过的pickle文件，可以立刻复原之前程序运行中的对象。用于读入MNIST数据集的load_mnist()函数内部也使用了pickle功能（在第2次及以后读入时）。利用pickle功能，可以高效地完成MNIST数据的准备工作。</p><p>​试着显示MNIST图像：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir)<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> dataset.mnist <span class="hljs-keyword">import</span> load_mnist<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">img_show</span>(<span class="hljs-params">img</span>):<br>    pil_img = Image.fromarray(np.uint8(img))<br>    pil_img.show()<br><br>(x_train, t_train), (x_test, t_test) = load_mnist(flatten=<span class="hljs-literal">True</span>,<br>normalize=<span class="hljs-literal">False</span>)<br>img = x_train[<span class="hljs-number">0</span>]<br>label = t_train[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(label) <span class="hljs-comment"># 5</span><br><br><span class="hljs-built_in">print</span>(img.shape)          <span class="hljs-comment"># (784,)</span><br>img = img.reshape(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>) <span class="hljs-comment"># 把图像的形状变成原来的尺寸</span><br><span class="hljs-built_in">print</span>(img.shape)          <span class="hljs-comment"># (28, 28)</span><br><br>img_show(img)<br><br></code></pre></td></tr></table></figure><p>​flatten&#x3D;True时读入的图像是以一列（一维）NumPy数组的形式保存的。因此，显示图像时，需要把它变为原来的28像素×28像素的形状。可以通过reshape()方法的参数指定期望的形状，更改NumPy数组的形状。此外，还需要把保存为NumPy数组的图像数据转换为PIL用的数据对象，这个转换处理由Image.fromarray()来完成。</p><h3 id="神经网络的推理处理"><a href="#神经网络的推理处理" class="headerlink" title="神经网络的推理处理"></a>神经网络的推理处理</h3><p>​对这个MNIST数据集实现神经网络的推理处理。神经网络的输入层有784个神经元，输出层有10个神经元。输入层的784这个数字来源于图像大小的28×28 &#x3D; 784，输出层的10这个数字来源于10类别分类（数字0到9，共10类别）。此外，这个神经网络有2个隐藏层，第1个隐藏层有50个神经元，第2个隐藏层有100个神经元。这个50和100可以设置为任何值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># coding: utf-8</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir)  <span class="hljs-comment"># 为了导入父目录的文件而进行的设定</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">from</span> dataset.mnist <span class="hljs-keyword">import</span> load_mnist<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">if</span> x.ndim == <span class="hljs-number">2</span>:<br>        x = x.T<br>        x = x - np.<span class="hljs-built_in">max</span>(x, axis=<span class="hljs-number">0</span>)<br>        y = np.exp(x) / np.<span class="hljs-built_in">sum</span>(np.exp(x), axis=<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> y.T<br><br>    x = x - np.<span class="hljs-built_in">max</span>(x) <span class="hljs-comment"># 溢出对策</span><br>    <span class="hljs-keyword">return</span> np.exp(x) / np.<span class="hljs-built_in">sum</span>(np.exp(x))<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_data</span>():<br>    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="hljs-literal">True</span>, flatten=<span class="hljs-literal">True</span>, one_hot_label=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">return</span> x_test, t_test<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_network</span>():<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;sample_weight.pkl&quot;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        network = pickle.load(f)<br>    <span class="hljs-keyword">return</span> network<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">network, x</span>):<br>    W1, W2, W3 = network[<span class="hljs-string">&#x27;W1&#x27;</span>], network[<span class="hljs-string">&#x27;W2&#x27;</span>], network[<span class="hljs-string">&#x27;W3&#x27;</span>]<br>    b1, b2, b3 = network[<span class="hljs-string">&#x27;b1&#x27;</span>], network[<span class="hljs-string">&#x27;b2&#x27;</span>], network[<span class="hljs-string">&#x27;b3&#x27;</span>]<br><br>    a1 = np.dot(x, W1) + b1<br>    z1 = sigmoid(a1)<br>    a2 = np.dot(z1, W2) + b2<br>    z2 = sigmoid(a2)<br>    a3 = np.dot(z2, W3) + b3<br>    y = softmax(a3)<br><br>    <span class="hljs-keyword">return</span> y<br><br><br>x, t = get_data()<br>network = init_network()<br>accuracy_cnt = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x)):<br>    y = predict(network, x[i])<br>    p= np.argmax(y) <span class="hljs-comment"># 获取概率最高的元素的索引</span><br>    <span class="hljs-keyword">if</span> p == t[i]:<br>        accuracy_cnt += <span class="hljs-number">1</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy:&quot;</span> + <span class="hljs-built_in">str</span>(<span class="hljs-built_in">float</span>(accuracy_cnt) / <span class="hljs-built_in">len</span>(x)))<br></code></pre></td></tr></table></figure><p>​init_network()会读入保存在pickle文件sample_weight.pkl中的学习到的权重参数。这个文件中以字典变量的形式保存了权重和偏置参数。</p><p>​现在，我们用这3个函数来实现神经网络的推理处理。然后，评价它的识别精度(accuracy)，即能在多大程度上正确分类。</p><p>​predict()函数以NumPy数组的形式输出各个标签对应的概率。比如输出[0.1, 0.3, 0.2, …, 0.04]的数组，该数组表示“0”的概率为0.1，“1”的概率为0.3，等等。然后，我们取出这个概率列表中的最大值的索引（第几个元素的概率最高），作为预测结果。可以用np.argmax(x)函数取出数组中的最大值的索引，np.argmax(x)将获取被赋给参数x的数组中的最大值元素的索引。最后，比较神经网络所预测的答案和正确解标签，将回答正确的概率作为识别精度。</p><p>​在这个例子中，我们把load_mnist函数的参数normalize设置成了True。将normalize设置成True后，函数内部会进行转换，将图像的各个像素值除以255，使得数据的值在0.0～1.0的范围内。像这样把数据限定到某个范围内的处理称为正规化(normalization)。此外，对神经网络的输入数据进行某种既定的转换称为预处理(pre-processing)。这里，作为对输入图像的一种预处理，我们进行了正规化。</p><h2 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h2><p>​这种打包式的输入数据称为批(batch)。</p><p>​批处理对计算机的运算大有利处，可以大幅缩短每张图像的处理时间。</p><p>​为什么批处理可以缩短处理时间呢？这是因为大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化。并且，在神经网络的运算中，当数据传送成为瓶颈时，批处理可以减轻数据总线的负荷（严格地讲，相对于数据读入，可以将更多的时间用在计算上）。也就是说，批处理一次性计算大型数组要比分开逐步计算各个小型数组速度更快。</p><p>​我们给定了参数axis&#x3D;1。这指定了在100×10的数组中，沿着第1维方向（以第1维为轴）找到值最大的元素的索引（第0维对应第1个维度）。</p><p>​使用批处理，可以实现高速且高效的运算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">x, t = get_data()<br>network = init_network()<br><br>batch_size = <span class="hljs-number">100</span> <span class="hljs-comment"># 批数量</span><br>accuracy_cnt = <span class="hljs-number">0</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(x), batch_size):<br>    x_batch = x[i:i+batch_size]<br>    y_batch = predict(network, x_batch)<br>    p = np.argmax(y_batch, axis=<span class="hljs-number">1</span>)<br>    accuracy_cnt += np.<span class="hljs-built_in">sum</span>(p == t[i:i+batch_size])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># coding: utf-8</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir)  <span class="hljs-comment"># 为了导入父目录的文件而进行的设定</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">from</span> dataset.mnist <span class="hljs-keyword">import</span> load_mnist<br><span class="hljs-keyword">from</span> functions <span class="hljs-keyword">import</span> sigmoid, softmax<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_data</span>():<br>    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="hljs-literal">True</span>, flatten=<span class="hljs-literal">True</span>, one_hot_label=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">return</span> x_test, t_test<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_network</span>():<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;sample_weight.pkl&quot;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        network = pickle.load(f)<br>    <span class="hljs-keyword">return</span> network<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">network, x</span>):<br>    w1, w2, w3 = network[<span class="hljs-string">&#x27;W1&#x27;</span>], network[<span class="hljs-string">&#x27;W2&#x27;</span>], network[<span class="hljs-string">&#x27;W3&#x27;</span>]<br>    b1, b2, b3 = network[<span class="hljs-string">&#x27;b1&#x27;</span>], network[<span class="hljs-string">&#x27;b2&#x27;</span>], network[<span class="hljs-string">&#x27;b3&#x27;</span>]<br><br>    a1 = np.dot(x, w1) + b1<br>    z1 = sigmoid(a1)<br>    a2 = np.dot(z1, w2) + b2<br>    z2 = sigmoid(a2)<br>    a3 = np.dot(z2, w3) + b3<br>    y = softmax(a3)<br><br>    <span class="hljs-keyword">return</span> y<br><br><br>x, t = get_data()<br>network = init_network()<br><br>batch_size = <span class="hljs-number">100</span> <span class="hljs-comment"># 批数量</span><br>accuracy_cnt = <span class="hljs-number">0</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(x), batch_size):<br>    x_batch = x[i:i+batch_size]<br>    y_batch = predict(network, x_batch)<br>    p = np.argmax(y_batch, axis=<span class="hljs-number">1</span>)<br>    accuracy_cnt += np.<span class="hljs-built_in">sum</span>(p == t[i:i+batch_size])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy:&quot;</span> + <span class="hljs-built_in">str</span>(<span class="hljs-built_in">float</span>(accuracy_cnt) / <span class="hljs-built_in">len</span>(x)))<br><br></code></pre></td></tr></table></figure><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>​神经网络和感知机在信号的按层传递这一点上是相同的，但是，向下一个神经元发送信号时，改变信号的激活函数有很大差异。神经网络中使用的是平滑变化的sigmoid函数，而感知机中使用的是信号急剧变化的阶跃函数。这个差异对于神经网络的学习非常重要。</p><ul><li>神经网络中的激活函数使用平滑变化的sigmoid函数或ReLU函数。</li><li>通过巧妙地使用NumPy多维数组，可以高效地实现神经网络。</li><li>机器学习的问题大体上可以分为回归问题和分类问题。</li><li>关于输出层的激活函数，回归问题中一般用恒等函数，分类问题中一般用softmax函数。</li><li>分类问题中，输出层的神经元的数量设置为要分类的类别数</li><li>·输入数据的集合称为批。通过以批为单位进行推理处理，能够实现高速的运算。</li></ul>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI-Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Win7环境下VMareTools的安装</title>
    <link href="/2025/03/04/win7-vmare-tools/"/>
    <url>/2025/03/04/win7-vmare-tools/</url>
    
    <content type="html"><![CDATA[<h1 id="安装VMare-Tools"><a href="#安装VMare-Tools" class="headerlink" title="安装VMare Tools"></a>安装VMare Tools</h1><h2 id="安装流程"><a href="#安装流程" class="headerlink" title="安装流程"></a>安装流程</h2><p>1、在VMare中启动win7后，点击VMare主界面的**虚拟机(M)**选项，启动VMare Tools的安装;</p><p><img src="/2025/03/04/win7-vmare-tools/vmare_tools_dow_1.png"></p><p><img src="/2025/03/04/win7-vmare-tools/vmare_tools_dow_2.png"></p><p><img src="/2025/03/04/win7-vmare-tools/vmare_tools_dow_3.png"></p><p>2、点击允许 setup64.exe，一路下一步直到完成安装；</p><p><img src="/2025/03/04/win7-vmare-tools/vmare_tools_dow_4.png"></p><h2 id="安装失败的问题"><a href="#安装失败的问题" class="headerlink" title="安装失败的问题"></a>安装失败的问题</h2><p>​安装程序中途报错，显示无法自动安装 Virtual Machine Communication Interface Sockets (VSock) 驱动程序，必须手动安装此驱动程序。</p><p><img src="/2025/03/04/win7-vmare-tools/vmare_tools_dow_5.png"></p><p>​原因是微软更新了驱动程序的签名算法，从 2019 年初开始，逐步弃用SHA-1，改为SHA-2。可以通过安装补丁来解决这个问题。</p><p>​可以从 Microsoft Update Catalog 下载 KB4474419 和 KB4490628 这两个补丁，然后安装到 Win7 虚拟机中。在没有成功安装 VMware Tools的情况下，传文件不太方便，可以用虚拟机里浏览器访问下载页面，然后直接在虚拟机里下载安装。下载地址：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim">https://www.catalog.<span class="hljs-keyword">update</span>.microsoft.<span class="hljs-keyword">com</span>/<span class="hljs-built_in">search</span>.aspx?q=kb4474419<br></code></pre></td></tr></table></figure><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim">https://www.catalog.<span class="hljs-keyword">update</span>.microsoft.<span class="hljs-keyword">com</span>/<span class="hljs-built_in">search</span>.aspx?q=<span class="hljs-number">4490628</span><br></code></pre></td></tr></table></figure><p>​安装补丁后，重启虚拟机，然后重新 VMware Tools 即可。</p><p>​</p><p>​或者使用附录的ISO文件进行补丁安装。</p><h2 id="附录文件"><a href="#附录文件" class="headerlink" title="附录文件"></a>附录文件</h2><p>1、<a href="https://dl.google.com/release2/chrome/acihtkcueyye3ymoj2afvv7ulzxa_109.0.5414.120/109.0.5414.120_chrome_installer.exe">适合win7的32位Google </a></p><p>2、<a href="ISO%E8%A1%A5%E4%B8%81.zip">下载 ISO </a></p><p>3、<a href="ISO%E8%A1%A5%E4%B8%81.z01">下载 ISO 1</a></p><p>4、<a href="ISO%E8%A1%A5%E4%B8%81.z02">下载 ISO 2</a></p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p>1、<a href="https://ihacksoft.com/archive/1397.html">完美解决：安装程序无法自动安装 Virtual Machine Communication…… - 嗨软</a></p><p>2、<a href="https://www.catalog.update.microsoft.com/search.aspx?q=kb4474419">https://www.catalog.update.microsoft.com/search.aspx?q=kb4474419</a></p><p>3、<a href="https://blog.csdn.net/Drug_/article/details/141402451">Chrome 浏览器 Windows 7 最终版下载（官方原版）_chrome win7-CSDN博客</a></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>Env_Config</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VMaretools</tag>
      
      <tag>win7</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DS的基础使用</title>
    <link href="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/"/>
    <url>/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="AI原生应用市场"><a href="#AI原生应用市场" class="headerlink" title="AI原生应用市场"></a>AI原生应用市场</h2><ul><li>国产大模型迭代和应用加速（技术突破：强化学习和混合专家架构结合）；</li><li>端侧和边缘计算应用繁荣；</li><li>智能安全和安全智能。</li></ul><h2 id="中小型企业"><a href="#中小型企业" class="headerlink" title="中小型企业"></a>中小型企业</h2><ul><li>价格竞争；</li><li>生态选择；</li><li>场景应用。</li></ul><h2 id="政府及头部企业市场"><a href="#政府及头部企业市场" class="headerlink" title="政府及头部企业市场"></a>政府及头部企业市场</h2><ul><li>智算中心+专业智能体；</li><li>企业级安全与合规保障；</li><li>终端及边缘智能化。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>破除算力霸权；</li><li>商业模型颠覆（NaaS:模型即服务）；</li><li>技术平权。</li></ul><h1 id="基础介绍"><a href="#基础介绍" class="headerlink" title="基础介绍"></a>基础介绍</h1><h2 id="ChatGPT"><a href="#ChatGPT" class="headerlink" title="ChatGPT"></a>ChatGPT</h2><ul><li>“Chat”代表聊天；</li><li>“G”（Generative）代表生成式；</li><li>“P”（Pre-trained）代表预训练；</li><li>“T”<strong>（Transformer）代表Transformer架构</strong>。</li><li>在GPT (Generative Pre-trained Transformer)中，生成式 (generative)意味着这个模型能够生成新的文本序列。</li></ul><h2 id="影响模型的三要素"><a href="#影响模型的三要素" class="headerlink" title="影响模型的三要素"></a>影响模型的三要素</h2><ul><li>算力；</li><li>算法；</li><li>数据。</li></ul><h2 id="提示词框架：BRROKE（融合OKR）"><a href="#提示词框架：BRROKE（融合OKR）" class="headerlink" title="提示词框架：BRROKE（融合OKR）"></a>提示词框架：BRROKE（融合OKR）</h2><ol><li>背景：提供足够的背景信息，使得GPT能够理解问题的上下文；</li><li>角色：设定特定的角色，让GPT能根据该角色来生成响应；</li><li>目标：明确任务目标，让GPT清楚知道需要实现什么；</li><li>要点：定义关键的、可以衡量的结果，以便让GPT知道如何衡量目标的完成情况；</li><li>测试：1次不对，就鼓励AI分步骤，多试几次。通过调整来测试结果，并根据需要进行优化。</li></ol><h2 id="五个技巧"><a href="#五个技巧" class="headerlink" title="五个技巧"></a>五个技巧</h2><ul><li>定目标——抛弃套路，直抒胸臆，但是注意细节；<ul><li>通用公式：我要XX，要给XX用，希望达到XX效果，但是担心XX问题。</li></ul></li><li>说人话；</li><li>补数据——数据类型包括文案、示例、PDF、Excel等，用来给AI补充背景信息；</li><li>勤反问——发挥人的主动性；</li><li>善联动。</li></ul><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E5%AE%9A%E7%9B%AE%E6%A0%87%E7%9A%84%E7%BB%86%E8%8A%82.png"></p><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E8%AF%B4%E4%BA%BA%E8%AF%9D.png"></p><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E8%A1%A5%E6%95%B0%E6%8D%AE.png"></p><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E9%AB%98%E7%BA%A7%E6%90%9C%E7%B4%A2%E6%8A%80%E5%B7%A7.png"></p><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E5%8B%A4%E5%8F%8D%E9%97%AE.png"></p><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E5%AE%9E%E6%93%8D.png"></p><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E4%B8%8EAI%E6%B2%9F%E9%80%9A.png"></p><h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><ul><li>秘塔AI搜索；</li><li>纳米AI搜索；</li><li>DeepSeek +阿里通义 快速生成PPT模板；</li><li>DeepSeek +即梦AI 完成AI绘画；</li><li>AI精选工具库</li></ul><p><img src="/2025/02/21/DS%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/%E4%B8%8D%E5%90%8CAI%E7%9A%84%E4%BC%98%E5%8A%BF.png"></p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI-Use</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>个人博客搭建流程</title>
    <link href="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/"/>
    <url>/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="一、前期准备"><a href="#一、前期准备" class="headerlink" title="一、前期准备"></a>一、前期准备</h1><h2 id="1、github账号"><a href="#1、github账号" class="headerlink" title="1、github账号"></a>1、github账号</h2><h2 id="2、git安装"><a href="#2、git安装" class="headerlink" title="2、git安装"></a>2、git安装</h2><p>配置</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">git config <span class="hljs-attr">--global</span> user<span class="hljs-selector-class">.name</span> <span class="hljs-string">&quot;Your Name&quot;</span><br>git config <span class="hljs-attr">--global</span> user<span class="hljs-selector-class">.email</span> <span class="hljs-string">&quot;email@example.com&quot;</span><br></code></pre></td></tr></table></figure><h2 id="3、NodeJS安装和（可选）配置"><a href="#3、NodeJS安装和（可选）配置" class="headerlink" title="3、NodeJS安装和（可选）配置"></a>3、NodeJS安装和（可选）配置</h2><p>备注：以管理员身份打开cmd窗口</p><ol><li><p><a href="https://nodejs.org/zh-cn/download/">NodeJS官网下载</a>，安装路径可以选择自定义路径，如D:\myweb\NodeJs</p></li><li><p>验证NodeJS安装，即查看node和npm版本</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs crmsh"><span class="hljs-keyword">node</span> <span class="hljs-title">-v</span><br>npm -v<br></code></pre></td></tr></table></figure></li><li><p>在安装目录（D:\myweb\NodeJs）下创建两个文件夹，<code>node_global</code> 存放全局包，<code>node_cache</code> 存放node缓存和log记录</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus">npm root -<span class="hljs-selector-tag">g</span> <br><span class="hljs-comment">//查看当前路径</span><br></code></pre></td></tr></table></figure></li><li><p>在cmd命令行中执行如下两条命令</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs routeros">npm<span class="hljs-built_in"> config </span><span class="hljs-built_in">set</span><span class="hljs-built_in"> prefix </span><span class="hljs-string">&quot;D:\xgweb\NodeJs\node_global&quot;</span><br><br>npm<span class="hljs-built_in"> config </span><span class="hljs-built_in">set</span> cache <span class="hljs-string">&quot;D:\xgweb\NodeJs\node_cache&quot;</span><br><br></code></pre></td></tr></table></figure></li><li><p>设置电脑环境变量，右键 “我的电脑”&#x3D;》属性&#x3D;》高级系统设置&#x3D;》环境变量；进入环境变量对话框，在【系统变量】中新建环境变量 <code>NODE_PATH</code>，值为 <code>D:\xgweb\NodeJs\node_global\node_modules</code>，其中 <code>D:\xgweb\NodeJs\node_global</code> 是新创建的全局模块安装路径；修改【用户变量】中的 <code>path</code> 变量，将 <code>C:\Users\..\AppData\Roaming\npm</code> 修改为<code>D:\xgweb\NodeJs\node_global</code></p></li><li><p>重启电脑，使配置生效</p></li></ol><h1 id="二、创建仓库"><a href="#二、创建仓库" class="headerlink" title="二、创建仓库"></a>二、创建仓库</h1><ol><li><p>在<code>GitHub</code>上创建一个新的代码仓库用于保存我们的网页。</p></li><li><p>点击<code>Your repositories</code>，进入仓库页面。点击<code>New</code>按钮，进入仓库创建页面，填写仓库名，格式必须为<code>&lt;用户名&gt;.github.io</code>，然后点击<code>Create repository</code>。</p></li><li><p>点击<code>creating a new file</code>创建一个新文件，作为我们网站的主页。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-meta">&lt;!DOCTYPE <span class="hljs-keyword">html</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">html</span> <span class="hljs-attr">lang</span>=<span class="hljs-string">&quot;en&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">head</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">charset</span>=<span class="hljs-string">&quot;UTF-8&quot;</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">title</span>&gt;</span>111<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">head</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">body</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">h1</span>&gt;</span>111的个人主页<span class="hljs-tag">&lt;/<span class="hljs-name">h1</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">h1</span>&gt;</span>Hello ~<span class="hljs-tag">&lt;/<span class="hljs-name">h1</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">body</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">html</span>&gt;</span><br><br></code></pre></td></tr></table></figure></li><li><p>在浏览器中访问成功</p></li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/page1.png"></p><h1 id="三、安装Hexo"><a href="#三、安装Hexo" class="headerlink" title="三、安装Hexo"></a>三、安装Hexo</h1><ol><li><p>安装Hexo(cmd管理员模式下)和查看版本</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs avrasm">npm install -g hexo-<span class="hljs-keyword">cli</span><br>hexo -v<br></code></pre></td></tr></table></figure><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo_download.png"></p></li><li><p>创建一个项目并初始化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo init hexo-blog<br><span class="hljs-built_in">cd</span> hexo-blog<br>npm install<br>//cmd管理员模式<br></code></pre></td></tr></table></figure><p>as</p></li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo_init_1.png"></p><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/npm_install.png"></p><p>其内部文件结构如下（红框内为npm install 安装的）：</p><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo-doc.png"></p><ol start="3"><li>本地启动<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs css">hexo <span class="hljs-selector-tag">g</span><br>hexo s<br></code></pre></td></tr></table></figure><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo_start.png"></li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo-doc-1.png"></p><p>如上所示，public文件夹内为新生成的web文件，共计11项。</p><ol start="4"><li>浏览器访问 <a href="http://localhost:4000/">http://localhost:4000</a></li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-1.png"></p><h1 id="四、更换主题"><a href="#四、更换主题" class="headerlink" title="四、更换主题"></a>四、更换主题</h1><h2 id="1、NexT主题"><a href="#1、NexT主题" class="headerlink" title="1、NexT主题"></a>1、NexT主题</h2><ol><li>下载NexT主题</li></ol><figure class="highlight autoit"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs autoit">git clone git<span class="hljs-symbol">@github</span>.com:iissnan/hexo-theme-<span class="hljs-keyword">next</span>.git themes/<span class="hljs-keyword">next</span><br></code></pre></td></tr></table></figure><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/clone_next.png"></p><ol start="2"><li>打开 _config.yml 文件，该文件为站点配置文件</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/theme_set_next.png"></p><ol start="3"><li>本地启动，出现以下显示问题</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/next_error.png"></p><ol start="4"><li><p>原因是hexo在5.0之后把swig给删除了需要自己手动安装</p><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs coffeescript"><span class="hljs-built_in">npm</span> i hexo-renderer-swig<br></code></pre></td></tr></table></figure><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/npm_swig.png"></p></li><li><p>重新启动</p></li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-next.png"></p><p>参考：<a href="https://github.com/iissnan/hexo-theme-next/issues/2253">https://github.com/iissnan/hexo-theme-next/issues/2253</a></p><h2 id="2、Fluid主题"><a href="#2、Fluid主题" class="headerlink" title="2、Fluid主题"></a>2、Fluid主题</h2><ol><li>安装主题</li></ol><p>下载 <a href="https://github.com/fluid-dev/hexo-theme-fluid/releases">最新 release 版本</a> 解压到 <code>themes</code> 目录，并将解压出的文件夹重命名为 <code>fluid</code></p><ol start="2"><li>指定主题</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/theme_set_fluid.png"></p><ol start="3"><li><p>创建[关于页]</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs actionscript">hexo <span class="hljs-keyword">new</span> page about<br></code></pre></td></tr></table></figure><p>编辑博客目录下 <code>/source/about/index.md</code>，添加 <code>layout</code> 属性。</p></li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo_new_page_about.png"></p><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/page_about.png"></p><ol start="4"><li>重新启动</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-fluid.png"></p><h1 id="五、创建文章"><a href="#五、创建文章" class="headerlink" title="五、创建文章"></a>五、创建文章</h1><h2 id="1、文章创建和图片插入"><a href="#1、文章创建和图片插入" class="headerlink" title="1、文章创建和图片插入"></a>1、文章创建和图片插入</h2><ol><li>修改 Hexo 博客目录中的 <code>_config.yml</code>，打开这个配置是为了在生成文章的时候生成一个同名的资源目录用于存放图片文件。</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/config_set.png"></p><ol start="2"><li>创建一篇新文章，名为《测试文章》</li></ol><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">hexo <span class="hljs-built_in">new</span> <span class="hljs-built_in">post</span> 测试文章<br></code></pre></td></tr></table></figure><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/config_set_display.png"></p><ol start="3"><li>在md文件中插入图片的常用三种方式</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><br>&#123;<span class="hljs-string">%</span> <span class="hljs-string">asset_img</span> <span class="hljs-string">test.png</span> <span class="hljs-string">图片引用方法一</span> <span class="hljs-string">%</span>&#125;<br><br><br><span class="hljs-type">![</span><span class="hljs-string">图片引用方法二](test.png)</span><br><br><span class="hljs-string">需要在配置文件添加：</span><br><span class="hljs-attr">marked:</span><br>  <span class="hljs-attr">prependRoot:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">postAsset:</span> <span class="hljs-literal">true</span><br><br><br><br><span class="hljs-type">![</span><span class="hljs-string">图片引用方法三](/images/test.png)</span><br><span class="hljs-string">//images文件在在\source\images目录下</span><br><br></code></pre></td></tr></table></figure><p>如下分别是：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs maxima">![](/测试文章/BA.jpg)<br>![](BA.jpg)<br>&#123;<span class="hljs-symbol">%</span> asset_img BA.jpg This <span class="hljs-built_in">is</span> an <span class="hljs-built_in">example</span> <span class="hljs-built_in">image</span> <span class="hljs-symbol">%</span>&#125;<br>&#123;<span class="hljs-symbol">%</span> asset_img <span class="hljs-string">&quot;BA.jpg&quot;</span> <span class="hljs-string">&quot;spaced title&quot;</span> <span class="hljs-symbol">%</span>&#125;<br></code></pre></td></tr></table></figure><p>对于如上四种中的第二种图片引用方法，需要对配置文件进行以下修改：</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-params">post_asset_folder:</span> <span class="hljs-literal">true</span><br><span class="hljs-params">marked:</span><br>  <span class="hljs-params">prependRoot:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-params">postAsset:</span> <span class="hljs-literal">true</span><br><br></code></pre></td></tr></table></figure><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-2-1.png"></p><h2 id="2、md文件图片同步显示"><a href="#2、md文件图片同步显示" class="headerlink" title="2、md文件图片同步显示"></a>2、md文件图片同步显示</h2><p>如果需要同时在md文件显示图片，需要安装<strong>hexo-asset-image</strong> 插件</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">npm install hexo-asset-<span class="hljs-selector-tag">image</span> <span class="hljs-attr">--save</span><br></code></pre></td></tr></table></figure><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo_asset_image_dow.png"></p><p>重新启动</p><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-2-2.png"></p><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-2.png"></p><p>为什么会出现以下 &#x2F;.com&#x2F;&#x2F; 情况，其实是 <strong>hexo-asset-image</strong> 插件的bug</p><h3 id="方案一"><a href="#方案一" class="headerlink" title="方案一"></a>方案一</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs verilog">npm uninstall hexo-asset-image<br>npm install https:<span class="hljs-comment">//github.com/CodeFalling/hexo-asset-image</span><br>hexo clean<br>hexo <span class="hljs-keyword">generate</span><br>hexo server <br></code></pre></td></tr></table></figure><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/hexo_asset_image_dow_1.png"></p><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-2-3.png"></p><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-2-3-1.png"></p><p>此时可以看出其他三种方法的解析路径出现多余的&#x2F;02&#x2F;10</p><h3 id="方案二"><a href="#方案二" class="headerlink" title="方案二"></a>方案二</h3><p>对hexo-asset-image（第一版）文件夹中的index.js中的文件进行如下修改：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-comment">//$(this).attr(&#x27;src&#x27;, config.root + link + src); </span><br>$(this)<span class="hljs-selector-class">.attr</span>(<span class="hljs-string">&#x27;src&#x27;</span>, data<span class="hljs-selector-class">.permalink</span><span class="hljs-selector-class">.split</span>(<span class="hljs-string">&#x27;example.com&#x27;</span>)<span class="hljs-selector-attr">[1]</span>+ <span class="hljs-attribute">src</span>);<br></code></pre></td></tr></table></figure><p>其问题如上图所示</p><p>参考：<a href="https://blog.csdn.net/sluck_0430/article/details/136431303">hexo图片显示不出且图片路径错误&#x2F;.com&#x2F;&#x2F;_hexo显示不了图片-CSDN博客</a></p><h3 id="方案三"><a href="#方案三" class="headerlink" title="方案三"></a>方案三</h3><p>对hexo-asset-image（第一版）文件夹中的index.js中的文件进行如下修改（第23行的else分支）：</p><figure class="highlight cal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cal">//<span class="hljs-keyword">var</span> endPos = link.lastIndexOf(<span class="hljs-string">&#x27;.&#x27;</span>);<br><span class="hljs-keyword">var</span> endPos = link.lastIndexOf(<span class="hljs-string">&#x27;/&#x27;</span>);<br></code></pre></td></tr></table></figure><p>其问题依然如上图所示</p><p>参考：<a href="https://blog.asroads.com/post/95d84581.html">Hexo生成博文插入图片 | Asroads’Blog</a></p><p>最终决定卸载hexo-asset-image插件</p><h1 id="六、个性化页面展示"><a href="#六、个性化页面展示" class="headerlink" title="六、个性化页面展示"></a>六、个性化页面展示</h1><h2 id="博客标题"><a href="#博客标题" class="headerlink" title="博客标题"></a>博客标题</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-params">navbar:</span><br>  <span class="hljs-comment"># 导航栏左侧的标题，为空则按 hexo config 中 `title` 显示</span><br>  <span class="hljs-comment"># The title on the left side of the navigation bar. If empty, it is based on `title` in hexo config</span><br>  <span class="hljs-comment">#blog_title: &quot;Fluid&quot;</span><br>  <span class="hljs-params">blog_title:</span> <span class="hljs-string">&quot;blackstarry6&quot;</span><br></code></pre></td></tr></table></figure><h2 id="主页正中间的文字"><a href="#主页正中间的文字" class="headerlink" title="主页正中间的文字"></a>主页正中间的文字</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs nix"><span class="hljs-params">slogan:</span><br>  <span class="hljs-params">enable:</span> <span class="hljs-literal">true</span><br><br>  <span class="hljs-comment"># 为空则按 hexo config.subtitle 显示</span><br>  <span class="hljs-comment"># If empty, text based on `subtitle` in hexo config</span><br>  <span class="hljs-comment">#text: &quot;An elegant Material-Design theme for Hexo&quot;</span><br>  <span class="hljs-params">text:</span> <span class="hljs-string">&quot;XXX 个人博客&quot;</span><br></code></pre></td></tr></table></figure><h2 id="主页展示"><a href="#主页展示" class="headerlink" title="主页展示"></a>主页展示</h2><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/web-fluid-1.png"></p><h1 id="七、发布到Github-Pages"><a href="#七、发布到Github-Pages" class="headerlink" title="七、发布到Github Pages"></a>七、发布到Github Pages</h1><p>安装hexo-deployer-git</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ada">npm install hexo-deployer-git <span class="hljs-comment">--save</span><br><br></code></pre></td></tr></table></figure><p>修改配置文件，其中 <code>token</code> 为 <code>GitHub</code> 的 <code>Personal access tokens</code></p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs dts"><span class="hljs-symbol">deploy:</span><br><span class="hljs-symbol">  type:</span> git<br><span class="hljs-symbol">  repo:</span> XXX<br><span class="hljs-symbol">  branch:</span> main<br><span class="hljs-symbol">  token:</span> XXX<br><br></code></pre></td></tr></table></figure><p>部署到github</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">hexo <span class="hljs-selector-tag">g</span> -d<br></code></pre></td></tr></table></figure><p>参考：</p><p><a href="https://blog.csdn.net/yaorongke/article/details/119089190">GitHub Pages + Hexo搭建个人博客网站，史上最全教程_hexo博客-CSDN博客</a></p><p><a href="https://blog.csdn.net/yaorongke/article/details/119085413">Git安装(Windows)_windows git安装包-CSDN博客</a></p><p><a href="https://blog.csdn.net/yaorongke/article/details/119084295">NodeJS安装及配置(Windows)_welcome to node.js v14.21.3. type “.help” for more-CSDN博客</a></p><p><a href="https://blog.csdn.net/zimeng303/article/details/112167688">NodeJs 的安装及配置环境变量_nodejs配置环境变量-CSDN博客</a></p><p><a href="https://blog.csdn.net/m0_43401436/article/details/107191688">hexo博客中插入图片失败——解决思路及个人最终解决办法_hexo 文章插入图片失败-CSDN博客</a></p><h1 id="附录：SSH配置"><a href="#附录：SSH配置" class="headerlink" title="附录：SSH配置"></a>附录：SSH配置</h1><ol><li>生成密钥对</li></ol><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs armasm"><span class="hljs-symbol">ssh</span>-keygen -t ed25519 -C <span class="hljs-string">&quot;your_email@example.com&quot;</span><br><br><span class="hljs-comment">//如果您使用的是不支持 Ed25519 算法的旧版系统，请使用</span><br><span class="hljs-symbol">ssh</span>-keygen -t rsa -<span class="hljs-keyword">b</span> <span class="hljs-number">4096</span> -C <span class="hljs-string">&quot;your_email@example.com&quot;</span><br></code></pre></td></tr></table></figure><ol start="2"><li><p>在.&#x2F;ssh文件下查看生成的密钥对，复制.pub内的公钥内容</p></li><li><p>Github账号上添加公钥</p></li></ol><p><code>Settings</code>&#x3D;&gt;<code>SSH and GPG keys</code>&#x3D;&gt;<code>New SSH key</code>&#x3D;&gt;<code>Key</code>，在title中简单描述一下</p><ol start="4"><li>验证是否成功，即连接github服务器，此时出现如下情况：</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/ssh-1.png"></p><ol start="5"><li>换成443端口：</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/ssh-2.png"></p><ol start="6"><li>对.&#x2F;ssh文件中的config文件（没有的话自己建立）进行如下修改：</li></ol><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># Add section below to it</span><br><span class="hljs-attribute">Host</span> github.com<br>  <span class="hljs-attribute">Hostname</span> ssh.github.com<br>  <span class="hljs-attribute">Port</span> <span class="hljs-number">443</span><br><br></code></pre></td></tr></table></figure><ol start="7"><li>重新连接github服务器：</li></ol><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/ssh-3.png"></p><p>参考链接：</p><p><a href="https://blog.csdn.net/weixin_42310154/article/details/118340458">Github配置ssh key的步骤（大白话+包含原理解释）_github生成ssh key-CSDN博客</a></p><p><a href="https://githubdocs.cn/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent?platform=windows">在 SSH 代理中生成新的 SSH 密钥并添加它 - GitHub 文档 - GitHub 文档</a></p><p><a href="https://blog.csdn.net/misakivv/article/details/144929627">解决 ssh connect to host github.com port 22 Connection timed out_ssh: connect to host gitlab.sdlg.cn port 30022: co-CSDN博客</a></p><h2 id="可能出现的问题和解决方案"><a href="#可能出现的问题和解决方案" class="headerlink" title="可能出现的问题和解决方案"></a>可能出现的问题和解决方案</h2><p><img src="/2025/02/19/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%B5%81%E7%A8%8B/git_solve_1.png"></p><p>来源：<a href="https://blog.csdn.net/fearlessxmm/article/details/90401690">https://blog.csdn.net/fearlessxmm/article/details/90401690</a>  </p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>Web</category>
      
    </categories>
    
    
    <tags>
      
      <tag>web</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试文章</title>
    <link href="/2025/02/10/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <url>/2025/02/10/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<p>1</p><p><img src="/2025/02/10/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/BA.jpg"></p><p>1</p><p>banner_img:&#x2F;测试文章&#x2F;BA.jpg</p><p>官方引用</p><img src="/2025/02/10/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/BA.jpg" class="" title="This is an example image"><img src="/2025/02/10/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/BA.jpg" class="" title="spaced title">]]></content>
    
    
    
    <tags>
      
      <tag>exam</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/02/10/hello-world/"/>
    <url>/2025/02/10/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>exam</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于《白话机器学习的数学》的知识整理（4）</title>
    <link href="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/"/>
    <url>/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>​ 在进行回归和分类时，为了进行预测，我们定义了函数fθ(x)，然后根据训练数据求出了函数的参数θ。</p><p>​但是，其实我们真正想要的是通过预测函数得到预测值。以回归的那个例子来说，就是关于投入的广告费能带来多少点击量的预测值。所以我们希望fθ(x)对未知数据x输出的预测值尽可能正确。所以我们需要能够<strong>定量地表示机器学习模型的精度</strong>。</p><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><h4 id="回归问题的验证"><a href="#回归问题的验证" class="headerlink" title="回归问题的验证"></a>回归问题的验证</h4><p>​把获取的全部训练数据分成两份：一份用于测试，一份用于训练。然后用前者来评估模型。</p><p>​对于回归的情况，只要在训练好的模型上计算测试数据的误差的平方，再取其平均值就可以了。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-1.png"></p><p>​这个值被称为均方误差或者MSE，全称Mean Square Error。这个误差越小，精度就越高，模型也就越好。</p><p>​另外，在分类问题中也会出现模型只能拟合训练数据的问题哦。</p><h4 id="分类问题的验证"><a href="#分类问题的验证" class="headerlink" title="分类问题的验证"></a>分类问题的验证</h4><p>​“数据的分配方法不要太极端其实会更好”这一点与回归的时候也是一样的。</p><p>​由于回归是连续值，所以可以从误差入手，但是在分类中我们必须要考虑分类的类别是否正确。</p><p>​在回归中要考虑的是答案不完全一致时的误差，而分类中要考虑的是答案是否正确。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-2.png"></p><p>​我们可以使用表里的4个记号来计算分类的精度。精度的英文是Accuracy，它的计算表达式是这样的：</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-3.png"></p><p>​</p><h4 id="精确率和召回率"><a href="#精确率和召回率" class="headerlink" title="精确率和召回率"></a>精确率和召回率</h4><p>​第一个指标——精确率。它的英文是Precision。根据表达式来看，它的含义是在被分类为Positive的数据中，实际就是Positive的数据所占的比例。 这个值越高，说明分类错误越少。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-4.png"></p><p>​第二个指标是召回率，英文是Recall。根据表达式来看，它的含义是在Positive数据中，实际被分类为Positive的数据所占的比例。 这个值越高，说明被正确分类的数据越多。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-5.png"></p><p>​精确率和召回率都很高的模型就是一个好模型了，不过一般来说，精确率和召回率会一个高一个低，需要我们取舍。</p><h4 id="F值"><a href="#F值" class="headerlink" title="F值"></a>F值</h4><p>​评定综合性能的指标F值，即Fmeasure：</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-6.png"></p><p>​有时称F值为F1值会更准确，这一点需要注意。除F1值之外，还有一个带权重的F值指标。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-7.png"></p><p>​F1值在数学上是精确率和召回率的调和平均值。</p><p>​之前介绍的精确率和召回率都是以TP为主进行计算的，而以TN为主来计算精确率和召回率的表达式是这样的:</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-8.png"></p><p>​当数据不平衡时，使用数量少的那个会更好。</p><p>​把全部训练数据分为测试数据和训练数据的做法称为交叉验证。这是非常重要的方法。</p><p>​交叉验证的方法中，尤为有名的是K折交叉验证：</p><p>● 把全部训练数据分为K份</p><p>● 将K-1份数据用作训练数据，剩下的1份用作测试数据</p><p>● 每次更换训练数据和测试数据，重复进行K次交叉验证</p><p>● 最后计算K个精度的平均值，把它作为最终的精度</p><p>​不切实际地增加K值会非常耗费时间，所以我们必须要确定一个合适的K值。</p><p>​</p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><h4 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h4><p>​之前我们提到过的模型只能拟合训练数据的状态被称为过拟合，英文是overfitting。</p><p>​过拟合不止在回归时出现，在分类时也经常发生</p><p>​有几种方法可以避免过拟合：</p><p>● 增加全部训练数据的数量</p><p>● 使用简单的模型</p><p>● 正则化</p><p>​</p><h4 id="正则化的方法"><a href="#正则化的方法" class="headerlink" title="正则化的方法"></a>正则化的方法</h4><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-9.png"></p><p>​我们要对这个新的目标函数进行最小化，这种方法就称为正则化。</p><p>​m是参数的个数,不过一般来说不对θ0应用正则化。所以仔细看会发现j的取值是从1开始的。</p><p>​假如预测函数的表达式为fθ(x)&#x3D;θ0+θ1x+θ2x2，那么m&#x3D;2就意味着正则化的对象参数为θ1和θ2。</p><p>​θ0这种只有参数的项称为偏置项，一般不对它进行正则化。</p><p>​λ是决定正则化项影响程度的正的常数。这个值需要我们自己来定。</p><h4 id="正则化的效果"><a href="#正则化的效果" class="headerlink" title="正则化的效果"></a>正则化的效果</h4><p>​把目标函数分成两个部分。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-10.png"></p><p>​C(θ)是本来就有的目标函数项，R(θ)是正则化项。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-11.png"></p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-12.png"></p><p>​与加正则化项之前相比，θ1更接近0了。</p><p>​这就是正则化的效果。它可以防止参数变得过大，有助于参数接近较小的值。虽然我们只考虑了θ1，但其他θj参数的情况也是类似的。</p><p>​因为参数的值变小，意味着该参数的影响也会相应地变小。</p><p>​通过减小不需要的参数的影响，将复杂模型替换为简单模型来防止过拟合的方式。</p><h4 id="分类的正则化"><a href="#分类的正则化" class="headerlink" title="分类的正则化"></a>分类的正则化</h4><p>​逻辑回归的目标函数是对数似然函数，分类也是在这个目标函数中增加正则化项就行了，道理是相同的。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-13.png"></p><p>​对数似然函数本来以最大化为目标。但是，这次我想让它变成和回归的目标函数一样的最小化问题，所以加了负号。这样就可以像处理回归一样处理它，所以只要加上正则化项就可以了。</p><p>​反转符号是为了将最大化问题替换为最小化问题。反转了符号之后，在更新参数时就要像回归一样，与微分的函数的符号反方向移动才行。</p><p>​其微分结果如下：</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-13-1.png"></p><p>​R(θ)对θ0微分的结果为0，所以j&#x3D;0时表达式4.3.14中的λθj就消失了。因此实际上我们需要像这样区分两种情况：</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-13-2.png"></p><p>​刚才我介绍的方法其实叫L2正则化。除L2正则化方法之外，还有L1正则化方法。它的正则化项R是这样的:</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-14.png"></p><p>​L1正则化的特征是被判定为不需要的参数会变为0，从而减少变量个数。而L2正则化不会把参数变为0。</p><p>​ L2正则化会抑制参数，使变量的影响不会过大，而L1会直接去除不要的变量。</p><h3 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h3><h4 id="欠拟合"><a href="#欠拟合" class="headerlink" title="欠拟合"></a>欠拟合</h4><p>​反过来又有一种叫作欠拟合的状态，用英文说是underfitting。在这种情况下模型的性能也会变差。</p><p>​比如用直线对图中这种拥有复杂边界线的数据进行分类的情况，无论怎样做都不能很好地分类，最终的精度会很差。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-15.png"></p><p>​</p><h4 id="区分过拟合与欠拟合"><a href="#区分过拟合与欠拟合" class="headerlink" title="区分过拟合与欠拟合"></a>区分过拟合与欠拟合</h4><p>​训练数据较少时训练好的模型难以预测未知的数据，所以精度很低；反过来说，训练数据变多时，预测精度就会一点点地变高。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-16-1.png"></p><p>​将两份数据的精度用图来展示后，如果是这种形状，就说明出现了欠拟合的状态。也有一种说法叫作高偏差。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-16-2.png"></p><p>​而在过拟合的情况下，图是下面这样的。这也叫作高方差。</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-16-3.png"></p><p>​只对训练数据拟合得较好，这就是过拟合的特征。下图中需要注意的点在这里：</p><p><img src="/2024/08/01/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%884%EF%BC%89/4-16-4.png"></p><p>​像这样展示了数据数量和精度的图称为<strong>学习曲线</strong>。</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI-Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于《白话机器学习的数学》的知识整理（3）</title>
    <link href="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/"/>
    <url>/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="设置问题"><a href="#设置问题" class="headerlink" title="设置问题"></a>设置问题</h3><p>​不去考虑图像本身的内容，只根据尺寸把它分类为纵向图像和横向图像，二分类问题。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3.png"></p><p>​只要找到这条线，就可以根据点在线的哪一边来判断图像是横向还是纵向的了。</p><h3 id="内积"><a href="#内积" class="headerlink" title="内积"></a>内积</h3><p>​分类用图形来解释更容易理解，所以把它想象为有大小和方向的、带箭头的向量比较好。</p><p>​那条线，是使权重向量成为法线向量的直线。</p><p>​设权重向量为w，那么那条直线的表达式就是w*x&#x3D;0。</p><p>​w是权重一词的英文——weight的首字母。上次学习回归时，我们为了求未知参数θ做了很多事情，而w和θ是一样的。</p><p>​实向量空间的内积是各相应元素乘积的和，所以刚才的表达式也可以写成这样：</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-1.png"></p><p>​现在要考虑的是有宽和高的二维情况，所以n&#x3D;2就可以了。</p><p>​法线是与某条直线相垂直的向量。</p><p>​设权重向量为w&#x3D;(1,1)，w*x&#x3D;x1+x2，如下图：</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-2.png"></p><p>​这就是“使权重向量成为法线向量的直线”在图形上的解释。</p><ul><li>一开始并不存在你画的那种直线，而是要通过训练找到权重向量，然后才能得到与这个向量垂直的直线，最后根据这条直线就可以对数据进行分类了。</li></ul><h3 id="感知机（perceptron）"><a href="#感知机（perceptron）" class="headerlink" title="感知机（perceptron）"></a>感知机（perceptron）</h3><p>​感知机是接受多个输入后将每个值与各自的权重相乘，最后输出总和的模型。</p><p>![](C:\Users\10764\Desktop\Record\Reading Impression\白话机器学习的数学\image\3-3.png)</p><p>​</p><h4 id="训练前的准备"><a href="#训练前的准备" class="headerlink" title="训练前的准备"></a>训练前的准备</h4><p>​首先是训练数据。设表示宽的轴为x1、表示高的轴为x2，用y来表示图像是横向还是纵向的，横向的值为1、纵向的值为-1。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-4.png"></p><p>​接下来，根据参数向量x来判断图像是横向还是纵向的函数，即返回1或者-1的函数的定义如下。这个函数被称为判别函数。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-5.png"></p><p>​决定内积符号的是cos θ。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-6.png"></p><p>​内积是衡量向量之间相似程度的指标。结果为正，说明二者相似；为0则二者垂直；为负则说明二者不相似。</p><h4 id="权重向量的更新表达式"><a href="#权重向量的更新表达式" class="headerlink" title="权重向量的更新表达式"></a>权重向量的更新表达式</h4><p>​在这个基础上，我们可以这样定义权重向量的更新表达式。</p><p>![](C:\Users\10764\Desktop\Record\Reading Impression\白话机器学习的数学\image\3-7.png)</p><p>​i指的是训练数据的索引，而不是i次方的意思，这一点一定要注意。用这个表达式重复处理所有训练数据，更新权重向量。</p><ul><li>在图上随意画一个权重向量和直线。</li></ul><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-7-1.png"></p><ul><li>假设第一个训练数据是x(1)&#x3D;(125,30)，首先我们就用它来更新参数吧。</li></ul><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-7-2.png"></p><p>​现在权重向量w和训练数据的向量x(1)二者的方向几乎相反，w和x(1)之间的夹角θ的范围是90◦＜θ＜270◦，内积为负。也就是说，判别函数fw(x(1))的分类结果为-1。</p><p>​现在y(1)&#x3D;1，所以更新表达式是这样的，其实就是向量的加法——w+x(1)。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-7-3.png"></p><p>​这个w+x(1)就是下一个新的w，画一条与新的权重向量垂直的直线，相当于把原来的线旋转了一下。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-7-4.png"></p><p>​刚才x(1)与权重向量分居直线两侧，现在它们在同一侧了</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-7-5.png"></p><p>​这次θ＜90◦，所以内积为正，判别函数fw(x)的分类结果为1。而且x(1)的标签也为1，说明分类成功了。</p><ul><li>刚才处理的是标签值y&#x3D;1的情况，而对于y&#x3D;-1的情况，只是更新表达式的向量加法变成了减法而已，做的事情是一样的。</li><li>也就是说，虽然有加法和减法的区别，但它们的做法都是在分类失败时更新权重向量，使得直线旋转相应的角度。</li><li>像这样重复更新所有的参数，就是<strong>感知机的学习方法</strong>。</li></ul><h3 id="线性可分"><a href="#线性可分" class="headerlink" title="线性可分"></a>线性可分</h3><p>​如果要对时装的照片进行分类的话用感知机是做不到的，因为感知机只是简单的模型。</p><ul><li>感知机的最大的缺点就是它只能解决线性可分的问题。</li></ul><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-8.png"></p><p>​假设有上面这张图里的数据，其中圆点为1，叉号为-1，如果只用一条直线对这些数据进行分类，应该画一条什么样的线呢？答案是不可能的。</p><p>​线性可分指的就是能够使用直线分类的情况，像这样不能用直线分类的就不是线性可分。</p><p>​像照片这类的图像分类就不是线性可分了。这类图像数据的维度一般会很高，所以无法可视化。</p><ul><li>之前提到的感知机也被称为简单感知机或单层感知机，真的是很弱的模型。不过，既然有单层感知机，那么就会有多层感知机。实际上多层感知机就是神经网络了，它是表现力非常高的模型。</li></ul><p>​有一个不同于感知机的算法能够很好地应用于线性不可分问题。这个算法更实用。</p><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p>​接下来要讲的算法与感知机的不同之处在于，它是把分类作为概率来考虑的。另外，这里设横向的值为1、纵向的值为0。</p><p>​只要是两个不同的值，用什么都可以。在学习感知机时之所以设置值为1和-1，是因为这样会使参数更新表达式看起来更简洁，而现在则是设置为1和0会更简洁。</p><p>​</p><h4 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h4><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-9.png"></p><p>​exp的全称是exponential，即指数函数。exp(x)与ex含义相同，只是写法不同。它的图形如下：</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-9-1.png"></p><ul><li><p>θTx&#x3D;0时fθ(x)&#x3D;0.5，以及0＜fθ(x)＜1是sigmoid函数的两个特征。</p><p>因为sigmoid函数的取值范围是0＜fθ(x)＜1，所以它可以作为概率来使用。</p></li></ul><h4 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h4><p>​把未知数据x是横向图像的概率作为fθ(x)。其表达式是这样的：</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-10.png"></p><p>​这是在给出x数据时y&#x3D;1，即图像为横向的概率。</p><p>​结合sigmoid函数，有：</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-10-1.png"></p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-10-2.png"></p><p>​为了求得正确的参数θ而定义目标函数，进行微分，然后求参数的更新表达式。这种算法就称为<strong>逻辑回归</strong>。</p><h3 id="似然函数"><a href="#似然函数" class="headerlink" title="似然函数"></a>似然函数</h3><p>​逻辑回归的目标函数与之前的不一样哦。</p><p>● y&#x3D;1的时候，我们希望概率P(y&#x3D;1|x)是最大的；</p><p>● y&#x3D;0的时候，我们希望概率P(y&#x3D;0|x)是最大的。</p><p>​联合概率的表达式一般化的写法如下：</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-11.png"></p><ul><li><p>利用了任何数字的0次方都是1的特性；</p></li><li><p>回归的时候处理的是误差，所以要最小化，而现在考虑的是联合概率，我们希望概率尽可能大，所以要最大化。</p></li><li><p>这里的目标函数L(θ)也被称为似然，函数的名字L取自似然的英文单词Likelihood的首字母。</p></li><li><p>我们可以认为似然函数L(θ)中，使其值最大的参数θ能够最近似地说明训练数据。</p></li></ul><h3 id="对数似然函数"><a href="#对数似然函数" class="headerlink" title="对数似然函数"></a>对数似然函数</h3><ul><li><p>直接对似然函数进行微分有点困难，在此之前要把函数变形。</p><ol><li>首先它是联合概率。概率都是1以下的数，所以像联合概率这种概率乘法的值会越来越小。</li><li>另外还有一个，那就是乘法。与加法相比，乘法的计算量要大得多。</li></ol></li><li><p>只要取似然函数的对数就好了。像这样在等式两边加上log即可：</p></li></ul><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-11-1.png"></p><p>​我们现在考察的似然函数也是在L(θ1)＜L(θ2)时，有log L(θ1)＜log L(θ2)成立。也就是说，使L(θ)最大化等价于使log L(θ)最大化。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-11-2.png"></p><p>​现在我们考虑的只有y&#x3D;1和y&#x3D;0两种情况，所以应有P(y(i)&#x3D;0|x(i))+P(y(i)&#x3D;1|x(i))&#x3D;1。</p><h4 id="似然函数的微分"><a href="#似然函数的微分" class="headerlink" title="似然函数的微分"></a>似然函数的微分</h4><p>​最小化时要按照与微分结果的符号相反的方向移动，而最大化时要与微分结果的符号同向移动。</p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-12-1.png"></p><p><img src="/2024/07/31/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%883%EF%BC%89/3-12-2.png"></p><h3 id="线性不可分"><a href="#线性不可分" class="headerlink" title="线性不可分"></a>线性不可分</h3><p>​通过随意地增加次数，就可以得到复杂形状的决策边界了。</p><p>​还有一个名为SVM，也就是支持向量机的分类算法也很有名。此外，还有多分类的做法。</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI-Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于《白话机器学习的数学》的知识整理（2）</title>
    <link href="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/"/>
    <url>/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="设置问题"><a href="#设置问题" class="headerlink" title="设置问题"></a>设置问题</h3><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2.png"></p><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>​不可能让所有点的误差都等于0。所以我们要做的是让所有点的误差之和尽可能地小。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-0.png"></p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-1.png"></p><p>​我们只要想办法缩小误差虚线的高度，就能预测正确的点击量了。</p><p>​我们来把刚才说的内容用表达式展现出来。假设有n个训练数据，那么它们的误差之和可以用这样的表达式表示。这个表达式称为目标函数，E(θ)的E是误差的英语单词Error的首字母。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-2.png"></p><ul><li>x(i)和y(i)中的i不是i次幂的意思，而是指第i个训练数据；</li><li>我们对每个训练数据的误差取平方之后，全部相加，然后乘以1&#x2F;2。这么做是为了找到使E(θ)的值最小的θ。这样的问题称为最优化问题；</li><li>如果只是简单地计算差值，我们就得考虑误差为负值的情况，所以要计算误差的平方；</li><li>一般不用绝对值，而用平方。因为之后要对目标函数进行微分，比起绝对值，平方的微分更加简单；</li></ul><h4 id="最速下降法"><a href="#最速下降法" class="headerlink" title="最速下降法"></a>最速下降法</h4><ul><li>最速下降法或梯度下降法；</li></ul><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-3.png"></p><ol><li>A:&#x3D;B这种写法，它的意思是通过B来定义A。</li><li>η是称为学习率的正的常数，读作“伊塔”。根据学习率的大小，到达最小值的更新次数也会发生变化。换种说法就是收敛速度会不同。有时候甚至会出现完全无法收敛，一直发散的情况。</li><li>如果η较大，那么x:&#x3D;x-η(2x-2)会在两个值上跳来跳去，甚至有可能远离最小值。这就是发散状态。而当η较小时，移动量也变小，更新次数就会增加，但是值确实是会朝着收敛的方向而去。</li></ol><p>​如 图2-0 所示，fθ(x)拥有θ0和θ1两个参数。也就是说这个目标函数是拥有θ0和θ1的双变量函数，所以不能用普通的微分，而要用偏微分。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-4.png"></p><p>令<img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-5-1.png"></p><p>有<img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-5-2.png"></p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-5-3.png"></p><p>即<img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-5-4.png"></p><p>同理<img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-5-5.png"></p><p>综上所述：</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-5-6.png"></p><h3 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h3><p>​对于一开始我在图中添加的数据点来说，其实曲线比直线拟合得更好。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-6.png"></p><p>​当我们把fθ(x)定义为二次函数，就能用它来表示这条曲线了。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-7.png"></p><p>​或者用更大次数的表达式也可以。这样就能表示更复杂的曲线了。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-8.png"></p><p>​虽然次数越大拟合得越好，但难免也会出现过拟合的问题。</p><p>​对二次函数进行微分，最终得到：</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-9.png"></p><p>​像这样增加函数中多项式的次数，然后再使用函数的分析方法被称为<strong>多项式回归</strong>。</p><h3 id="多重回归"><a href="#多重回归" class="headerlink" title="多重回归"></a>多重回归</h3><p>​实际中要解决的很多问题是变量超过2个的复杂问题。</p><p>​如之前只是根据广告费来预测点击量，现在呢，决定点击量的除了广告费之外，还有广告的展示位置和广告版面的大小等多个要素。现在变量达到了3个以上，就无法可视化了。</p><p>​这次我们只考虑广告版面的大小，设广告费为x1、广告栏的宽为x2、广告栏的高为x3，那么fθ可以表示如下：</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-10.png"></p><p>​</p><p>​当有n个变量时：</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-11-1.png"></p><p>​把θ和x用列向量来定义：</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-11-2.png"></p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-11-3.png"></p><p>即：<img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-11-4.png"></p><p>​设u&#x3D;E(θ)、v&#x3D;fθ(x)的部分是一样的。为了一般化，我们可以考虑对第j个元素θj偏微分的表达式。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-12-1.png"></p><p>​u对v微分的部分是一样的，所以只需要求v对θj的微分就好了。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-12-2.png"></p><p>​第j个参数的更新表达式就是这样的:</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-12-3.png"></p><p>​想这样包含了多个变量的回归称为<strong>多重回归</strong>。</p><p>​所谓的最速下降法就是对所有的训练数据都重复进行计算。现在可以收集大量的数据，但是训练数据越多，循环次数也就越多，那么计算起来不就非常花时间了吗？为此：</p><h3 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h3><p>​最速下降法除了计算花时间以外，还有一个缺点——那就是容易陷入局部最优解。</p><p>​选用随机数作为初始值的情况比较多。不过这样每次初始值都会变，进而导致陷入局部最优解的问题</p><p>​如 图2-12-3 所示表达式使用了所有训练数据的误差，在随机梯度下降法中会随机选择一个训练数据，并使用它来更新参数。这个表达式中的k就是被随机选中的数据索引。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-13.png"></p><p>​ 最速下降法更新1次参数的时间，随机梯度下降法可以更新n次。此外，随机梯度下降法由于训练数据是随机选择的，更新参数时使用的又是选择数据时的梯度，所以不容易陷入目标函数的局部最优解。</p><p>​除了随机选择1个训练数据的做法，此外还有随机选择m个训练数据来更新参数的做法。设随机选择m个训练数据的索引的集合为K，那么我们这样来更新参数。</p><p><img src="/2024/07/23/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/2-14.png"></p><p>​ 假设训练数据有100个，那么在m&#x3D;10时，创建一个有10个随机数的索引的集合，例如K&#x3D;{61, 53, 59, 16, 30, 21, 85, 31, 51, 10}，然后重复更新参数。这种做法被称为<strong>小批量（mini-batch）梯度下降法</strong>。</p><p>​不管是随机梯度下降法还是小批量梯度下降法，我们都必须考虑学习率η。把η设置为合适的值是很重要的。</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI-Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于《白话机器学习的数学》的知识整理（1）</title>
    <link href="/2024/07/22/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/"/>
    <url>/2024/07/22/%E5%9F%BA%E4%BA%8E%E3%80%8A%E7%99%BD%E8%AF%9D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E3%80%8B%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><ul><li>经常听到的是鉴别垃圾邮件、用图像进行人脸识别、电商网站的推荐功能之类的。</li><li>虽然它的应用场景很多，但它不是万能的。了解机器学习适合的应用场景，明确它能做什么、不能做什么也很重要。</li></ul><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul><li>无论是过去还是现在，计算机都特别擅长处理重复的任务。所以计算机能够比人类更高效地读取大量的数据、学习数据的特征并从中找出数据的模式。这样的任务也被称为<strong>机器学习或者模式识别</strong>。</li><li>受益于 <ol><li>具备了能够收集大量数据的环境</li><li>具备了能够处理大量数据的环境</li></ol></li><li>当我们打算用机器学习做什么事情的时候，首先需要的就是数据。因为机器学习就是从数据中找出特征和模式的技术。</li><li>比起可以学习到大量知识，计算机能够更快地处理数据这一点更令人激动。现在可以使用GPU进行数值计算，Hadoop、Spark之类的分布式处理技术也逐渐成熟，所以才说现在“具备了能够处理大量数据的环境”。</li></ul><h3 id="相关算法"><a href="#相关算法" class="headerlink" title="相关算法"></a>相关算法</h3><ul><li><p>回归-regression</p><p>简单易懂地说，回归就是在处理连续数据如时间序列数据时使用的技术。如股价时间表。</p></li><li><p>分类-classification</p><p>只有两个类别的问题称为二分类，有三个及以上的问题称为多分类，比如数字的识别就属于多分类问题。</p><p>这种技术可以用来自动识别明信片上手写的邮件编码。有一个很有名的数据集叫MNIST，其中收集了大量手写的数字图片，以及图片实际的数字信息。</p></li><li><p>聚类-clustering</p><p>聚类与分类相似，却又有些不同。聚类考虑的问题是：假设在有100名学生的学校进行摸底考试，然后根据考试成绩把100名学生分为几组，根据分组结果，我们能得出某组偏重理科、某组偏重文科这样有意义的结论。</p><p>它与分类的区别在于数据带不带标签。也有人把标签称为正确答案数据。</p></li></ul><p>​使用有标签的数据进行的学习称为<strong>有监督学习</strong>，与之相反，使用没有标签的数据进行的学习称为<strong>无监督学习</strong>。回归和分类是有监督学习，而聚类是无监督学习。</p>]]></content>
    
    
    <categories>
      
      <category>技术</category>
      
      <category>AI</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AI-Learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
