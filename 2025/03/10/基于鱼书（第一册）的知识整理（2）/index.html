

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="blackstarry6">
  <meta name="keywords" content="">
  
    <meta name="description" content="本文是对鱼书（第一册）进行学习后的的知识整理，包含两种损失函数的学习和实现、数值微分的基本概念和应用以及两层神经网络的实现">
<meta property="og:type" content="article">
<meta property="og:title" content="基于鱼书（第一册）的知识整理（2）">
<meta property="og:url" content="http://example.com/2025/03/10/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="本文是对鱼书（第一册）进行学习后的的知识整理，包含两种损失函数的学习和实现、数值微分的基本概念和应用以及两层神经网络的实现">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2025/03/10/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/%E4%B8%89%E7%B1%BB%E8%AF%86%E5%88%AB%E7%AE%97%E6%B3%95.png">
<meta property="og:image" content="http://example.com/2025/03/10/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/%E6%A2%AF%E5%BA%A6%E6%B3%95.png">
<meta property="article:published_time" content="2025-03-10T13:16:31.000Z">
<meta property="article:modified_time" content="2025-03-22T13:51:52.026Z">
<meta property="article:author" content="blackstarry6">
<meta property="article:tag" content="AI-Learning">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2025/03/10/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/%E4%B8%89%E7%B1%BB%E8%AF%86%E5%88%AB%E7%AE%97%E6%B3%95.png">
  
  
  
  <title>基于鱼书（第一册）的知识整理（2） - Hexo</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>blackstarry6</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="基于鱼书（第一册）的知识整理（2）"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-03-10 21:16" pubdate>
          2025年3月10日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          7.6k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          63 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">基于鱼书（第一册）的知识整理（2）</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="神经网络的学习"><a href="#神经网络的学习" class="headerlink" title="神经网络的学习"></a>神经网络的学习</h1><p>​	这里所说的“学习”是指从训练数据中自动获取最优权重参数的过程。</p>
<p>​	为了使神经网络能进行学习，将导入损失函数这一指标。而学习的目的就是以该损失函数为基准，找出能使它的值达到最小的权重参数。</p>
<h2 id="从数据中学习"><a href="#从数据中学习" class="headerlink" title="从数据中学习"></a>从数据中学习</h2><p>​	神经网络的特征就是可以从数据中学习。所谓“从数据中学习”，是指可以由数据自动决定权重参数的值。</p>
<p>​	对于线性可分问题，感知机是可以利用数据自动学习的。根据“感知机收敛定理”，通过有限次数的学习，线性可分问题是可解的。但是，非线性可分问题则无法通过（自动）学习来解决。</p>
<h3 id="数据驱动"><a href="#数据驱动" class="headerlink" title="数据驱动"></a>数据驱动</h3><p>​	从数据中寻找答案、从数据中发现模式、根据数据讲故事……这些机器学习所做的事情，如果没有数据的话，就无从谈起。因此，<code>数据是机器学习的核心</code>。</p>
<p>​	人们以自己的经验和直觉为线索，通过反复试验推进工作。而机器学习的方法则极力避免人为介入，尝试从收集到的数据中发现答案（模式）。神经网络或深度学习则比以往的机器学习方法更能避免人为介入。</p>
<p>​	如果让我们自己来设计一个能将5正确分类的程序，就会意外地发现这是一个很难的问题。人可以简单地识别出5，但却很难明确说出是基于何种规律而识别出了5。此外，每个人都有不同的写字习惯，要发现其中的规律是一件非常难的工作。</p>
<p>​	一种方案是，先从图像中提取特征量，再用机器学习技术学习这些特征量的模式。</p>
<ul>
<li>这里所说的“特征量”是指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。</li>
<li>图像的特征量通常表示为向量的形式。</li>
<li>在计算机视觉领域，常用的特征量包括SIFT、SURF和HOG等。</li>
<li>使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、KNN等分类器进行学习。</li>
</ul>
<p>​	机器学习的方法中，由机器从收集到的数据中找出规律性。与从零开始想出算法相比，这种方法可以更高效地解决问题，也能减轻人的负担。</p>
<p>​	但是需要注意的是，将图像转换为向量时使用的特征量仍是由人设计的。对于不同的问题，必须使用合适的特征量（必须设计专门的特征量），才能得到好的结果。</p>
<p>​	也就是说，即使使用特征量和机器学习的方法，也需要针对不同的问题人工考虑合适的特征量。</p>
<p><img src="/2025/03/10/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/%E4%B8%89%E7%B1%BB%E8%AF%86%E5%88%AB%E7%AE%97%E6%B3%95.png" srcset="/img/loading.gif" lazyload></p>
<p>​	深度学习有时也称为端到端机器学习(end-to-end machine learning)。这里所说的端到端是指从一端到另一端的意思，也就是从原始数据（输入）中获得目标结果（输出）的意思。</p>
<p>​	神经网络的优点是对所有的问题都可以用同样的流程来解决。也就是说，与待处理的问题无关，神经网络可以将数据直接作为原始数据，进行“端对端”的学习。</p>
<h3 id="训练数据和测试数据"><a href="#训练数据和测试数据" class="headerlink" title="训练数据和测试数据"></a>训练数据和测试数据</h3><p>​	机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验等。首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。</p>
<p>​	为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。另外，训练数据也可以称为监督数据。</p>
<p>​	泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。获得泛化能力是机器学习的最终目标。</p>
<p>​	只对某个数据集过度拟合的状态称为过拟合(over fitting)。避免过拟合也是机器学习的一个重要课题。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>​	神经网络的学习通过某个指标表示现在的状态。然后，以这个指标为基准，寻找最优权重参数。神经网络的学习中所用的指标称为损失函数(loss function)。这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等。</p>
<p>​	损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。以“性能的恶劣程度”为指标可能会使人感到不太自然，但是如果给损失函数乘上一个负值，就可以解释为“在多大程度上不坏”，即“性能有多好”。并且，“使性能的恶劣程度达到最小”和“使性能的优良程度达到最大”是等价的，不管是用“恶劣程度”还是“优良程度”，做的事情本质上都是一样的。</p>
<h3 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a>均方误差</h3><p>$$<br>\text{MSE} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2<br>$$</p>
<ul>
<li><p>前者为真实值，后者为预测值。</p>
</li>
<li><p>n 是样本的数量。</p>
</li>
</ul>
<p>​	均方误差用于衡量预测值与实际值之间的平均平方差，通常用于回归问题的评估指标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_squared_error</span>(<span class="hljs-params">y, t</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0.5</span> * np.<span class="hljs-built_in">sum</span>((y-t)**<span class="hljs-number">2</span>)<br><br></code></pre></td></tr></table></figure>



<h3 id="交叉熵误差"><a href="#交叉熵误差" class="headerlink" title="交叉熵误差"></a>交叉熵误差</h3><p>$$<br>\text{Cross-Entropy} &#x3D; - \sum_{i&#x3D;1}^{n} \left( y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right)<br>$$</p>
<ul>
<li>前者是真实标签（通常为 0 或 1）。后者第 i个预测概率（预测类别为 1 的概率）。</li>
<li>n是样本的数量。</li>
</ul>
<p>​	实际上只计算对应正确解标签的输出的自然对数。比如，假设正确解标签的索引是“2”，与之对应的神经网络的输出是0.6，则交叉熵误差是-log 0.6 &#x3D; 0.51；若“2”对应的输出是0.1，则交叉熵误差为-log 0.1 &#x3D; 2.30。也就是说，交叉熵误差的值是由正确解标签所对应的输出结果决定的。</p>
<p>​	交叉熵误差常用于分类问题中，衡量模型的预测概率与实际标签之间的差异。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cross_entropy_error</span>(<span class="hljs-params">y, t</span>):<br>    delta = <span class="hljs-number">1e-7</span><br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(t * np.log(y + delta))<br><br></code></pre></td></tr></table></figure>

<p>​	函数内部在计算np.log时，加上了一个微小值delta。这是因为，当出现np.log(0)时，np.log(0)会变为负无限大的-inf，这样一来就会导致后续计算无法进行。作为保护性对策，添加一个微小值可以防止负无限大的发生。</p>
<h3 id="mini-batch学习"><a href="#mini-batch学习" class="headerlink" title="mini-batch学习"></a>mini-batch学习</h3><p>​	使用训练数据进行学习，严格来说，就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。因此，计算损失函数时必须将所有的训练数据作为对象。也就是说，如果训练数据有100个的话，我们就要把这100个损失函数的总和作为学习的指标。</p>
<p>​	通过除以N，可以求单个数据的“平均损失函数”。通过这样的平均化，可以获得和训练数据的数量无关的统一指标。比如，即便训练数据有1000个或10000个，也可以求得单个数据的平均损失函数。</p>
<p>​	另外，MNIST数据集的训练数据有60000个，如果以全部数据为对象求损失函数的和，则计算过程需要花费较长的时间。再者，如果遇到大数据，数据量会有几百万、几千万之多，这种情况下以全部数据为对象计算损失函数是不现实的。因此，我们从全部数据中选出一部分，作为全部数据的“近似”。</p>
<p>​	</p>
<p>​	神经网络的学习也是从训练数据中选出一批数据（称为mini-batch,小批量），然后对每个mini-batch进行学习。比如，从60000个训练数据中随机选择100笔，再用这100笔数据进行学习。这种学习方式称为mini-batch学习。</p>
<h3 id="mini-batch版交叉熵误差的实现"><a href="#mini-batch版交叉熵误差的实现" class="headerlink" title="mini-batch版交叉熵误差的实现"></a>mini-batch版交叉熵误差的实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cross_entropy_error</span>(<span class="hljs-params">y, t</span>):<br>    <span class="hljs-keyword">if</span> y.ndim == <span class="hljs-number">1</span>:<br>        t = t.reshape(<span class="hljs-number">1</span>, t.size)<br>        y = y.reshape(<span class="hljs-number">1</span>, y.size)<br><br>    batch_size = y.shape[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(t * np.log(y + <span class="hljs-number">1e-7</span>)) / batch_size<br><br></code></pre></td></tr></table></figure>

<p>​	y是神经网络的输出，t是监督数据。y的维度为1时，即求单个数据的交叉熵误差时，需要改变数据的形状。并且，当输入为mini-batch时，要用batch的个数进行正规化，计算单个数据的平均交叉熵误差。</p>
<p>​	当监督数据是标签形式（非one-hot表示，而是像“2”“7”这样的标签）时，交叉熵误差可通过如下代码实现.。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cross_entropy_error</span>(<span class="hljs-params">y, t</span>):<br>    <span class="hljs-keyword">if</span> y.ndim == <span class="hljs-number">1</span>:<br>        t = t.reshape(<span class="hljs-number">1</span>, t.size)<br>        y = y.reshape(<span class="hljs-number">1</span>, y.size)<br><br>    batch_size = y.shape[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(np.log(y[np.arange(batch_size), t] + <span class="hljs-number">1e-7</span>)) / batch_size<br><br></code></pre></td></tr></table></figure>

<p>​	实现的要点是，由于one-hot表示中t为0的元素的交叉熵误差也为0，因此针对这些元素的计算可以忽略。换言之，如果可以获得神经网络在正确解标签处的输出，就可以计算交叉熵误差。因此，t为one-hot表示时通过t *np.log(y)计算的地方，在t为标签形式时，可用np.log( y[np.arange (batch_size), t] )实现相同的处理（为了便于观察，这里省略了微小值1e-7）。</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">np</span>.arange (batch_size)会生成一个从<span class="hljs-number">0</span>到batch_size-<span class="hljs-number">1</span>的数组。比如当batch_size为<span class="hljs-number">5</span>时，np.arange(batch_size)会生成一个NumPy数组[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>,<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]。因为t中标签是以[<span class="hljs-number">2</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">9</span>, <span class="hljs-number">4</span>]的形式存储的，所以y[np.arange(batch_size), t]能抽出各个数据的正确解标签对应的神经网络的输出（在这个例子中，y[np.arange(batch_size), t]会生成NumPy数组[y[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>], y[<span class="hljs-number">1</span>,<span class="hljs-number">7</span>], y[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>], y[<span class="hljs-number">3</span>,<span class="hljs-number">9</span>], y[<span class="hljs-number">4</span>,<span class="hljs-number">4</span>]]）。<br></code></pre></td></tr></table></figure>



<h3 id="为何要设定损失函数"><a href="#为何要设定损失函数" class="headerlink" title="为何要设定损失函数"></a>为何要设定损失函数</h3><p>​	在神经网络的学习中，寻找最优参数（权重和偏置）时，要寻找使损失函数的值尽可能小的参数。为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值。</p>
<p>​	在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为0。</p>
<p>​	因为即便识别精度有所改善，它的值也不会像32.0123 …%这样连续变化，而是变为33%、34%这样的不连续的、离散的值。而如果把损失函数作为指标，则当前损失函数的值可以表示为0.92543 …这样的值。并且，如果稍微改变一下参数的值，对应的损失函数也会像0.93432 …这样发生连续性的变化。</p>
<p>​	识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值也是不连续地、突然地变化。作为激活函数的阶跃函数也有同样的情况。出于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行。</p>
<h2 id="数值微分"><a href="#数值微分" class="headerlink" title="数值微分"></a>数值微分</h2><p>​	梯度法使用梯度的信息决定前进的方向。</p>
<p>​	如果按照导数定义进行计算，会产生以下问题：</p>
<ol>
<li>因为想把尽可能小的值赋给h，所以h使用了10e-50这个微小值。但是，这样反而产生了舍入误差(rounding error)。所谓舍入误差，是指因省略小数的精细部分的数值（比如，小数点第8位以后的数值）而造成最终的计算结果上的误差。</li>
<li>真的导数（真的切线）和定义中得到的导数的值在严格意义上并不一致。这个差异的出现是因为h不可能无限接近0。</li>
</ol>
<p>​	数值微分含有误差。为了减小这个误差，我们可以计算函数f在(x+h)和(x-h)之间的差分。因为这种计算方法以x为中心，计算它左右两边的差分，所以也称为中心差分（而(x+h)和x之间的差分称为前向差分）。</p>
<p>​	利用微小的差分求导数的过程称为数值微分(numerical differentiation)。而基于数学式的推导求导数的过程，则用“解析性”(analytic)一词，称为“解析性求解”或者“解析性求导”。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">numerical_diff</span>(<span class="hljs-params">f, x</span>):<br>    h = <span class="hljs-number">1e-4</span> <span class="hljs-comment"># 0.0001</span><br>    <span class="hljs-keyword">return</span> (f(x+h) - f(x-h)) / (<span class="hljs-number">2</span>*h)<br><br></code></pre></td></tr></table></figure>

<p>​	偏导数和单变量的导数一样，都是求某个地方的斜率。不过，偏导数需要将多个变量中的某一个变量定为目标变量，并将其他变量固定为某个值。</p>
<h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><p>​	由全部变量的偏导数汇总而成的向量称为梯度(gradient)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">numerical_gradient</span>(<span class="hljs-params">f, x</span>):<br>    h = <span class="hljs-number">1e-4</span> <span class="hljs-comment"># 0.0001</span><br>    grad = np.zeros_like(x) <span class="hljs-comment"># 生成和x形状相同的数组</span><br><br>    <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(x.size):<br>        tmp_val = x[idx]<br>        <span class="hljs-comment"># f(x+h)的计算</span><br>        x[idx] = tmp_val + h<br>        fxh1 = f(x)<br><br>        <span class="hljs-comment"># f(x-h)的计算</span><br>        x[idx] = tmp_val - h<br>        fxh2 = f(x)<br><br>        grad[idx] = (fxh1 - fxh2) / (<span class="hljs-number">2</span>*h)<br>        x[idx] = tmp_val <span class="hljs-comment"># 还原值</span><br><br>    <span class="hljs-keyword">return</span> grad<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">function_2</span>(<span class="hljs-params">x</span>):<br>	<span class="hljs-keyword">return</span> x[<span class="hljs-number">0</span>]**<span class="hljs-number">2</span> + x[<span class="hljs-number">1</span>]**<span class="hljs-number">2</span><br><span class="hljs-comment"># 或者return np.sum(x**2)</span><br><br><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>numerical_gradient(function_2, np.array([<span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>]))<br>array([ <span class="hljs-number">6.</span>,  <span class="hljs-number">8.</span>])<span class="hljs-number">2</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>numerical_gradient(function_2, np.array([<span class="hljs-number">0.0</span>, <span class="hljs-number">2.0</span>]))<br>array([ <span class="hljs-number">0.</span>,  <span class="hljs-number">4.</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>numerical_gradient(function_2, np.array([<span class="hljs-number">3.0</span>, <span class="hljs-number">0.0</span>]))<br>array([ <span class="hljs-number">6.</span>,  <span class="hljs-number">0.</span>])<br></code></pre></td></tr></table></figure>

<p>​	函数numerical_gradient(f, x)中，参数f为函数，x为NumPy数组，该函数对NumPy数组x的各个元素求数值微分。</p>
<p>​	实际上，梯度会指向各点处的函数值降低的方向。更严格地讲，梯度指示的方向是各点处的函数值减小最多的方向</p>
<h3 id="梯度法"><a href="#梯度法" class="headerlink" title="梯度法"></a>梯度法</h3><p>​	机器学习的主要任务是在学习时寻找最优参数。同样地，神经网络也必须在学习时找到最优参数（权重和偏置）。</p>
<p>​	这里所说的最优参数是指损失函数取最小值时的参数。但是，一般而言，损失函数很复杂，参数空间庞大，我们不知道它在何处能取得最小值。而通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">梯度表示的是各点处的函数值减小最多的方向。因此，无法保证梯度所指的方向就是函数的最小值或者真正应该前进的方向。实际上，在复杂的函数中，梯度指示的方向基本上都不是函数值最小处。<br></code></pre></td></tr></table></figure>

<figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs arcade">函数的极小值、最小值以及被称为鞍点(saddle <span class="hljs-built_in">point</span>)的地方，梯度为<span class="hljs-number">0</span>。极小值是局部最小值，也就是限定在某个范围内的最小值。鞍点是从某个方向上看是极大值，从另一个方向上看则是极小值的点。<br>虽然梯度法是要寻找梯度为<span class="hljs-number">0</span>的地方，但是那个地方不一定就是最小值（也有可能是极小值或者鞍点）。此外，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入被称为“学习高原”的无法前进的停滞期。<br></code></pre></td></tr></table></figure>

<p>​	虽然梯度的方向并不一定指向最小值，但沿着它的方向能够最大限度地减小函数的值。因此，在寻找函数的最小值（或者尽可能小的值）的位置的任务中，要以梯度的信息为线索，决定前进的方向。</p>
<p><img src="/2025/03/10/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%882%EF%BC%89/%E6%A2%AF%E5%BA%A6%E6%B3%95.png" srcset="/img/loading.gif" lazyload></p>
<p>​	η表示更新量，在神经网络的学习中，称为学习率(learning rate)。学习率决定在一次学习中，应该学习多少，以及在多大程度上更新参数。</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dns">学习率需要事先确定为某个值，比如<span class="hljs-number">0</span>.<span class="hljs-number">01或0.001</span>。一般而言，这个值过大或过小，都无法抵达一个“好的位置”。在神经网络的学习中，一般会一边改变学习率的值，一边确认学习是否正确进行了。<br></code></pre></td></tr></table></figure>

<p>​	上面的是表示更新一次的式子，这个步骤会反复执行。也就是说，每一步都按该式更新变量的值，通过反复执行此步骤，逐渐减小函数值。虽然这里只展示了有两个变量时的更新过程，但是即便增加变量的数量，也可以通过类似的式子（各个变量的偏导数）进行更新。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient_descent</span>(<span class="hljs-params">f, init_x, lr=<span class="hljs-number">0.01</span>, step_num=<span class="hljs-number">100</span></span>):<br>    x = init_x<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(step_num):<br>        grad = numerical_gradient(f, x)<br>        x -= lr * grad<br><br>    <span class="hljs-keyword">return</span> x<br><br></code></pre></td></tr></table></figure>

<p>​	像学习率这样的参数称为超参数。这是一种和神经网络的参数（权重和偏置）性质不同的参数。相对于神经网络的权重参数是通过训练数据和学习算法自动获得的，学习率这样的超参数则是人工设定的。一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定。</p>
<h3 id="神经网络的梯度"><a href="#神经网络的梯度" class="headerlink" title="神经网络的梯度"></a>神经网络的梯度</h3><p>​	神经网络的学习也要求梯度。这里所说的梯度是指损失函数关于权重参数的梯度。</p>
<h2 id="学习算法的实现"><a href="#学习算法的实现" class="headerlink" title="学习算法的实现"></a>学习算法的实现</h2><p>​	神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。神经网络的学习分成下面4个步骤。</p>
<ul>
<li>步骤1（mini-batch）：从训练数据中随机选出一部分数据，这部分数据称为mini-batch。我们的目标是减小mini-batch的损失函数的值。</li>
<li>步骤2（计算梯度）：为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度。梯度表示损失函数的值减小最多的方向。</li>
<li>步骤3（更新参数）：将权重参数沿梯度方向进行微小更新。</li>
<li>步骤4（重复0：重复步骤1、步骤2、步骤3。</li>
</ul>
<p>​	这个方法通过梯度下降法更新参数，不过因为这里使用的数据是随机选择的mini batch数据，所以又称为随机梯度下降法(stochastic gradient descent)。“随机”指的是“随机选择的”的意思，因此，随机梯度下降法是“对随机选择的数据进行的梯度下降法”。</p>
<p>​	深度学习的很多框架中，随机梯度下降法一般由一个名为SGD的函数来实现。SGD来源于随机梯度下降法的英文名称的首字母。</p>
<h3 id="2层神经网络的类"><a href="#2层神经网络的类" class="headerlink" title="2层神经网络的类"></a>2层神经网络的类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir)<br><span class="hljs-keyword">from</span> common.functions <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> common.gradient <span class="hljs-keyword">import</span> numerical_gradient<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TwoLayerNet</span>:<br><br>    <span class="hljs-comment">#输入层、隐藏层、输出层的神经元数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size,</span><br><span class="hljs-params">                 weight_init_std=<span class="hljs-number">0.01</span></span>):<br>        <span class="hljs-comment"># 初始化权重</span><br>        <span class="hljs-variable language_">self</span>.params = &#123;&#125;<br>        <br>        <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W1&#x27;</span>] = weight_init_std * \<br>                            np.random.randn(input_size, hidden_size)<br>        <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b1&#x27;</span>] = np.zeros(hidden_size)<br>        <br>        <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W2&#x27;</span>] = weight_init_std * \<br>                            np.random.randn(hidden_size, output_size)<br>        <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b2&#x27;</span>] = np.zeros(output_size)<br><br>        <br>        <span class="hljs-comment">#进行识别（推理），x是图像数据</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, x</span>):<br>        W1, W2 = <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W1&#x27;</span>], <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W2&#x27;</span>]<br>        b1, b2 = <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b1&#x27;</span>], <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b2&#x27;</span>]<br><br>        a1 = np.dot(x, W1) + b1<br>        z1 = sigmoid(a1)<br>        a2 = np.dot(z1, W2) + b2<br>        y = softmax(a2)<br><br>        <span class="hljs-keyword">return</span> y<br><br>    <span class="hljs-comment">#计算损失函数的值</span><br>    <span class="hljs-comment"># x:输入数据, t:监督数据（正确解标签）</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, x, t</span>):<br>        y = <span class="hljs-variable language_">self</span>.predict(x)<br><br>        <span class="hljs-keyword">return</span> cross_entropy_error(y, t)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">accuracy</span>(<span class="hljs-params">self, x, t</span>):<br>        y = <span class="hljs-variable language_">self</span>.predict(x)<br>        y = np.argmax(y, axis=<span class="hljs-number">1</span>)<br>        t = np.argmax(t, axis=<span class="hljs-number">1</span>)<br><br>        accuracy = np.<span class="hljs-built_in">sum</span>(y == t) / <span class="hljs-built_in">float</span>(x.shape[<span class="hljs-number">0</span>])<br>        <span class="hljs-keyword">return</span> accuracy<br><br>    <span class="hljs-comment">#计算权重参数的梯度</span><br>    <span class="hljs-comment"># x:输入数据, t:监督数据</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">numerical_gradient</span>(<span class="hljs-params">self, x, t</span>):<br>        loss_W = <span class="hljs-keyword">lambda</span> W: <span class="hljs-variable language_">self</span>.loss(x, t)<br><br>        grads = &#123;&#125;<br>        grads[<span class="hljs-string">&#x27;W1&#x27;</span>] = numerical_gradient(loss_W, <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W1&#x27;</span>])<br>        grads[<span class="hljs-string">&#x27;b1&#x27;</span>] = numerical_gradient(loss_W, <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b1&#x27;</span>])<br>        grads[<span class="hljs-string">&#x27;W2&#x27;</span>] = numerical_gradient(loss_W, <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W2&#x27;</span>])<br>        grads[<span class="hljs-string">&#x27;b2&#x27;</span>] = numerical_gradient(loss_W, <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b2&#x27;</span>])<br><br>        <span class="hljs-keyword">return</span> grads<br><br><br></code></pre></td></tr></table></figure>



<h3 id="mini-batch的实现"><a href="#mini-batch的实现" class="headerlink" title="mini-batch的实现"></a>mini-batch的实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># coding: utf-8</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir)  <span class="hljs-comment"># 为了导入父目录的文件而进行的设定</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> dataset.mnist <span class="hljs-keyword">import</span> load_mnist<br><span class="hljs-keyword">from</span> two_layer_net <span class="hljs-keyword">import</span> TwoLayerNet<br><br><span class="hljs-comment"># 读入数据</span><br>(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="hljs-literal">True</span>, one_hot_label=<span class="hljs-literal">True</span>)<br><br>network = TwoLayerNet(input_size=<span class="hljs-number">784</span>, hidden_size=<span class="hljs-number">50</span>, output_size=<span class="hljs-number">10</span>)<br><br>iters_num = <span class="hljs-number">10000</span>  <span class="hljs-comment"># 适当设定循环的次数</span><br>train_size = x_train.shape[<span class="hljs-number">0</span>]<br>batch_size = <span class="hljs-number">100</span><br>learning_rate = <span class="hljs-number">0.1</span><br><br>train_loss_list = []<br>train_acc_list = []<br>test_acc_list = []<br><br>iter_per_epoch = <span class="hljs-built_in">max</span>(train_size / batch_size, <span class="hljs-number">1</span>)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iters_num):<br>    batch_mask = np.random.choice(train_size, batch_size)<br>    x_batch = x_train[batch_mask]<br>    t_batch = t_train[batch_mask]<br>    <br>    <span class="hljs-comment"># 计算梯度</span><br>    <span class="hljs-comment">#grad = network.numerical_gradient(x_batch, t_batch)</span><br>    grad = network.gradient(x_batch, t_batch)<br>    <br>    <span class="hljs-comment"># 更新参数</span><br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> (<span class="hljs-string">&#x27;W1&#x27;</span>, <span class="hljs-string">&#x27;b1&#x27;</span>, <span class="hljs-string">&#x27;W2&#x27;</span>, <span class="hljs-string">&#x27;b2&#x27;</span>):<br>        network.params[key] -= learning_rate * grad[key]<br>    <br>    loss = network.loss(x_batch, t_batch)<br>    train_loss_list.append(loss)<br>    <br>    <span class="hljs-keyword">if</span> i % iter_per_epoch == <span class="hljs-number">0</span>:<br>        train_acc = network.accuracy(x_train, t_train)<br>        test_acc = network.accuracy(x_test, t_test)<br>        train_acc_list.append(train_acc)<br>        test_acc_list.append(test_acc)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;train acc, test acc | &quot;</span> + <span class="hljs-built_in">str</span>(train_acc) + <span class="hljs-string">&quot;, &quot;</span> + <span class="hljs-built_in">str</span>(test_acc))<br><br><span class="hljs-comment"># 绘制图形</span><br>markers = &#123;<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;s&#x27;</span>&#125;<br>x = np.arange(<span class="hljs-built_in">len</span>(train_acc_list))<br>plt.plot(x, train_acc_list, label=<span class="hljs-string">&#x27;train acc&#x27;</span>)<br>plt.plot(x, test_acc_list, label=<span class="hljs-string">&#x27;test acc&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;epochs&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;accuracy&quot;</span>)<br>plt.ylim(<span class="hljs-number">0</span>, <span class="hljs-number">1.0</span>)<br>plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p>​	mini-batch的大小为100，需要每次从60000个训练数据中随机取出100个数据（图像数据和正确解标签数据）。然后，对这个包含100笔数据的mini-batch求梯度，使用随机梯度下降法(SGD)更新参数。这里，梯度法的更新次数（循环的次数）为10000。每更新一次，都对训练数据计算损失函数的值，并把该值添加到数组中。</p>
<h2 id="基于测试数据的评价"><a href="#基于测试数据的评价" class="headerlink" title="基于测试数据的评价"></a>基于测试数据的评价</h2><p>​	通过反复学习可以使损失函数的值逐渐减小。不过这个损失函数的值，严格地讲是“对训练数据的某个mini-batch的损失函数”的值。训练数据的损失函数值减小，虽说是神经网络的学习正常进行的一个信号，但光看这个结果还不能说明该神经网络在其他数据集上也一定能有同等程度的表现。</p>
<p>​	神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数据，即确认是否会发生过拟合。过拟合是指，虽然训练数据中的数字图像能被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象。</p>
<p>​	神经网络学习的最初目标是掌握泛化能力，因此，要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据。</p>
<p>​	epoch是一个单位。一个epoch表示学习中所有训练数据均被使用过一次时的更新次数。比如，对于10000笔训练数据，用大小为100笔数据的mini-batch进行学习时，重复随机梯度下降法100次，所有的训练数据就都被“看过”了。此时，100次就是一个epoch。</p>
<p>​	在上面的例子中，每经过一个epoch，就对所有的训练数据和测试数据计算识别精度，并记录结果。之所以要计算每一个epoch的识别精度，是因为如果在for语句的循环中一直计算识别精度，会花费太多时间。并且，也没有必要那么频繁地记录识别精度（只要从大方向上大致把握识别精度的推移就可以了）。因此，我们才会每经过一个epoch就记录一次训练数据的识别精度。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>​	首先，为了能顺利进行神经网络的学习，我们导入了损失函数这个指标。以这个损失函数为基准，找出使它的值达到最小的权重参数，就是神经网络学习的目标。为了找到尽可能小的损失函数值，我们介绍了使用函数斜率的梯度法。</p>
<ul>
<li>机器学习中使用的数据集分为训练数据和测试数据。</li>
<li>神经网络用训练数据进行学习，并用测试数据评价学习到的模型的泛化能力。</li>
<li>神经网络的学习以损失函数为指标，更新权重参数，以使损失函数的值减小。</li>
<li>利用某个给定的微小值的差分求导数的过程，称为数值微分。利</li>
<li>用数值微分，可以计算权重参数的梯度。</li>
<li>数值微分虽然费时间，但是实现起来很简单。</li>
</ul>
<h1 id="补充：利用PyTorch实现高速计算"><a href="#补充：利用PyTorch实现高速计算" class="headerlink" title="补充：利用PyTorch实现高速计算"></a>补充：利用PyTorch实现高速计算</h1><h2 id="2层神经网络的类-1"><a href="#2层神经网络的类-1" class="headerlink" title="2层神经网络的类"></a>2层神经网络的类</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> itertools<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TwoLayerNet</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size, device=<span class="hljs-string">&#x27;cuda&#x27;</span>, weight_init_std=<span class="hljs-number">0.01</span></span>):<br>        <span class="hljs-variable language_">self</span>.device = device<br><br>        <span class="hljs-comment"># 参数初始化 (替换为 PyTorch 张量)</span><br>        <span class="hljs-variable language_">self</span>.params = &#123;<br>            <span class="hljs-string">&#x27;W1&#x27;</span>: torch.nn.Parameter(weight_init_std * torch.randn(input_size, hidden_size, device=device)),<br>            <span class="hljs-string">&#x27;b1&#x27;</span>: torch.nn.Parameter(torch.zeros(hidden_size, device=device)),<br>            <span class="hljs-string">&#x27;W2&#x27;</span>: torch.nn.Parameter(weight_init_std * torch.randn(hidden_size, output_size, device=device)),<br>            <span class="hljs-string">&#x27;b2&#x27;</span>: torch.nn.Parameter(torch.zeros(output_size, device=device))<br>        &#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># 输入类型转换</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(x, torch.Tensor):<br>            x = torch.tensor(x, dtype=torch.float32, device=<span class="hljs-variable language_">self</span>.device)<br><br>        W1, W2 = <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W1&#x27;</span>], <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;W2&#x27;</span>]<br>        b1, b2 = <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b1&#x27;</span>], <span class="hljs-variable language_">self</span>.params[<span class="hljs-string">&#x27;b2&#x27;</span>]<br><br>        <span class="hljs-comment"># 严格保持原始计算流程</span><br>        a1 = torch.matmul(x, W1) + b1<br>        z1 = torch.sigmoid(a1)<br>        a2 = torch.matmul(z1, W2) + b2<br>        y = F.softmax(a2, dim=<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">return</span> y<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, x, t</span>):<br>        <span class="hljs-comment"># 转换监督数据</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(t, torch.Tensor):<br>            t = torch.tensor(t, dtype=torch.float32, device=<span class="hljs-variable language_">self</span>.device)<br><br>        y = <span class="hljs-variable language_">self</span>.predict(x)<br><br>        <span class="hljs-comment"># 手工实现交叉熵误差 (与原始逻辑一致)</span><br>        delta = <span class="hljs-number">1e-7</span>  <span class="hljs-comment"># 防止log(0)</span><br>        batch_size = y.shape[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">return</span> -torch.<span class="hljs-built_in">sum</span>(t * torch.log(y + delta)) / batch_size<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">accuracy</span>(<span class="hljs-params">self, x, t</span>):<br>        <span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment"># 禁用梯度跟踪</span><br>            y = <span class="hljs-variable language_">self</span>.predict(x)<br>            y_labels = torch.argmax(y, dim=<span class="hljs-number">1</span>)<br><br>            <span class="hljs-comment"># 兼容 one-hot 和普通标签</span><br>            <span class="hljs-keyword">if</span> t.dim() &gt; <span class="hljs-number">1</span>:<br>                t_labels = torch.argmax(t, dim=<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">else</span>:<br>                t_labels = t.long()<br><br>            <span class="hljs-keyword">return</span> (y_labels == t_labels).<span class="hljs-built_in">float</span>().mean().item()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient</span>(<span class="hljs-params">self, x, t</span>):<br>        h = <span class="hljs-number">1e-4</span>  <span class="hljs-comment"># 保持原始步长</span><br>        grads = &#123;&#125;<br><br>        <span class="hljs-comment"># 确保输入为张量</span><br>        x = x <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(x, torch.Tensor) <span class="hljs-keyword">else</span> torch.tensor(x, device=<span class="hljs-variable language_">self</span>.device)<br>        t = t <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(t, torch.Tensor) <span class="hljs-keyword">else</span> torch.tensor(t, device=<span class="hljs-variable language_">self</span>.device)<br><br>        <span class="hljs-comment"># 遍历所有参数</span><br>        <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.params:<br>            param = <span class="hljs-variable language_">self</span>.params[key]<br>            grad = torch.zeros_like(param)<br><br>            <span class="hljs-comment"># 生成所有可能的多维索引</span><br>            dims = param.shape<br>            indices = itertools.product(*[<span class="hljs-built_in">range</span>(dim) <span class="hljs-keyword">for</span> dim <span class="hljs-keyword">in</span> dims])<br><br>            <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> indices:<br>                <span class="hljs-keyword">with</span> torch.no_grad():  <span class="hljs-comment"># 禁用梯度追踪</span><br>                    original_val = param[idx].item()<br>                    param[idx] = original_val + h<br><br>                fxh1 = <span class="hljs-variable language_">self</span>.loss(x, t).item()<br><br>                <span class="hljs-keyword">with</span> torch.no_grad():<br>                    param[idx] = original_val - h<br><br>                fxh2 = <span class="hljs-variable language_">self</span>.loss(x, t).item()<br><br>                <span class="hljs-keyword">with</span> torch.no_grad():<br>                    param[idx] = original_val  <span class="hljs-comment"># 恢复原始值</span><br><br>                grad[idx] = (fxh1 - fxh2) / (<span class="hljs-number">2</span> * h)<br><br>            grads[key] = grad<br><br>        <span class="hljs-keyword">return</span> grads<br></code></pre></td></tr></table></figure>

<p>​	不建议测试，理解梯度的前向传播即可，没有反向传播的梯度变化耗时过长。</p>
<h2 id="mini-batch的实现-1"><a href="#mini-batch的实现-1" class="headerlink" title="mini-batch的实现"></a>mini-batch的实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># cs.py</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir) <span class="hljs-comment"># 为了导入父目录中的文件而进行的设定</span><br><span class="hljs-keyword">from</span> dataset.mnist <span class="hljs-keyword">import</span> load_mnist<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader, TensorDataset<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-keyword">from</span> two_layer_net_torch <span class="hljs-keyword">import</span> TwoLayerNet<br><br><span class="hljs-comment"># 数据加载与转换</span><br>(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="hljs-literal">True</span>, one_hot_label=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 转换为 PyTorch 张量并发送到设备</span><br>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span><br><br>x_train = torch.tensor(x_train, dtype=torch.float32, device=device)<br>t_train = torch.tensor(t_train, dtype=torch.float32, device=device)<br>x_test = torch.tensor(x_test, dtype=torch.float32, device=device)<br>t_test = torch.tensor(t_test, dtype=torch.float32, device=device)<br><br><span class="hljs-comment"># 创建数据加载器</span><br>train_dataset = TensorDataset(x_train, t_train)<br>train_loader = DataLoader(train_dataset, batch_size=<span class="hljs-number">100</span>, shuffle=<span class="hljs-literal">True</span>)<br><br>test_dataset= TensorDataset(x_test,t_test)<br>test_loader= DataLoader(test_dataset, batch_size=<span class="hljs-number">100</span>, shuffle=<span class="hljs-literal">True</span>)<br><br><br>network = TwoLayerNet(input_size=<span class="hljs-number">784</span>, hidden_size=<span class="hljs-number">50</span>, output_size=<span class="hljs-number">10</span>, device=device)<br><br><span class="hljs-comment">#迭代次数与epoch设置</span><br>iters_num=<span class="hljs-number">10000</span><br>train_size = x_train.shape[<span class="hljs-number">0</span>]<br>batch_size = <span class="hljs-number">100</span><br>learning_rate = <span class="hljs-number">0.1</span><br>iter_per_epoch = <span class="hljs-built_in">max</span>(train_size / batch_size, <span class="hljs-number">1</span>)<br><br><span class="hljs-comment">#中间数据的记录</span><br>train_loss_list = []<br>train_acc_list = []<br>test_acc_list = []<br><br>start_time=time.time()<br><br><span class="hljs-comment"># 主训练循环</span><br>train_iter=<span class="hljs-built_in">iter</span>(train_loader);<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iters_num):<br>    <span class="hljs-comment"># 随机获取批量数据</span><br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-comment"># 获取下一个批次 (自动维持迭代状态)</span><br>        x_batch, t_batch = <span class="hljs-built_in">next</span>(train_iter)<br>    <span class="hljs-keyword">except</span> StopIteration:<br>        <span class="hljs-comment"># 当遍历完所有数据后，重新创建打乱的迭代器</span><br>        train_iter = <span class="hljs-built_in">iter</span>(train_loader)  <br>        x_batch, t_batch = <span class="hljs-built_in">next</span>(train_iter)<br><br>    <span class="hljs-comment"># 设备转移</span><br>    x_batch = x_batch.to(device, non_blocking=<span class="hljs-literal">True</span>)<br>    t_batch = t_batch.to(device, non_blocking=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-comment"># 计算梯度</span><br>    grads = network.gradient(x_batch, t_batch)<br><br>    <span class="hljs-comment"># 参数更新</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> network.params:<br>            network.params[key] -= learning_rate * grads[key]<br><br>    <span class="hljs-comment"># 记录损失</span><br>    loss = network.loss(x_batch, t_batch)<br>    train_loss_list.append(loss.item())<br><br>    <span class="hljs-comment"># 周期验证</span><br>    <span class="hljs-keyword">if</span> i % iter_per_epoch == <span class="hljs-number">0</span>:  <span class="hljs-comment"># 每epoch验证一次（MNIST共600 batch/epoch）</span><br>        <span class="hljs-comment"># 训练集精度</span><br>        correct_train = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> x, t <span class="hljs-keyword">in</span> train_loader:<br>            x = x.to(device)<br>            t = t.to(device)<br>            correct_train += (network.accuracy(x, t) * x.shape[<span class="hljs-number">0</span>])<br>        train_acc = correct_train / <span class="hljs-built_in">len</span>(train_dataset)<br><br>        <span class="hljs-comment"># 测试集精度</span><br>        correct_test = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> x, t <span class="hljs-keyword">in</span> test_loader:<br>            x = x.to(device)<br>            t = t.to(device)<br>            correct_test += (network.accuracy(x, t) * x.shape[<span class="hljs-number">0</span>])<br>        test_acc = correct_test / <span class="hljs-built_in">len</span>(test_dataset)<br><br>        train_acc_list.append(train_acc)<br>        test_acc_list.append(test_acc)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Iter <span class="hljs-subst">&#123;i&#125;</span>: Train Acc <span class="hljs-subst">&#123;train_acc:<span class="hljs-number">.4</span>f&#125;</span>, Test Acc <span class="hljs-subst">&#123;test_acc:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br>        <br>        end_time = time.time()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Cost:&#123;:.2f&#125;秒&quot;</span>.<span class="hljs-built_in">format</span>(end_time - start_time))<br>        start_time = time.time()<br>        <br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">for epoch in range(10):  # 更改为 epoch 循环</span><br><span class="hljs-string">    # 训练阶段</span><br><span class="hljs-string">    for x_batch, t_batch in train_loader:</span><br><span class="hljs-string">        grads = network.gradient(x_batch, t_batch)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        # 使用优化器更新参数</span><br><span class="hljs-string">        with torch.no_grad():</span><br><span class="hljs-string">            for name, param in network.params.items():</span><br><span class="hljs-string">                param -= learning_rate * grads[name]</span><br><span class="hljs-string"></span><br><span class="hljs-string">        # 记录损失</span><br><span class="hljs-string">        loss = network.loss(x_batch, t_batch)</span><br><span class="hljs-string">        train_loss_list.append(loss.item())</span><br><span class="hljs-string"></span><br><span class="hljs-string">    # 验证阶段</span><br><span class="hljs-string">    train_acc = network.accuracy(x_train, t_train)</span><br><span class="hljs-string">    test_acc = network.accuracy(x_test, t_test)</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    train_acc_list.append(train_acc)</span><br><span class="hljs-string">    test_acc_list.append(test_acc)</span><br><span class="hljs-string">    print(f&quot;Epoch &#123;epoch + 1&#125;: Train Acc &#123;train_acc:.4f&#125;, Test Acc &#123;test_acc:.4f&#125;&quot;)</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    end_time = time.time()</span><br><span class="hljs-string">    print(&quot;Cost:&#123;:.2f&#125;秒&quot;.format(end_time - start_time))</span><br><span class="hljs-string">    start_time=time.time()</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># 可视化结果</span><br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">#loss迭代结果展示</span><br><span class="hljs-string">x = np.arange(len(train_loss_list))</span><br><span class="hljs-string">plt.plot(x, train_loss_list, label=&#x27;loss&#x27;)</span><br><span class="hljs-string"></span><br><span class="hljs-string">plt.xlabel(&quot;iteration&quot;)</span><br><span class="hljs-string">plt.ylabel(&quot;loss&quot;)</span><br><span class="hljs-string"></span><br><span class="hljs-string">plt.ylim(0,3)</span><br><span class="hljs-string">plt.gca().margins(x=0)</span><br><span class="hljs-string"></span><br><span class="hljs-string">plt.show()</span><br><span class="hljs-string"></span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>markers = &#123;<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;s&#x27;</span>&#125;<br>x = np.arange(<span class="hljs-built_in">len</span>(train_acc_list))<br><br>plt.plot(x, train_acc_list, label=<span class="hljs-string">&#x27;train acc&#x27;</span>)<br>plt.plot(x, test_acc_list, label=<span class="hljs-string">&#x27;test acc&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>)<br><br>plt.xlabel(<span class="hljs-string">&quot;epochs&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;accuracy&quot;</span>)<br>plt.ylim(<span class="hljs-number">0</span>, <span class="hljs-number">1.0</span>)<br>plt.gca().margins(x=<span class="hljs-number">0</span>)<br><br>plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># cs.py</span><br><span class="hljs-keyword">import</span> sys, os<br>sys.path.append(os.pardir) <span class="hljs-comment"># 为了导入父目录中的文件而进行的设定</span><br><span class="hljs-keyword">from</span> dataset.mnist <span class="hljs-keyword">import</span> load_mnist<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader, TensorDataset<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> two_layer_net_torch <span class="hljs-keyword">import</span> TwoLayerNet<br><br><span class="hljs-comment"># 数据加载与转换</span><br>(x_train, t_train), (x_test, t_test) = load_mnist(normalize=<span class="hljs-literal">True</span>, one_hot_label=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 转换为 PyTorch 张量并发送到设备</span><br>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span><br><br>x_train = torch.tensor(x_train, dtype=torch.float32, device=device)<br>t_train = torch.tensor(t_train, dtype=torch.float32, device=device)<br>x_test = torch.tensor(x_test, dtype=torch.float32, device=device)<br>t_test = torch.tensor(t_test, dtype=torch.float32, device=device)<br><br><span class="hljs-comment"># 创建数据加载器</span><br>train_dataset = TensorDataset(x_train, t_train)<br>train_loader = DataLoader(train_dataset, batch_size=<span class="hljs-number">100</span>, shuffle=<span class="hljs-literal">True</span>)<br><br>network = TwoLayerNet(input_size=<span class="hljs-number">784</span>, hidden_size=<span class="hljs-number">50</span>, output_size=<span class="hljs-number">10</span>, device=device)<br><br><br>train_loss_list = []<br>train_acc_list = []<br>test_acc_list = []<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):  <span class="hljs-comment"># 更改为 epoch 循环</span><br>    <span class="hljs-comment"># 训练阶段</span><br>    <span class="hljs-comment">#network.train()</span><br>    <span class="hljs-keyword">for</span> x_batch, t_batch <span class="hljs-keyword">in</span> train_loader:<br>        grads = network.gradient(x_batch, t_batch)<br><br>        <span class="hljs-comment"># 使用优化器更新参数</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> network.params.items():<br>                param -= <span class="hljs-number">0.1</span> * grads[name]<br><br>        <span class="hljs-comment"># 记录损失</span><br>        loss = network.loss(x_batch, t_batch)<br>        train_loss_list.append(loss.item())<br><br>    <span class="hljs-comment"># 验证阶段</span><br>    <span class="hljs-comment">#network.eval()</span><br>    train_acc = network.accuracy(x_train, t_train)<br>    test_acc = network.accuracy(x_test, t_test)<br>    train_acc_list.append(train_acc)<br>    test_acc_list.append(test_acc)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch + <span class="hljs-number">1</span>&#125;</span>: Train Acc <span class="hljs-subst">&#123;train_acc:<span class="hljs-number">.4</span>f&#125;</span>, Test Acc <span class="hljs-subst">&#123;test_acc:<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 可视化结果</span><br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">x = np.arange(len(train_loss_list))</span><br><span class="hljs-string">plt.plot(x, train_loss_list, label=&#x27;loss&#x27;)</span><br><span class="hljs-string"></span><br><span class="hljs-string">plt.xlabel(&quot;iteration&quot;)</span><br><span class="hljs-string">plt.ylabel(&quot;loss&quot;)</span><br><span class="hljs-string"></span><br><span class="hljs-string">plt.ylim(0,3)</span><br><span class="hljs-string">plt.gca().margins(x=0)</span><br><span class="hljs-string"></span><br><span class="hljs-string">plt.show()</span><br><span class="hljs-string"></span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br>markers = &#123;<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;s&#x27;</span>&#125;<br>x = np.arange(<span class="hljs-built_in">len</span>(train_acc_list))<br><br>plt.plot(x, train_acc_list, label=<span class="hljs-string">&#x27;train acc&#x27;</span>)<br>plt.plot(x, test_acc_list, label=<span class="hljs-string">&#x27;test acc&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>)<br><br>plt.xlabel(<span class="hljs-string">&quot;epochs&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;accuracy&quot;</span>)<br>plt.ylim(<span class="hljs-number">0</span>, <span class="hljs-number">1.0</span>)<br>plt.gca().margins(x=<span class="hljs-number">0</span>)<br><br>plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>)<br>plt.show()<br><br></code></pre></td></tr></table></figure>


                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%8A%80%E6%9C%AF/" class="category-chain-item">技术</a>
  
  
    <span>></span>
    
  <a href="/categories/%E6%8A%80%E6%9C%AF/AI/" class="category-chain-item">AI</a>
  
  
    <span>></span>
    
  <a href="/categories/%E6%8A%80%E6%9C%AF/AI/%E5%AD%A6%E4%B9%A0/" class="category-chain-item">学习</a>
  
  

  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/AI-Learning/" class="print-no-link">#AI-Learning</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>基于鱼书（第一册）的知识整理（2）</div>
      <div>http://example.com/2025/03/10/基于鱼书（第一册）的知识整理（2）/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>blackstarry6</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年3月10日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/03/22/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89/" title="扩散模型学习（一）">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">扩散模型学习（一）</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/03/08/%E5%9F%BA%E4%BA%8E%E9%B1%BC%E4%B9%A6%EF%BC%88%E7%AC%AC%E4%B8%80%E5%86%8C%EF%BC%89%E7%9A%84%E7%9F%A5%E8%AF%86%E6%95%B4%E7%90%86%EF%BC%881%EF%BC%89/" title="基于鱼书（第一册）的知识整理（1）">
                        <span class="hidden-mobile">基于鱼书（第一册）的知识整理（1）</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
